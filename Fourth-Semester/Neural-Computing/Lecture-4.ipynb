{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hebbian Learning**\n",
    "\n",
    "- Neurons strengthen their connection with each other based on repeated co-activation. This is often summarized as **\"cells that fire together, wire together.\"**\n",
    "\n",
    "### **Simple Perceptron (McCulloch-Pitts Model)**\n",
    "\n",
    "\n",
    "$$\n",
    "O = \\text{sign}(w^T x + b)\n",
    "$$\n",
    "Where:\n",
    "- $ w $ is the vector of weights.\n",
    "- $ x $ is the input vector.\n",
    "- $ b $ is the bias term.\n",
    "- $ \\text{sign}() $ is the activation function that outputs either +1 or -1 depending on the result of the linear sum.\n",
    "\n",
    "---\n",
    "\n",
    "### **Network Topologies**\n",
    "\n",
    "- **Types:**\n",
    "  - **Feedforward Networks**: Neurons are arranged in layers, with no connections going backward (no loops). This is the simplest form of an artificial neural network.\n",
    "  - **Recurrent Networks**: Neurons have feedback loops, meaning the output of a neuron can influence its own future input. These are often used in time-series or sequence-based tasks.\n",
    "  - **Deep Neural Networks (DNNs)**: These are multi-layered networks, usually with many hidden layers between the input and output layers. DNNs can learn more complex, hierarchical representations of data.\n",
    "\n",
    "- **Deep Neural Networks**: These can solve problems where simpler networks fail, as they can automatically extract complex features from data. However, they are computationally expensive and require more data for training.\n",
    "\n",
    "---\n",
    "\n",
    "- **Common Activation Functions**:\n",
    "  1. **Step function (Heaviside function)**: Outputs either 0 or 1 (binary classification).\n",
    "  2. **Sigmoid function**: Outputs values between 0 and 1. Often used for probabilistic outputs.\n",
    "  3. **Tanh function**: Outputs values between -1 and 1. It is a scaled version of the sigmoid.\n",
    "  4. **ReLU (Rectified Linear Unit)**: Outputs 0 for negative inputs and the input itself for positive inputs. It has become one of the most widely used activation functions for hidden layers.\n",
    "\n",
    "![Activation Functions](https://www.researchgate.net/profile/Aaron-Stebner-2/publication/341310767/figure/fig7/AS:890211844255749@1589254451431/Common-activation-functions-in-artificial-neural-networks-NNs-that-introduce.ppm)\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning Rule**\n",
    "\n",
    "The **learning rule** defines how a neural network learns from its training data. In supervised learning, the network is trained on labeled examples, and the goal is to adjust weights and biases to minimize the error between predicted and actual outputs.\n",
    "\n",
    "- **Training Process**: During training adjust using a learning algorithm (e.g., **gradient descent**).\n",
    "  \n",
    "- **Epochs**: A complete run of the entire training dataset. Happens more to ensure that the network generalizes well to unseen data.\n",
    "\n",
    "- **Generalization**: The ability of a neural network to perform well on unseen data after being trained on examples.\n",
    "\n",
    "### **Perceptron Learning Rule**\n",
    "\n",
    "- For each training example $ (x_p, y_p) $, where $ x_p $ is the input vector and $ y_p $ is the desired output, the perceptron computes the output $ O_p $ as:\n",
    "  $$\n",
    "  O_p = \\text{sign}(w^T x_p + b)\n",
    "  $$\n",
    "  - If the output $ O_p $ is incorrect, the weights are updated as follows:\n",
    "  $$\n",
    "  w' = w + \\eta y_p x_p\n",
    "  $$\n",
    "  Where:\n",
    "  - $ \\eta $ is the **learning rate** (a small constant, typically between 0 and 1).\n",
    "  - $ y_p $ is the true label (+1 or -1).\n",
    "  - $ x_p $ is the input vector for the current pattern.\n",
    "\n",
    "---\n",
    "\n",
    "The **AND logical function** is **linearly separable** because you can draw a straight line that separates the true outputs from the false ones.\n",
    "\n",
    "- **Non-linearly separable**: Problems like **XOR** (exclusive OR) are not linearly separable, meaning no single line can separate the two classes.\n",
    "\n",
    "#### **XOR Table**:\n",
    "\n",
    "| Input  | Output |\n",
    "|--------|--------|\n",
    "| (0,0)  |   0    |\n",
    "| (0,1)  |   1    |\n",
    "| (1,0)  |   1    |\n",
    "| (1,1)  |   0    |\n",
    "\n",
    "#### **Solution**:  \n",
    "- You can use a **multi-layer network** (e.g., a multilayer perceptron with a hidden layer) to transform the data into a higher-dimensional space where it becomes linearly separable.\n",
    "- **Transformation**: We apply two functions, $ g_1(\\mathbf{x}) $ and $ g_2(\\mathbf{x}) $, to map the inputs to a new space.\n",
    "   - We compute:  \n",
    "     $$\n",
    "     y_1 = \\text{sign}(g_1(\\mathbf{x})), \\quad y_2 = \\text{sign}(g_2(\\mathbf{x}))\n",
    "     $$\n",
    "3. **New space**: The transformed data points become **linearly separable** in this new space, meaning a straight line can now divide the classes.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
