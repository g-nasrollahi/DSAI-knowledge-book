{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **The Hopfield Network (HN)**\n",
    "- **Recurrent neural network** with **associative memory** that evolves until it reaches a stable state, which is one of the stored patterns (memories).\n",
    "- **Inspiration**: The network is inspired by the **Ising model** in physics, which describes magnetic fields and spins.\n",
    "- The network is **fully connected** and have a **weight matrix** that stores the connection strengths between neurons.\n",
    "- Neurons have two possible states: **-1** (inactive) or **+1** (active), which is a thresholded output.\n",
    "- The network stores a set of patterns $\\xi_1, \\xi_2, \\dots, \\xi_p$, and when presented with a new (not stored) pattern $x$, the network will evolve to produce the pattern that most closely resembles $x$.\n",
    "- The connections between neurons are **bidirectional** (if neuron $i$ is connected to neuron $j$ with weight $w_{ij}$, then neuron $j$ is connected to neuron $i$ with the same weight $w_{ji} = w_{ij}$).\n",
    "- A Hopfield Network with 4096 neurons can store and recall up to **three 64x64 pixel images**.\n",
    "- The network is capable of **restoring distorted patterns** to their original forms. Even if the input pattern is corrupted, the network will converge to the nearest stored pattern.\n",
    "- The network can store approximately **0.15N** patterns, where $N$ is the number of neurons. For example, a network with 1000 neurons can store about 150 patterns.\n",
    "\n",
    "#### Ways to Improve Capacity:\n",
    "1. **Sparse Coding**: Store patterns where only a small fraction of neurons are activated, improving efficiency.\n",
    "2. **Modified Learning Rules**: Use rules like the **Storkey Learning Rule** to improve the separation between stored patterns.\n",
    "3. **Increase the Number of Neurons**: More neurons directly increase the capacity to store patterns.\n",
    "4. **Hierarchical Networks**: Structuring the memory storage reduces interference and enhances capacity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### **Stable Patterns**\n",
    "- **Condition**:\n",
    "  - The condition for a stored pattern $\\xi_i$ to be stable in the network is:\n",
    "  $$\n",
    "  O_i = \\text{sgn} \\left( \\sum_{j} w_{ij} \\xi_j \\right) = \\xi_i\n",
    "  $$\n",
    "  This means that if the weights are set as $w_{ij} \\propto \\xi_i \\xi_j$, the network will return the stored pattern $\\xi$.\n",
    "\n",
    "- **Attractors**:\n",
    "  - **Attractors** are stable patterns that the network will eventually settle into.\n",
    "  - If the network is initialized with a random or noisy input, it will converge to one of the stored patterns (or its reverse).\n",
    "\n",
    "- **Crosstalk Term**: The crosstalk term $A$ represents the interference between stored patterns and is given by:\n",
    "  $$\n",
    "  A = \\frac{1}{N} \\sum_{j} \\sum_{\\mu \\neq \\nu} \\xi_\\mu^i \\xi_\\mu^j \\xi_\\nu^j\n",
    "  $$\n",
    "  If $A$ is too large, retrieval errors may occur, and the network may converge to a **spurious attractor** (an unintended pattern).\n",
    "  \n",
    "- **Spurious Attractors**: These are **undesired minima** in the energy landscape, where the network converges to a combination of stored patterns, instead of a single pattern. Spurious attractors arise when the stored patterns are too similar or when too many patterns are stored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## **Update Function**\n",
    "\n",
    "When you input a pattern into the Hopfield network, the update function adjusts the network's states one step at a time, getting it closer to one of the stored patterns.\n",
    "\n",
    "The **update rule** for neuron $i$ at time $t+1$ is:\n",
    "\n",
    "$$\n",
    "O_i (t+1) = \\text{sgn} \\left( \\sum_j w_{ij} O_j(t) \\right)\n",
    "$$\n",
    "Where:\n",
    "- $ O_i (t+1) $ is the new state of neuron $i$ at time $t+1$.\n",
    "- $ w_{ij} $ is the weight between neurons $i$ and $j$.\n",
    "- $ O_j(t) $ is the state of neuron $j$ at time $t$.\n",
    "\n",
    "Each update step causes the system's energy to either decrease or stay constant, which drives the system towards a **stable state** (a stored pattern).\n",
    "\n",
    "\n",
    "\n",
    "- **Hamming Distance**: This measures how many bits differ between the input and the stored pattern. It can be seen as how many steps it takes for the network to reach the stored pattern, but typically, each update step reduces the Hamming Distance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Energy Function**\n",
    "The energy function measures the \"status\" of the network. With each update, the network's energy decreases, and the goal is to minimize this energy. When the energy reaches a local minimum, the network is stable and has reached a stored pattern.\n",
    "\n",
    "The energy of the network at time $t$ is given by:\n",
    "$$\n",
    "H = -\\frac{1}{2} \\sum_{i,j} w_{ij} x_i x_j = -\\frac{1}{2} \\mathbf{x}^T W \\mathbf{x}\n",
    "$$\n",
    "Where:\n",
    "- $ \\mathbf{x} $ is the **vector** of neuron states at time $t$ (a vector of values $x_i$, where each $x_i$ is either +1 or -1).\n",
    "- $ W $ is the **weight matrix**, where $w_{ij}$ represents the connection strength between neuron $i$ and neuron $j$.\n",
    "- The term $ x_i x_j $ represents the interaction between two neurons $i$ and $j$.\n",
    "\n",
    "The **negative sign** ensures that the energy decreases as the neurons move towards stable configurations.\n",
    "\n",
    "\n",
    "- **Energy Minimization**: The system's goal is to minimize its **energy function**. The energy of the system decreases monotonically as the network updates, leading it to stable states (attractors).\n",
    "- The energy function for the Hopfield Network can be defined as:\n",
    "  $$\n",
    "  E = -\\sum_{i} \\sum_{j} w_{ij} O_i O_j\n",
    "  $$\n",
    "  The update rule ensures that the energy of the system decreases with each iteration, and the network converges to a local minimum, which corresponds to a stored pattern.\n",
    "\n",
    "- **Gradient Descent**:\n",
    "  - The weight matrix can be optimized using **gradient descent**, where the goal is to minimize the energy function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "### **Hebbian Rule for Multiple Patterns**\n",
    "- To store multiple patterns, the weights $w_{ij}$ are set as a **superposition of terms**:\n",
    "  $$\n",
    "  w_{ij} = \\frac{1}{N} \\sum_{p=1}^{P} \\xi_{i}^{(p)} \\xi_{j}^{(p)}\n",
    "  $$\n",
    "  This rule is based on **Hebbian learning**, which states that if two neurons are active together in multiple patterns, their connection strengthens.\n",
    "  \n",
    "- **Result**: The network will evolve to restore the pattern that is closest to the input, making it an **associative memory** model.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
