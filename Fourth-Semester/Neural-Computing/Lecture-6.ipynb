{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87de005d",
   "metadata": {},
   "source": [
    "* **Loss function for a linear perceptron:** $ L(w, \\mathbf{x}_p) = \\frac{1}{2} (y_p - w^T \\mathbf{x}_p)^2 $\n",
    "* **Weight update (Gradient Descent):** $ w_k \\leftarrow w_k - \\eta \\frac{\\partial L}{\\partial w_k} = w_k + \\eta (y_p - O_p) x_k^p $\n",
    "* **Loss Functions**:\n",
    "  * **Mean Square Error** (MSE): $ \\text{MSE} = \\frac{1}{2n} \\sum_{i=1}^n (t^{(i)} - O^{(i)})^2 $\n",
    "  * **Mean Absolute Error** (MAE): $ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |t^{(i)} - O^{(i)}| $\n",
    "  * **Cross-Entropy Loss** (works well with softmax outputs) : Error will increase more when the output is far from the actual value.\n",
    "  * $ \\text{Cross-Entropy} = - \\sum_{i=1}^n t^{(i)} \\log p^{(i)} $\n",
    "- **Regression with continuous outputs**:\n",
    "  - **Saturating activation functions:** Output is limited to range like [0,1] or [-1,1] (e.g., sigmoid, tanh) can lead to vanishing gradients, making training slow.\n",
    "  - **Non-saturating activation functions:** Output is zero for y<0 and the same if y>= 0 (e.g., ReLU) help avoid this issue, allowing for faster convergence.\n",
    "- **Multi-label classification**: If there are multiple labels the output.\n",
    "  - **Softmax activation function**: Converts raw scores into probabilities for each class.\n",
    "  - **Cross-entropy loss**: Measures the difference between predicted probabilities and actual labels.\n",
    "* **Gradient Descent** Variants\n",
    "  * **Batch Gradient Descent:**\n",
    "    Uses **all** data points to calculate gradients. Precise but slow and needs more memory.\n",
    "  * **Stochastic Gradient Descent (SGD):**\n",
    "    Uses **one random** data point per update. Faster but noisier.\n",
    "  * **Mini-batch Gradient Descent:**\n",
    "    Uses a small batch (e.g., 32 or 64) of random points. Combines pros of batch and SGD.\n",
    "* **Epoch** is when a dataset entirely passed but **iteration** is one update step which may not all data points passed (in mini-batch everytime a part of the dataset is used)\n",
    "* **Momentum** and **Nesterov Accelerated Gradient**: Techniques to improve convergence speed and stability in gradient descent by smoothing, reducing oscillations and helping escape local minima (by **inertia**)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
