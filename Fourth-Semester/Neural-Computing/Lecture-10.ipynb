{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a631544",
   "metadata": {},
   "source": [
    "Sure! Here’s a simplified, complete exam note for your Lecture 8 on Recurrent Neural Networks (RNNs) from Elena Raponi’s Neural Computing course. I added examples to make concepts clearer.\n",
    "\n",
    "---\n",
    "\n",
    "# Lecture 8: Recurrent Neural Networks (RNNs)\n",
    "\n",
    "**Neural Computing Course – Leiden University**\n",
    "**Lecturer: Elena Raponi**\n",
    "Date: 14/04/2025\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Motivation – Time Series Forecasting\n",
    "\n",
    "* Many data come as sequences (time series), like weather data, stock prices, or text sentences.\n",
    "* Data points depend on previous points (temporal dependency).\n",
    "* **Question:** Can normal multilayer neural networks (NNs) handle sequences?\n",
    "* **Problem:**\n",
    "\n",
    "  * How much past data should we use?\n",
    "  * Multilayer NNs ignore order and time dependency.\n",
    "  * Relationships in data may change over time.\n",
    "\n",
    "*Example:* Predict tomorrow's temperature using past week’s data — simple NNs don’t consider order, but time matters!\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Introduction\n",
    "\n",
    "* Traditional multilayer NNs have no cycles (no feedback).\n",
    "* What if we add cycles?\n",
    "* Cycles mean the network’s output depends on previous outputs (memory).\n",
    "* Given the same input, output changes over time forming a sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Hopfield Networks (Memory example)\n",
    "\n",
    "* A type of recurrent network used for associative memory.\n",
    "* Uses activation functions like tanh or sigmoid.\n",
    "* The network “settles” into stable states called attractors — these represent stored memories.\n",
    "* Given a noisy or incomplete input, it retrieves the closest stored pattern.\n",
    "\n",
    "*Example:* Remember patterns like handwritten digits; if input is noisy ‘3’, it recalls a clean ‘3’.\n",
    "\n",
    "* **Discrete Hopfield Network:** outputs limited to {-1, 1} using sign function.\n",
    "* Weight update: $w_{ij} \\propto x_i x_j$ to store patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Vanilla Recurrent Neural Network\n",
    "\n",
    "* Adds a **hidden state** $h(t)$ that acts as short-term memory, remembers past info.\n",
    "* At each time step $t$, output $O(t)$ depends on input $x(t)$ and previous hidden state $h(t-1)$.\n",
    "\n",
    "*Example:* In language, to understand the current word, the RNN remembers previous words.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Sentiment Classification (Simple NLP task)\n",
    "\n",
    "* Input: sequence of words (e.g., a movie review).\n",
    "* Output: classify review as **positive** or **negative**.\n",
    "* The RNN processes words one by one, updating hidden state, then outputs sentiment prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Training RNNs – Backpropagation Through Time (BPTT)\n",
    "\n",
    "* Unfold the RNN over time steps into a long feedforward network.\n",
    "* Compute gradients at each step, then average them.\n",
    "* Update weights accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Problems with Vanilla RNNs\n",
    "\n",
    "* **Vanishing gradients:** gradients become too small, so the network \"forgets\" early inputs.\n",
    "* **Exploding gradients:** gradients become too large, causing unstable training.\n",
    "* Depends on the spectral norm (largest singular value) of weight matrix $W$.\n",
    "\n",
    "*Solution:* Use **LSTM** networks to handle long-term dependencies better.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Long Short-Term Memory Networks (LSTMs)\n",
    "\n",
    "* Introduced by Hochreiter & Schmidhuber (1997).\n",
    "* Key idea: use a **cell state** $C_t$ that carries long-term memory through the network.\n",
    "* Use **gates** (sigmoid units) to control flow of information:\n",
    "\n",
    "### Gates in LSTM:\n",
    "\n",
    "* **Forget gate $f_t$:** Decides what information to throw away from the cell state.\n",
    "* **Input gate $i_t$:** Decides what new information to add to the cell state.\n",
    "* **Output gate $o_t$:** Decides what part of the cell state to output.\n",
    "\n",
    "*Why sigmoid?* It outputs values between 0 and 1, which act as \"filters\" or \"scales\" for information flow.\n",
    "\n",
    "*Why tanh for input?* To keep the added information in a suitable range (-1 to 1).\n",
    "\n",
    "---\n",
    "\n",
    "### Cell State – “Constant Error Carousel”\n",
    "\n",
    "* The cell state lets gradients flow unchanged through many time steps, solving vanishing gradient problems.\n",
    "* But, if forget gate values $f_t$ are close to 1 for many steps, old info is preserved well; if close to 0, info is quickly forgotten.\n",
    "* For very long sequences, this still can be tricky, leading to the use of **attention mechanisms**.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Sequence-to-Sequence (Seq2seq) Models\n",
    "\n",
    "* Popular for tasks like machine translation and multi-step forecasting.\n",
    "* Consist of two parts:\n",
    "\n",
    "### Encoder\n",
    "\n",
    "* Encodes input sequence into a hidden representation.\n",
    "* Uses word embeddings + RNN/LSTM layers.\n",
    "* Initial hidden state often random.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "* Generates output sequence from the encoder’s final hidden state.\n",
    "* Uses previous outputs as inputs for next steps.\n",
    "\n",
    "*Example:* Translate English sentence to French word by word.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary of Key Points\n",
    "\n",
    "| Topic                         | Main Idea                                          | Example                         |\n",
    "| ----------------------------- | -------------------------------------------------- | ------------------------------- |\n",
    "| Time Series Forecasting       | Data has order & time dependency                   | Predict stock price over time   |\n",
    "| Hopfield Network              | Associative memory, stable states                  | Recall handwritten digits       |\n",
    "| Vanilla RNN                   | Hidden state remembers short past                  | Sentiment classification        |\n",
    "| BPTT                          | Training method unfolding RNN in time              | Backprop through time steps     |\n",
    "| Vanishing/Exploding Gradients | Problems in training vanilla RNNs                  | Forgetting or unstable training |\n",
    "| LSTM                          | Uses gates & cell state to remember long-term info | Language modeling, speech       |\n",
    "| Seq2seq Model                 | Encoder-decoder for sequence translation           | Machine translation             |\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also make a diagram summary or help with practice questions! Would that help?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
