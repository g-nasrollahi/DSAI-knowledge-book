{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63afdef6",
   "metadata": {},
   "source": [
    "## 1. Learning Rule of the Perceptron\n",
    "\n",
    "* We have **P training points (patterns)**, each with inputs and a label $y_p$ which is either $-1$ or $+1$.\n",
    "* The perceptron has weights $w_j$ connecting inputs to output.\n",
    "* For input $x_p$, the output is:\n",
    "\n",
    "$$\n",
    "O_p = \\text{sign}(w^T x_p)\n",
    "$$\n",
    "\n",
    "* If the output $O_p$ is wrong (not equal to $y_p$), update weights:\n",
    "\n",
    "$$\n",
    "w_j \\leftarrow w_j + \\Delta_j\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\Delta_j = \\begin{cases}\n",
    "2 \\eta y_p x_{jp}, & \\text{if } O_p \\neq y_p \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* $\\eta$ is the learning rate (a small positive number).\n",
    "\n",
    "* Equivalent update formulas are:\n",
    "\n",
    "$$\n",
    "\\Delta_j = \\eta (1 - O_p y_p) x_{jp} \\quad \\text{or} \\quad \\Delta_j = \\eta (y_p - O_p) x_{jp}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Perceptron Optimality and Margin\n",
    "\n",
    "* Instead of just correct sign matching, we want the product $y_p w^T x_p$ to be **larger than a margin** $N \\rho$, where:\n",
    "\n",
    "  * $N$ is the number of inputs\n",
    "  * $\\rho > 0$ is the margin size (a fixed positive number)\n",
    "\n",
    "* Condition:\n",
    "\n",
    "$$\n",
    "y_p w^T x_p > N \\rho\n",
    "$$\n",
    "\n",
    "* When this is true, the pattern is confidently classified (not just barely correct).\n",
    "\n",
    "* Update rule with margin becomes:\n",
    "\n",
    "$$\n",
    "\\Delta_j = \\eta \\, \\Theta \\left(N \\rho - y_p w^T x_p\\right) y_p x_{jp}\n",
    "$$\n",
    "\n",
    "where $\\Theta$ is the step function:\n",
    "\n",
    "$$\n",
    "\\Theta(z) = \\begin{cases}\n",
    "1 & \\text{if } z \\geq 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* This means weights update only if the margin condition fails.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Margin and Optimal Weights\n",
    "\n",
    "* Define $\\hat{x}_p = y_p x_p$ to combine input and label.\n",
    "\n",
    "* The margin condition is:\n",
    "\n",
    "$$\n",
    "w^T \\hat{x}_p > N \\rho\n",
    "$$\n",
    "\n",
    "* **Margin $M(w)$** is the distance of the closest point to the decision boundary:\n",
    "\n",
    "$$\n",
    "M(w) = \\frac{1}{\\|w\\|} \\min_p w^T \\hat{x}_p\n",
    "$$\n",
    "\n",
    "* The **best weights $w^*$** maximize the margin:\n",
    "\n",
    "$$\n",
    "w^* = \\arg \\max_w M(w) = \\arg \\max_w \\frac{\\min_p w^T \\hat{x}_p}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "* Maximizing margin means better, more robust classification.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Proof of Convergence of the Learning Rule** (Simple Explanation)\n",
    "\n",
    "### What we want to prove:\n",
    "\n",
    "If data is linearly separable (there exists some $w^*$ that classifies all correctly with margin > 0), then the perceptron learning algorithm **will find weights $w$ that classify correctly after a finite number of updates**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Define margin of best solution\n",
    "\n",
    "$$\n",
    "M(w^*) = \\frac{1}{\\|w^*\\|} \\min_p w^{*T} \\hat{x}_p > 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Express current weights after $H$ updates\n",
    "\n",
    "Starting from $w=0$, after updating on misclassified points:\n",
    "\n",
    "$$\n",
    "w = \\eta \\sum_{p=1}^P H^p \\hat{x}_p\n",
    "$$\n",
    "\n",
    "where $H^p$ is how many times pattern $p$ was used in an update, and $H = \\sum_p H^p$ is total updates.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Growth of overlap with $w^*$\n",
    "\n",
    "Calculate the dot product between $w$ and $w^*$:\n",
    "\n",
    "$$\n",
    "w \\cdot w^* = \\eta \\sum_p H^p \\hat{x}_p \\cdot w^* \\geq \\eta \\min_p (\\hat{x}_p \\cdot w^*) \\sum_p H^p = \\eta H M(w^*) \\|w^*\\|\n",
    "$$\n",
    "\n",
    "This means the projection $w \\cdot w^*$ grows **at least linearly with $H$**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Growth of weight length $\\|w\\|$\n",
    "\n",
    "At each update:\n",
    "\n",
    "$$\n",
    "\\Delta \\|w\\|^2 = \\|w + \\eta \\hat{x}_p\\|^2 - \\|w\\|^2 = \\eta^2 \\|\\hat{x}_p\\|^2 + 2 \\eta w \\cdot \\hat{x}_p\n",
    "$$\n",
    "\n",
    "Since $\\hat{x}_p$ components are bounded (usually $\\pm 1$), assume:\n",
    "\n",
    "$$\n",
    "\\|\\hat{x}_p\\|^2 \\leq N\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "w \\cdot \\hat{x}_p \\leq N p\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\Delta \\|w\\|^2 \\leq N \\eta (\\eta + 2p)\n",
    "$$\n",
    "\n",
    "After $H$ steps,\n",
    "\n",
    "$$\n",
    "\\|w\\|^2 \\leq H N \\eta (\\eta + 2p)\n",
    "$$\n",
    "\n",
    "Thus, $\\|w\\|$ grows **at most proportional to $\\sqrt{H}$**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Combining the two\n",
    "\n",
    "Look at normalized overlap:\n",
    "\n",
    "$$\n",
    "\\frac{w \\cdot w^*}{\\|w\\|} \\geq \\frac{\\eta H M(w^*) \\|w^*\\|}{\\sqrt{H} \\sqrt{N \\eta (\\eta + 2p)}} = \\text{something that grows as } \\sqrt{H}\n",
    "$$\n",
    "\n",
    "This cannot grow without bound because cosine between $w$ and $w^*$ is at most 1.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Bound on $H$\n",
    "\n",
    "Define:\n",
    "\n",
    "$$\n",
    "\\phi = \\frac{(w \\cdot w^*)^2}{\\|w\\|^2 \\|w^*\\|^2} = \\cos^2(\\alpha) \\leq 1\n",
    "$$\n",
    "\n",
    "From the previous inequality,\n",
    "\n",
    "$$\n",
    "1 \\geq \\phi \\geq H \\frac{M(w^*)^2 \\eta}{N (\\eta + 2p)}\n",
    "$$\n",
    "\n",
    "Rearranged:\n",
    "\n",
    "$$\n",
    "H \\leq \\frac{N (\\eta + 2p)}{M(w^*)^2 \\eta}\n",
    "$$\n",
    "\n",
    "This says the total number of updates $H$ is **bounded and finite**.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary:\n",
    "\n",
    "* If data is linearly separable, the perceptron learning will stop after finite updates.\n",
    "* The number of updates depends on the margin $M(w^*)$, learning rate $\\eta$, and number of inputs $N$.\n",
    "* Larger margin means fewer updates needed.\n",
    "\n",
    "---\n",
    "\n",
    "### If you want, I can also provide a simple example with numbers to illustrate how the weights update!\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to add a short example or explain any part more simply?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
