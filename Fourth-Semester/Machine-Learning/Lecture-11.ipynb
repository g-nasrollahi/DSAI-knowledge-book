{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c50f631",
   "metadata": {},
   "source": [
    "### **xAI**\n",
    "\n",
    "### White-box models (Inherently interpretable)\n",
    "\n",
    "* Decision Trees\n",
    "* Logical models\n",
    "* Linear models (linear regression/classification)\n",
    "\n",
    "### Black-box models (Not inherently interpretable)\n",
    "\n",
    "* Neural networks\n",
    "* Random forests\n",
    "* Support Vector Machines (SVMs)\n",
    "* Deep learning models (CNNs, Transformers, LLMs)\n",
    "* Ensemble methods\n",
    "\n",
    "---\n",
    "\n",
    "## Categories of Explainable AI Methods\n",
    "\n",
    "| Category                  | Description                                                                              |\n",
    "| ------------------------- | ---------------------------------------------------------------------------------------- |\n",
    "| **Post-hoc**              | Explain model decisions *after* training (applies to any model, especially black-boxes). |\n",
    "| **Intrinsic**             | Models designed to be interpretable by themselves (white-box models).                    |\n",
    "| **Model-specific**        | Methods designed for a particular type of model (e.g., linear regression coefficients).  |\n",
    "| **Model-agnostic**        | Methods that work without needing to know model internals (just input-output behavior).  |\n",
    "| **Local explainability**  | Explain a single prediction for one specific input instance.                             |\n",
    "| **Global explainability** | Explain the overall behavior of the entire model.                                        |\n",
    "\n",
    "---\n",
    "\n",
    "## Local Explainability Methods\n",
    "\n",
    "### Feature Attribution\n",
    "\n",
    "* Assign importance scores to input features for a single prediction.\n",
    "* **Gradient-based methods:** Use gradients (derivatives) to see how input changes affect output (mostly for neural networks).\n",
    "* **Surrogate-based methods:** Train a simple, interpretable model (surrogate) locally around one instance to approximate the black-box model.\n",
    "\n",
    "  * Examples: **LIME** and **SHAP**.\n",
    "\n",
    "### Example-based Explanations\n",
    "\n",
    "* **Counterfactuals:** Show how changing features would change the prediction (what-if scenarios).\n",
    "* **Adversarial examples:** Small, possibly meaningless changes to inputs that flip predictions (helps understand model weaknesses).\n",
    "* **Data influence:** Measures how much each training sample affects the prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Global Explainability Methods\n",
    "\n",
    "### Probing\n",
    "\n",
    "* Train simple models to examine what parts of a neural network (like hidden layers) learn (e.g., shapes, colors in images).\n",
    "\n",
    "### Mechanistic Interpretability (MechInterp)\n",
    "\n",
    "* Study individual neurons or circuits in neural nets to understand their roles.\n",
    "* Use **causal interventions** (removing or altering neurons) to test effects on output.\n",
    "\n",
    "### Concept-based Explainability (e.g., TCAV)\n",
    "\n",
    "* Measure how strongly the model’s layers respond to human-understandable concepts (like \"striped\" patterns in images).\n",
    "* Helps explain *what* the model “looks for” when making decisions.\n",
    "\n",
    "---\n",
    "\n",
    "### LIME\n",
    "\n",
    "1. Start with an instance $x$ to explain.\n",
    "2. Generate many slightly altered samples around $x$ (perturbations).\n",
    "3. Get predictions for these samples from the black-box model $f$.\n",
    "4. Weight these samples by how close they are to $x$.\n",
    "5. Train a simple interpretable model $g$ (regression or random forest) on the weighted samples and their predictions.\n",
    "6. Use $g$ to explain which features are important for the prediction on $x$.\n",
    "\n",
    "\n",
    "### Limitations of LIME\n",
    "\n",
    "* Explanations can vary each time you run it (not very stable).\n",
    "* Can be manipulated or fooled (adversarial risk).\n",
    "* Computationally expensive because it builds a surrogate model for every instance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
