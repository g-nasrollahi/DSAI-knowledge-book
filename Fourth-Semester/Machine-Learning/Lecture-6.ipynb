{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "269cfc8d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## The Least-Squares Method\n",
    "\n",
    "- Uni-variate Linear Regression (Normal Linear Regression):\n",
    "- Residual (error)  = $\\hat{f}(x_i) - f(x_i)$.:\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluating Regression Performance\n",
    "\n",
    "#### Residual plots\n",
    "\n",
    "A plot of the amount of error (residuals) against the predicted values. We can decide if the model is suitable or we should try a different (Non-linear) model.\n",
    "\n",
    "![8.png](../../Files/fourth-semester/ml/8.png)\n",
    "\n",
    "* **Root Mean Squared Error (RMSE)**:\n",
    "\n",
    "  $$\n",
    "  \\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (f(x_i) - \\hat{f}(x_i))^2}\n",
    "  $$\n",
    "\n",
    "RMSE is the square root of the average of the squared differences between the predicted and actual values. (standard deviation of the residuals)\n",
    "\n",
    "* **Coefficient of determination $R^2$**:\n",
    "\n",
    "  $$\n",
    "  R^2 = 1 - \\frac{\\mathrm{RSS}}{\\mathrm{TSS}}\n",
    "  $$\n",
    "\n",
    "Where:\n",
    "\n",
    "* Residual Sum of Squares (RSS):\n",
    "\n",
    "  $$\n",
    "  \\mathrm{RSS} = \\sum_{i=1}^n (f(x_i) - \\hat{f}(x_i))^2\n",
    "  $$\n",
    "\n",
    "* Total Sum of Squares (TSS):\n",
    "\n",
    "  $$\n",
    "  \\mathrm{TSS} = \\sum_{i=1}^n (f(x_i) - \\bar{f})^2\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### Effect of Outliers\n",
    "\n",
    "Outliers can strongly affect the regression line because least squares penalizes large residuals heavily.\n",
    "\n",
    "**Possible solutions:**\n",
    "\n",
    "* Train model → detect and remove outliers with residual plots → retrain.\n",
    "* Use **Total Least Squares** that accounts for noise in both $x$ and $y$.\n",
    "\n",
    "\n",
    "## Regularised Regression (Ridge Regression)\n",
    "\n",
    "With few data points, the model can **overfit** the training data (low error on training, high error on test).\n",
    "\n",
    "Regularisation adds a penalty on large weights to avoid overfitting.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text\"{ERROR} + \\lambda \\cdot \\text{Penalty on Weights}\"\n",
    "$$\n",
    "\n",
    "Where $\\lambda$ is a hyperparameter that controls the amount of regularization.\n",
    "- Low $\\lambda$ → model tries harder to fit the data exactly.\n",
    "- High $\\lambda$ → model keeps weights small and simpler, less sensitive to noise.\n",
    "\n",
    "<img src=\"../../Files/fourth-semester/ml/9.png\" width=\"600px\">\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Using Least Squares for Classification\n",
    "\n",
    "### Linear Models for Classification\n",
    "\n",
    "We can encode two classes as real numbers:\n",
    "\n",
    "* Positive class: $y^+ = +1$\n",
    "* Negative class: $y^- = -1$\n",
    "\n",
    "Train linear regression to predict these labels.\n",
    "\n",
    "### Decision Rule\n",
    "\n",
    "Predict class for an instance $x$ by:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathrm{sign}(\\mathbf{w} \\cdot \\mathbf{x} - t)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathrm{sign}(z) = \\begin{cases}\n",
    "+1 & z > 0 \\\\\n",
    "0 & z = 0 \\\\\n",
    "-1 & z < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82001cb4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "## Support Vector Machines (SVM)\n",
    "\n",
    "<img src=\"../../Files/fourth-semester/ml/10.png\" width=\"600px\">\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
