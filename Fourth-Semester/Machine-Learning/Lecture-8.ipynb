{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d7acb0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Probability\n",
    "\n",
    "* **Frequentist:** Uses hypothesis testing and p-values to check if there is a meaningful difference.\n",
    "* **Bayesian:** Uses prior knowledge and calculates the probability one design is better than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a97be8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "\n",
    "## 2. Probabilistic Models\n",
    "\n",
    "\n",
    "**Decision rules:**\n",
    "\n",
    "**Maximum a posteriori (MAP) decision rule:**\n",
    "\n",
    "Consider both likelihood and prior.\n",
    "\n",
    "**Maximum likelihood (ML) decision rule:**\n",
    "\n",
    "Consider only likelihood, assume priors are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe7173",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "\n",
    "## 3. Probability Distributions for Continuous Variables\n",
    "\n",
    "**Continuous variables:** like age, height — take any numeric value.\n",
    "\n",
    "---\n",
    "\n",
    "### Gaussian (Normal) distribution:\n",
    "\n",
    "* Characterized by mean (μ) and standard deviation (σ).\n",
    "* Probability density for a value x:\n",
    "\n",
    "$$\n",
    "P(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "* Multivariate Gaussian extends this to vectors with mean vector μ and covariance matrix Σ.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Classification with Gaussians\n",
    "\n",
    "* Two classes with Gaussian distributions.\n",
    "* Each class has parameters (mean μ and std dev σ).\n",
    "* Find a decision boundary to separate classes.\n",
    "* Using likelihood ratio of the two Gaussians, find where LR=1 → decision boundary.\n",
    "\n",
    "---\n",
    "\n",
    "### Mixture Model\n",
    "\n",
    "Data comes from a mix of two Gaussians:\n",
    "\n",
    "$$\n",
    "P(x| \\oplus) = \\text{Gaussian with } \\mu_\\oplus, \\sigma_\\oplus\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x| \\ominus) = \\text{Gaussian with } \\mu_\\ominus, \\sigma_\\ominus\n",
    "$$\n",
    "\n",
    "Use likelihood ratio for classification.\n",
    "\n",
    "---\n",
    "\n",
    "### Decision boundaries scenarios:\n",
    "\n",
    "* If σ’s are equal, decision boundary is the midpoint between means.\n",
    "* If σ’s differ, the decision regions can be more complex, possibly non-contiguous.\n",
    "\n",
    "---\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "* Used to find parameters (μ, σ) that make the observed data most likely.\n",
    "* Example: Given data points, estimate the mean by maximizing the likelihood function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639eaed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "\n",
    "## 4. Probability Distributions for Categorical Variables\n",
    "\n",
    "**Categorical variables:** like words in a document.\n",
    "\n",
    "---\n",
    "\n",
    "* **Multivariate Bernoulli distribution:** models presence or absence of words (0 or 1).\n",
    "* **Multinomial distribution:** models counts of word occurrences (how many times each word appears).\n",
    "\n",
    "Formulas:\n",
    "\n",
    "* Bernoulli:\n",
    "\n",
    "$$\n",
    "P(X=(x_1,...,x_k)) = \\prod_{i=1}^k \\theta_i^{x_i} (1-\\theta_i)^{1-x_i}\n",
    "$$\n",
    "\n",
    "* Multinomial (total n words):\n",
    "\n",
    "$$\n",
    "P(X=(x_1,...,x_k)) = \\frac{n!}{x_1! ... x_k!} \\prod_{i=1}^k \\theta_i^{x_i}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f651854",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "\n",
    "## 5. Naïve Bayes Model\n",
    "\n",
    "Assumes **features are independent given the class** — this is a simplification but works well in practice.\n",
    "\n",
    "* Used in spam classification: assumes words occur independently in an email.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** This assumption is usually false because words depend on each other (e.g., \"scientific\" and \"experiment\" often appear together), but the model still works well.\n",
    "\n",
    "---\n",
    "\n",
    "### Using Naïve Bayes for classification:\n",
    "\n",
    "* Use decision rules (ML or MAP).\n",
    "* ML assumes uniform class distribution; MAP uses class priors.\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "\n",
    "Vocabulary: words a, b, c\n",
    "Email has: 3 'a's, 1 'b', 0 'c's\n",
    "Classify as spam or ham using:\n",
    "\n",
    "* Multivariate Bernoulli: uses binary presence (1 or 0)\n",
    "* Multinomial: uses counts of words\n",
    "\n",
    "---\n",
    "\n",
    "### Training Naïve Bayes\n",
    "\n",
    "* Estimate parameters (probabilities of words given class) from training data.\n",
    "* Use frequency counts: e.g., probability θ = (number of times word appears in class) / (total words in class).\n",
    "\n",
    "---\n",
    "\n",
    "### Zero frequency problem\n",
    "\n",
    "If a word never appears with a class in training data, its probability estimate is zero, making the whole product zero.\n",
    "\n",
    "**Solution:** Smoothing (Laplace correction):\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{d + 1}{n + k}\n",
    "$$\n",
    "\n",
    "where d = number of successes, n = trials, k = number of categories.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12886fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "## 6. Logistic Regression\n",
    "\n",
    "* Used for **binary classification**.\n",
    "* Estimates the probability p of belonging to a class.\n",
    "* Logistic function:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{e^x}{1 + e^x}\n",
    "$$\n",
    "\n",
    "* Model:\n",
    "\n",
    "$$\n",
    "\\hat{p}(x) = \\frac{1}{1 + e^{-(w \\cdot x - t)}}\n",
    "$$\n",
    "\n",
    "where w are weights, t is threshold.\n",
    "\n",
    "* Uses maximum likelihood to find w and t.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820a9c7f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 7. Gaussian Mixture Models (GMM)\n",
    "\n",
    "* Data generated by several Gaussian distributions with unknown labels.\n",
    "* Goal: estimate parameters (means μj, covariances Σj) of each Gaussian without knowing the class labels.\n",
    "\n",
    "---\n",
    "\n",
    "### Chicken and egg problem:\n",
    "\n",
    "To classify points, need parameters. To estimate parameters, need class labels.\n",
    "\n",
    "---\n",
    "\n",
    "### Expectation-Maximization (EM) algorithm:\n",
    "\n",
    "Iterative method to solve above problem.\n",
    "\n",
    "1. **Initialize** parameters randomly.\n",
    "2. **E-step:** Compute probabilities of each point belonging to each class given parameters.\n",
    "3. **M-step:** Update parameters using these probabilities.\n",
    "4. Repeat until convergence."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
