{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d7acb0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Probability\n",
    "\n",
    "* **Frequentist:** Uses hypothesis testing and p-values to check if there is a meaningful difference.\n",
    "* **Bayesian:** Uses prior knowledge and calculates the probability one design is better than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a97be8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Probabilistic Models\n",
    "\n",
    "\n",
    "**Decision rules:**\n",
    "\n",
    "**Maximum a posteriori (MAP) decision rule:**\n",
    "\n",
    "Consider both likelihood and prior.\n",
    "\n",
    "**Maximum likelihood (ML) decision rule:**\n",
    "\n",
    "Consider only likelihood, assume priors are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe7173",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Probability Distributions for Continuous Variables\n",
    "\n",
    "\n",
    "### Using Gaussians to classify\n",
    "\n",
    "* Two groups (classes) each have their own Gaussian with different means and spreads.\n",
    "* We find a **decision boundary** that separates the two groups.\n",
    "* This boundary is where the chance of belonging to each group is equal.\n",
    "\n",
    "\n",
    "### Mixture Model\n",
    "\n",
    "* Data can come from a mix of two Gaussian groups.\n",
    "* Each group has its own mean and spread.\n",
    "* We use the likelihood of each group to decide which class a data point belongs to.\n",
    "\n",
    "\n",
    "### Decision boundary cases\n",
    "\n",
    "* If spreads (σ) are the same, the boundary is in the middle of the means.\n",
    "* If spreads differ, the boundary can be more complex.\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "* A method to find the best mean and spread for the data.\n",
    "* It chooses values that make the observed data most likely.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639eaed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Probability Distributions for Categorical Variables\n",
    "\n",
    "**Categorical variables:** like words in a document.\n",
    "\n",
    "* **Multivariate Bernoulli distribution:** models presence or absence of words (0 or 1).\n",
    "* **Multinomial distribution:** models counts of word occurrences (how many times each word appears).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f651854",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Naïve Bayes Model\n",
    "\n",
    "Assumes **features are independent given the class** — this is a simplification but works well in practice.\n",
    "\n",
    "* Used in spam classification: assumes words occur independently in an email.\n",
    "\n",
    "This assumption is usually false because words depend on each other (e.g., \"scientific\" and \"experiment\" often appear together), but the model still works well.`\n",
    "\n",
    "### Zero frequency problem\n",
    "\n",
    "If a word never appears with a class in training data, its probability estimate is zero, making the whole product zero.\n",
    "\n",
    "**Solution:** Smoothing (Laplace correction):\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{d + 1}{n + k}\n",
    "$$\n",
    "\n",
    "where d = number of successes, n = trials, k = number of categories.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12886fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "## Logistic Regression\n",
    "\n",
    "* Used for **binary classification**.\n",
    "* Estimates the probability p of belonging to a class.\n",
    "\n",
    "![](https://media.datacamp.com/legacy/image/upload/v1660054820/Regression_charts_b9de7355cf.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820a9c7f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Gaussian Mixture Models (GMM)\n",
    "\n",
    "* Data generated by several Gaussian distributions with unknown labels.\n",
    "* Goal: estimate parameters (means μj, covariances Σj) of each Gaussian without knowing the class labels.\n",
    "\n",
    "To classify points, need parameters. To estimate parameters, need class labels.\n",
    "\n",
    "\n",
    "### Expectation-Maximization (EM) algorithm:\n",
    "\n",
    "Iterative method to solve above problem.\n",
    "\n",
    "1. **Initialize** parameters randomly.\n",
    "2. **E-step:** Compute probabilities of each point belonging to each class given parameters.\n",
    "3. **M-step:** Update parameters using these probabilities.\n",
    "4. Repeat until convergence.\n",
    "\n",
    "<img src=\"../../Files/fourth-semester/ml/12.png\" width=\"400px\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
