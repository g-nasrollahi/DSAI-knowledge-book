{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f1f42c",
   "metadata": {},
   "source": [
    "### Multi-class Classifiers\n",
    "\n",
    "* **Inherently non-binary:** Algorithms that directly support multiple classes (e.g., decision trees).\n",
    "* **Inherently binary:** Algorithms designed for two classes (e.g., Support Vector Machines, SVM). These need strategies to extend to multi-class.\n",
    "\n",
    "### Strategies to Convert Binary Classifiers to Multi-class\n",
    "\n",
    "1. **One versus Rest (One-vs-All):**\n",
    "   Train $k$ classifiers, each distinguishing one class from all others.\n",
    "\n",
    "2. **One versus One (All pairs):**\n",
    "   Train classifiers for every pair of classes, so total $k(k-1)/2$ classifiers.\n",
    "\n",
    "### Complexity Comparison\n",
    "\n",
    "| Approach    | Training complexity   | Inference complexity |\n",
    "| ----------- | --------------------- | -------------------- |\n",
    "| One-vs-Rest | $O(k m^\\alpha)$       | $O(k^\\beta)$         |\n",
    "| One-vs-One  | $O(k^2 (m/k)^\\alpha)$ | $O(k^2 \\beta)$       |\n",
    "\n",
    "* $k$: number of classes\n",
    "* $m$: number of instances\n",
    "* $\\alpha$, $\\beta$: algorithm-specific complexities\n",
    "\n",
    "So One-vs-Rest is generally more efficient for large $k$.\n",
    "\n",
    "\n",
    "# Evaluating Multi-class Classifiers\n",
    "\n",
    "### Confusion Matrix (Multi-class)\n",
    "\n",
    "Example (3 classes):\n",
    "\n",
    "| Actual \\ Predicted | C1 | C2 | C3 | Total |\n",
    "| ------------------ | -- | -- | -- | ----- |\n",
    "| C1                 | 15 | 2  | 3  | 20    |\n",
    "| C2                 | 7  | 15 | 8  | 30    |\n",
    "| C3                 | 2  | 3  | 45 | 50    |\n",
    "| Total              | 24 | 20 | 56 | 100   |\n",
    "\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{15 + 15 + 45}{100} = 0.75\n",
    "$$\n",
    "\n",
    "\n",
    "### Precision and Recall per Class\n",
    "\n",
    "* For each class:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{Predicted Positives}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{Actual Positives}}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "| Class | Precision      | Recall         |\n",
    "| ----- | -------------- | -------------- |\n",
    "| C1    | 15 / 24 = 0.63 | 15 / 20 = 0.75 |\n",
    "| C2    | 15 / 20 = 0.75 | 15 / 30 = 0.50 |\n",
    "| C3    | 45 / 56 = 0.80 | 45 / 50 = 0.90 |\n",
    "\n",
    "### Overall Precision and Recall\n",
    "\n",
    "* Weighted averages based on class proportions, e.g.,\n",
    "\n",
    "$$\n",
    "\\text{Precision} = 0.20 \\times 0.63 + 0.30 \\times 0.75 + 0.50 \\times 0.80 = 0.75\n",
    "$$\n",
    "\n",
    "\n",
    "### ROC Curve for Multi-class\n",
    "\n",
    "* For multi-class, draw a separate ROC curve for each class using the **one-versus-rest** approach (treat each class as \"positive\" and the rest as \"negative\").\n",
    "* To summarize performance, use:\n",
    "   * **Macro-average:** Calculates ROC for each class separately, then averages the results equally across classes (ignores class imbalance).\n",
    "   * **Micro-average:**  Calculates ROC for all classes together, treating each instance equally (considers class imbalance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b540f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Regression\n",
    "\n",
    "### Residuals and Loss\n",
    "\n",
    "* Residual = Actual value − Predicted value\n",
    "* Loss functions measures how far predictions are from true values (Sum of residuals squared).\n",
    "* Common loss functions:\n",
    "  * **Mean Squared Error (MSE):** Average of squared residuals.\n",
    "  * **Mean Absolute Error (MAE):** Average of absolute residuals.\n",
    "  * **Huber Loss:** Combines MSE and MAE, less sensitive to outliers.\n",
    "\n",
    "\n",
    "\n",
    "### Bias-Variance Tradeoff (dilemma)\n",
    "\n",
    "* **Bias:** Error from wrong assumptions in model.\n",
    "* **Variance:** Error from sensitivity to small fluctuations in training set.\n",
    "* Low bias → complex model; low variance → simple model.\n",
    "* Goal: balance both for best generalization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf60ae79",
   "metadata": {},
   "source": [
    "# Unsupervised and Descriptive Learning\n",
    "\n",
    "### Evaluating Clustering Performance\n",
    "\n",
    "- With Ground Truth (We know the real groups): Use metrics like \n",
    "   - Rand index\n",
    "   - Precision\n",
    "   - Recall\n",
    "   - F1 score.\n",
    "- Without Ground Truth (No real labels): Use internal metrics like \n",
    "   - Davies-Bouldin index\n",
    "   - Calinski-Harabasz Index\n",
    "   - Silhouette coefficient\n",
    "      - $ s = \\frac{b - a}{\\max(a,b)} $\n",
    "      * $a$: average distance to points in the same cluster\n",
    "      * $b$: average distance to points in the nearest other cluster\n",
    "      * Values close to +1 → good clustering; close to 0 → borderline; negative → wrong cluster.\n",
    "\n",
    "\n",
    "# Sub-group Discovery\n",
    "\n",
    "### What is Sub-group Discovery?\n",
    "\n",
    "* Finds **interesting subgroups** in data that are statistically unusual, not just aiming for classification accuracy.\n",
    "* Example: Finding groups with high risk of heart disease.\n",
    "\n",
    "### Evaluating Subgroups\n",
    "\n",
    "* Compare subgroup’s class distribution with overall distribution.\n",
    "* Chi-squared to see if the subgroup differs significantly.\n",
    "\n",
    "![Subgroup Discovery Example](../../Files/fourth-semester/ml/3.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
