{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning - Lecture 1\n",
    "\n",
    "### **Machine Learning's Core Ingredients**\n",
    "- **Tasks** define the problem machine learning is trying to solve. (Predictive vs. Descriptive).\n",
    "  - **A. Predictive Tasks (Supervised Learning: Help predict next data)**\n",
    "    - **Classification**\n",
    "    - **Regression**\n",
    "    - **Predictive Clustering**\n",
    "\n",
    "  - **B. Descriptive Tasks (Unsupervised Learning: Help find hidden patterns)**\n",
    "    - **Descriptive Clustering** → Groups data into clusters which is not known beforehand.\n",
    "      - Example: Customer segmentation, plant species grouping.\n",
    "    - **Association Rule Mining** → Identifies relationships between variables.  \n",
    "      - Example: Market basket analysis (if a person buys bread, they might buy butter).\n",
    "    - **Subgroup Discovery** → Finds interesting patterns within a specific class of data.  \n",
    "      - Example: Detecting risk groups for diseases.\n",
    "\n",
    "Rule mining vs subgroup discovery:\n",
    "- **Rule Mining** → Generalizes rules across the entire dataset.\n",
    "- **Subgroup Discovery** → Focuses on specific subgroups within the data.\n",
    "\n",
    "\n",
    "|                   | *Predictive model*                | *Descriptive model*                          |\n",
    "|-------------------|---------------------------------|---------------------------------------------|\n",
    "| *Supervised learning*   | Classification, Regression  | Subgroup discovery                         |\n",
    "| *Unsupervised learning* | Predictive clustering      | Descriptive clustering, Association rule discovery |\n",
    "\n",
    "\n",
    "  - **A. Model Categorization by Intuition**\n",
    "    - **Geometric Models** → Use distances, transformations, and hyperplanes.\n",
    "      - Example: **Linear regression, SVM**.\n",
    "    - **Probabilistic Models** → Use probability distributions to reduce uncertainty.\n",
    "      - Example: **Naïve Bayes**.\n",
    "    - **Logical Models** → Use interpretable rules for decision-making.\n",
    "      - Example: **Decision Trees**.\n",
    "\n",
    "  - **B. Model Categorization by Mode of Operation (modus operandi)**\n",
    "    - **Grouping Models** → Divide instance space into segments.  \n",
    "      - Example: **Decision Trees**.\n",
    "    - **Grading Models** → Learn a single global function over all instances.  \n",
    "      - Example: **SVM, Linear Regression**.\n",
    "\n",
    "  - **C. Model Phases**\n",
    "    - **Training Phase** → The model learns from data.\n",
    "    - **Inference Phase** → The trained model makes predictions on new data.\n",
    "    - **Training takes longer than inference** due to learning complexity.\n",
    "\n",
    "- **Features**\n",
    "  - **Feature Engineering**\n",
    "    - **Feature Construction** → Creating features from raw data.\n",
    "    - **Discretization** → Converting numerical data into categorical bins.\n",
    "    - **Feature Transformation** → Mapping data into a new space (e.g., PCA).\n",
    "    - **Feature Selection** → Removing redundant or irrelevant features.\n",
    "\n",
    "  - **Feature Extraction for Different Data Types**\n",
    "    - **Images** → Textures, color distribution.\n",
    "    - **Text** → Word frequencies, TF-IDF.\n",
    "    - **Time Series** → Maximum, minimum, trends.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
