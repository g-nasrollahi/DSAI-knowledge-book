{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9b9e1a",
   "metadata": {},
   "source": [
    "### Core Components of AutoML:\n",
    "\n",
    "1. **Search Algorithm:**\n",
    "\n",
    "   * The method used to explore different models and hyperparameters.\n",
    "   * Examples:\n",
    "\n",
    "     * **Random/Grid Search:** Try many options randomly or systematically.\n",
    "     * **Bayesian Optimization:** Uses past results to pick promising options next.\n",
    "     * **Hyperband:** Efficiently allocates resources to many configurations and quickly drops bad ones.\n",
    "\n",
    "2. **Search Space:**\n",
    "\n",
    "   * All the possible models and hyperparameter settings AutoML can try.\n",
    "   * Includes ranges for hyperparameters (like learning rate, number of layers) and different model types.\n",
    "\n",
    "3. **Evaluation Mechanism:**\n",
    "\n",
    "   * How the AutoML system measures model quality.\n",
    "   * Usually uses train/validation/test splits or cross-validation to evaluate performance.\n",
    "   * May use methods like multi-armed bandits or learning curves to decide which models to test more.\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Problems in AutoML:\n",
    "\n",
    "* **Algorithm Selection Problem:** Pick the best model for the dataset.\n",
    "* **Hyperparameter Optimization Problem (HPO):** For a chosen model, find the best hyperparameters.\n",
    "* **Combined Algorithm Selection and Hyperparameter Optimization (CASH):** Find the best model **and** its best hyperparameters together.\n",
    "* Other advanced problems: Neural Architecture Search (NAS), Few-shot Learning.\n",
    "\n",
    "\n",
    "### Common Search Algorithms:\n",
    "\n",
    "* **Grid Search:** Simple but does not scale well with many parameters.\n",
    "* **Random Search:** Tries random points; better than grid for high dimensions, easy to parallelize.\n",
    "* **Bayesian Optimization:** Builds a probabilistic model of the function and uses it to select promising hyperparameters to try next. More data efficient but harder to parallelize.\n",
    "\n",
    "---\n",
    "\n",
    "### How Bayesian Optimization Works:\n",
    "\n",
    "1. Fit a surrogate probabilistic model (e.g., Gaussian Process) on observed points.\n",
    "2. Use an **acquisition function** (like Expected Improvement) to decide which hyperparameters to test next.\n",
    "3. Evaluate the model with those hyperparameters.\n",
    "4. Update the surrogate model with the new result.\n",
    "5. Repeat.\n",
    "\n",
    "---\n",
    "\n",
    "### Search Space Details:\n",
    "\n",
    "* Hyperparameters can be:\n",
    "\n",
    "  * **Categorical:** Choices like activation function (\"relu\", \"sigmoid\").\n",
    "  * **Numerical:** Continuous (learning rate) or integer (number of layers).\n",
    "  * **Conditional:** Some hyperparameters only matter if others have certain values (e.g., \"beta1\" only if using Adam optimizer).\n",
    "* Sampling strategies decide how to pick values in the search space.\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-Armed Bandits & Successive Halving:\n",
    "\n",
    "* Treat each model/hyperparameter setting as a slot machine arm to pull.\n",
    "* **Successive Halving:** Start with many candidates, test briefly, drop the worst half, test remaining longer, repeat until one winner remains.\n",
    "* Used to save time by not fully training poor configurations.\n",
    "\n",
    "---\n",
    "\n",
    "### Extensions:\n",
    "\n",
    "* **Hyperband:** An improved version of Successive Halving with better budget allocation.\n",
    "* Combining Bayesian optimization with bandit methods improves efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Curve Models:\n",
    "\n",
    "* Predict how well a model will perform if trained longer, allowing early stopping of bad configurations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
