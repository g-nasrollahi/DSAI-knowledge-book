{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d17368d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### What are Tree Models?\n",
    "\n",
    "* Tree models = **supervised learning** = *non-linear relationships** (they split data based on rules, not lines) = logical = **easy to interpret**\n",
    "* There are three main types:\n",
    "\n",
    "  1. **Decision Trees** (single tree)\n",
    "  2. **Random Forests** (many decision trees combined)\n",
    "  3. **Gradient Boosting** (trees built sequentially to correct errors)\n",
    "\n",
    "\n",
    "## Decision Trees\n",
    "\n",
    "* It’s a **tree-like graph** where:\n",
    "\n",
    "  * **Nodes**: Questions about features (e.g., “Is temperature > 20?”)\n",
    "  * **Branches**: Answers to the question (e.g., Yes/No)\n",
    "  * **Leaves**: Final prediction/class label\n",
    "\n",
    "### Elements of a Decision Tree\n",
    "\n",
    "![Decision Tree Example](../../Files/fourth-semester/ml/4.png)\n",
    "\n",
    "\n",
    "### Max Depth and Number of Leaves in Decision Trees\n",
    "\n",
    "* If there are **d binary features** (features with only 2 values like 0 or 1),\n",
    "\n",
    "  * The max depth of the tree is **d + 1**\n",
    "  * Max number of leaves is **2^d** (all combinations of feature values)\n",
    "\n",
    "* We want to split data so each child group is as **pure** as possible (mostly one class).\n",
    "* Two types of splits:\n",
    "\n",
    "  * **Pure split**: each child has only one class (ideal)\n",
    "  * **Impure split**: children have mixed classes (common in real life)\n",
    "\n",
    "### Measuring Purity (Impurity)\n",
    "\n",
    "* **Impurity** measures how mixed the classes are in a node.\n",
    "* Lower impurity = better split.\n",
    "* Common impurity measures:\n",
    "\n",
    "  * **Misclassification error:** min(p, 1-p), where p is the fraction of one class\n",
    "  * **Entropy:** measures uncertainty, calculated as:\n",
    "\n",
    "    $$\n",
    "    -p \\log_2 p - (1-p) \\log_2 (1-p)\n",
    "    $$\n",
    "  * **Gini index:** measures probability of misclassification, calculated as:\n",
    "\n",
    "    $$\n",
    "    2 p (1 - p)\n",
    "    $$\n",
    "\n",
    "\n",
    "### Comparing Splits Using Impurity\n",
    "\n",
    "* Calculate weighted impurity for children after the split.\n",
    "* Choose the split that **reduces impurity the most** (called **purity gain** or **information gain**):\n",
    "\n",
    "  $$\n",
    "  \\text{Purity Gain} = \\text{Impurity before split} - \\text{Weighted impurity after split}\n",
    "  $$\n",
    "* The split with the **highest purity gain** is the best split.\n",
    "\n",
    "### For Handling Continuous Features, we check impurity for each possible threshold and pick the best.\n",
    "\n",
    "---\n",
    "\n",
    "## Pruning Decision Trees\n",
    "\n",
    "* Trees can become very big and memorize training data (overfit).\n",
    "* **Pruning** reduces tree size by removing weak branches.\n",
    "* **Reduced error pruning:**\n",
    "\n",
    "  * Start from leaves, replace a node with majority class label.\n",
    "  * Keep the change only if validation accuracy does not drop.\n",
    "* Pruning helps the tree generalize better to new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dcaefc",
   "metadata": {},
   "source": [
    "\n",
    "## Sensitivity to Skewed Class Distributions\n",
    "\n",
    "* When classes are imbalanced (one class much bigger), trees may be biased.\n",
    "* For example, positive class errors may be more costly.\n",
    "* Solutions:\n",
    "\n",
    "  * Add more samples to minority class (oversampling)\n",
    "  * Adjust impurity calculations or splitting criteria to be less sensitive to class imbalance\n",
    "* Different impurity measures respond differently to class imbalance:\n",
    "> **Note:** Entropy and Gini index are sensitive to fluctuations in the class distribution, pGini ($\\sqrt{\\text{Gini}}$) isn’t.\n",
    "\n",
    "---\n",
    "\n",
    "## Regression Trees\n",
    "\n",
    "### What is a Regression Tree?\n",
    "\n",
    "* Decision trees can also be used when the target is a **continuous numeric value** (not a class).\n",
    "\n",
    "* Instead of class labels, leaf nodes hold **mean values** of the target variable in that subset.\n",
    "* Splitting tries to minimize the **variance** of target values in child nodes.\n",
    "\n",
    "### Measuring Impurity in Regression\n",
    "\n",
    "* Use **variance** (average squared difference from mean) instead of class impurity:\n",
    "\n",
    "  $$\n",
    "  \\text{Var}(Y) = \\frac{1}{|Y|} \\sum_{y \\in Y} (y - \\bar{y})^2\n",
    "  $$\n",
    "* Find splits that **reduce variance** the most (variance reduction).\n",
    "\n",
    "\n",
    "### Finding Splits in Regression Trees\n",
    "\n",
    "* Try different thresholds on each feature.\n",
    "* Calculate weighted variance after splitting.\n",
    "* Choose the split with **lowest weighted variance**.\n",
    "\n",
    "![](https://www.solver.com/sites/default/files/RT_Output_MinErrorTree.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
