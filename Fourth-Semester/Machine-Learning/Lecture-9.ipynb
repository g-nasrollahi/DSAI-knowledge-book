{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc7079b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "Concept: Wisdom of Crowds & Condorcet’s Jury Theorem\n",
    "\n",
    "P is the probability that a single voter (or model) is correct.\n",
    "- If p > 0.5, adding more voters/models improves accuracy.\n",
    "- If p < 0.5, adding more voters/models makes accuracy worse.\n",
    "\n",
    "* Combines multiple models to improve predictions where each model is trained on slightly different data (re-weighted or resampled).\n",
    "\n",
    "<img src=\"../../Files/fourth-semester/ml/13.png\" alt=\"Ensemble Learning\" width=\"400\"/>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Bootstrapping\n",
    "\n",
    "* Train and test many times on these random samples, and samples can repeat in each training set. This process helps reduce mistakes caused by just one fixed training set.\n",
    "\n",
    "### Bagging (Bootstrap Aggregating)\n",
    "\n",
    "* Steps:\n",
    "\n",
    "  1. Create multiple bootstrap samples from the original data.\n",
    "  2. Train separate models (learners) on each sample. (Bootstrap)\n",
    "  3. Combine their predictions by voting or averaging. (Aggregate)\n",
    "* This reduces variance (makes model more stable).\n",
    "\n",
    "### Subspace Sampling\n",
    "\n",
    "* Instead of using all features, you randomly pick only a small set of features for each model. Different models look at different parts of the data, so their errors won’t be the same. This helps the combined model (ensemble) perform better and avoid overfitting.\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "* Bagging + random feature selection for trees.\n",
    "* Train many decision trees on different bootstrap samples and random feature subsets.\n",
    "* Predict by majority vote (classification) or average (regression).\n",
    "\n",
    "<img src=\"https://anasbrital98.github.io/assets/img/20/random-forest.jpg\" alt=\"Random Forest\" width=\"400px\"/>\n",
    "\n",
    "---\n",
    "\n",
    "### Bagging vs Boosting\n",
    "\n",
    "* Bagging: models are independent, combine by voting, reduces variance.\n",
    "* Boosting: models build sequentially, each model learns from errors of previous, reduces bias.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1153/1*XzaoQxMf4uLD5DIHz8JwbA.png)\n",
    "\n",
    "### Weight Assignment in Boosting\n",
    "\n",
    "* Initially, all instances have equal weights summing to 1.\n",
    "* After each round, misclassified points get higher weight, correctly classified get lower weight, so model focuses on hard cases.\n",
    "* Example: if error rate is ϵ, weight for misclassified = 1/(2ϵ), for correctly classified = 1/(2(1−ϵ)).\n",
    "\n",
    "### Boosting Models\n",
    "\n",
    "- XGBoost (Extreme Gradient Boosting): Gradient Boosting with regularization.\n",
    "- LightGBM: Gradient Boosting with histogram-based learning.\n",
    "- AdaBoost: Boosting with adaptive weights.\n",
    "- Gradient Boosting: Boosting with gradient descent optimization.\n",
    "---\n",
    "\n",
    "## Bias and Variance\n",
    "\n",
    "### Definitions\n",
    "\n",
    "* **Bias:** Error from wrong assumptions in the learning algorithm (systematic error).\n",
    "* **Variance:** Error from sensitivity to small fluctuations in the training set (model changes a lot with different data).\n",
    "* Total expected error = Bias² + Variance + Irreducible error.\n",
    "\n",
    "### Examples\n",
    "\n",
    "* High bias: model is too simple (e.g., linear model on non-linear data).\n",
    "* High variance: model fits training data too closely and changes a lot with different samples (e.g., decision trees).\n",
    "\n",
    "### Role of Bagging and Boosting\n",
    "\n",
    "* Bagging mainly reduces variance (good for high-variance models like trees).\n",
    "* Boosting mainly reduces bias (good for high-bias models like weak linear classifiers).\n",
    "\n",
    "### Beyond Bagging and Boosting\n",
    "\n",
    "* **Stacking:** Train multiple base models (e.g., decision trees, SVMs) and combine their predictions using a meta-model (e.g., logistic regression).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5f863",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "* Split data into k parts (folds).\n",
    "* Train on k-1 folds, test on the 1 remaining fold. Repeat k times (each fold used once for testing).\n",
    "* Gives average performance and variance estimate.\n",
    "\n",
    "#### Leave-One-Out Cross-Validation (LOOCV)\n",
    "\n",
    "LOOCV is a special case of k-fold cross-validation where each fold contains a single instance, so the model is trained on all data except one point and tested on that point, repeating for every data point—providing high accuracy but at significant computational cost for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### P-Values:\n",
    "\n",
    "- Null Hypothesis (H0): Says no difference or no relationship exists.\n",
    "- Alternative Hypothesis (H1): Says there is a difference or relationship.\n",
    "- P-value: Probability of observing the data if H0 is true. \n",
    "- If p-value < α (significance level, like 0.05), it means the results are unlikely to be just by chance. So, you usually say the result is statistically significant.\n",
    "Sure! Here’s a clearer and simpler explanation of your notes about these tests:\n",
    "\n",
    "\n",
    "### **t-test**\n",
    "\n",
    "* A t-test checks if the difference between two groups' averages (means) is real or just by chance.\n",
    "* It helps you know if two things really behave differently.\n",
    "\n",
    "There are two main types:\n",
    "\n",
    "1. **Independent t-test:** Compare two separate groups (like comparing scores of men vs women).\n",
    "2. **Paired t-test:** Compare the same group at two times (like before and after treatment).\n",
    "\n",
    "\n",
    "### Comparing Algorithms on Multiple Datasets — Why t-test doesn’t work well\n",
    "\n",
    "* Scores from different datasets can’t be compared directly because datasets can be very different.\n",
    "* So, t-test is not good when you want to compare algorithm performance over many datasets.\n",
    "\n",
    "\n",
    "### Wilcoxon Signed-Rank Test\n",
    "\n",
    "* This is a better test for comparing two algorithms on many datasets.\n",
    "* Steps:\n",
    "\n",
    "  1. For each dataset, find the difference in performance between two algorithms.\n",
    "  2. Take the absolute values of these differences and rank them from smallest to largest.\n",
    "  3. Sum the ranks separately for positive and negative differences.\n",
    "  4. Use the smaller sum as the test statistic.\n",
    "* If the test statistic is below a certain critical value, you can say the algorithms perform differently (reject null hypothesis that they are equal).\n",
    "\n",
    "\n",
    "### Friedman Test\n",
    "\n",
    "* Used when you want to compare **more than two algorithms** over multiple datasets.\n",
    "* Steps:\n",
    "\n",
    "  1. Rank the algorithms on each dataset (1 for best, k for worst).\n",
    "  2. Calculate the average rank for each algorithm across all datasets.\n",
    "  3. Check if the differences in average ranks are big enough to say the algorithms perform differently (test the null hypothesis that they all perform equally).\n",
    "\n",
    "### Post-hoc Tests (like Nemenyi Test)\n",
    "\n",
    "* After Friedman test shows there is a difference, you want to know **which pairs** of algorithms are different.\n",
    "* Calculate a **Critical Difference (CD)** value.\n",
    "* If the difference between the average ranks of two algorithms is greater than CD, their performance difference is significant.\n",
    "\n",
    "\n",
    "### Critical Difference Diagram\n",
    "\n",
    "* A simple visual to show which algorithms differ significantly.\n",
    "* Algorithms are placed on a line based on their average ranks.\n",
    "* Groups of algorithms connected by a horizontal line mean their performances are **not significantly different**.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/9594042/126643854-43725afa-e8a6-4fe1-b26f-020eb53c5c2b.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
