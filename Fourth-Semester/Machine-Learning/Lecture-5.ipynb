{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c928d91a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Distance-based Models\n",
    "\n",
    "* **Examplars** are the centroid training data points used to make predictions (representative points of clusters)\n",
    "* The closest exemplars have the biggest influence on deciding the class of the new instance.\n",
    "\n",
    "\n",
    "### Measure distance\n",
    "\n",
    "#### Minkowski Distance (p-norm)\n",
    "\n",
    "For two points $x$ and $y$ with $d$ features:\n",
    "\n",
    "$$\n",
    "Dis_p(x, y) = \\left( \\sum_{j=1}^d |x_j - y_j|^p \\right)^{1/p}\n",
    "$$\n",
    "\n",
    "* $p$ controls the type of distance.\n",
    "* If $p=1$, this is **Manhattan distance**.\n",
    "* If $p=2$, this is **Euclidean distance**.\n",
    "* As $p \\to \\infty$, the distance becomes **Chebyshev distance** (For maximum difference along any coordinate).\n",
    "\n",
    "Changing $p$ lets us adjust how we measure \"closeness\" between points. Different values of $p$ capture different notions of distance, which can be more suitable for certain data types or problems. By choosing the right $p$, we can help the algorithm perform better for the specific data or task at hand.\n",
    "\n",
    "### What makes a **distance metric** valid?\n",
    "\n",
    "A distance metric $Dis$ must satisfy:\n",
    "\n",
    "1. **Zero distance to itself:** $Dis(x, x) = 0$\n",
    "2. **Positive for different points:** $Dis(x, y) > 0$ if $x \\neq y$\n",
    "3. **Symmetry:** $Dis(x, y) = Dis(y, x)$\n",
    "4. **Triangle inequality:** $Dis(x, z) \\leq Dis(x, y) + Dis(y, z)$\n",
    "\n",
    "If the 2nd condition allows zero even if points differ, it's called a **pseudo-metric**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a06db",
   "metadata": {},
   "source": [
    "## Nearest-Neighbour Classification (KNN)\n",
    "\n",
    "  * **Centroids**: average (mean) point of cluster (may not be an actual data point).\n",
    "  * **Medoids**: the actual data point closest to all others in the cluster. (time consuming to calculate)\n",
    "\n",
    "\n",
    "![image](https://machinelearningknowledge.ai/wp-content/uploads/2018/08/KNN-Classification.gif)\n",
    "\n",
    "* Each training instance acts as an exemplar.\n",
    "* To classify a new point, find the **k nearest training points**.\n",
    "* The class with the majority vote wins.\n",
    "* Can also do regression by averaging the target values of neighbours.\n",
    "\n",
    "Hot to choose k?\n",
    "* **Small k**: Low bias, High variance (sensitive to noise) ,overfitting\n",
    "* **Large k**: High bias, Low variance (smoother decision boundary), underfitting\n",
    "* **Optimal k**: usually between 3 and 10.\n",
    "* Use cross-validation to find the best k for your data.\n",
    "* **Odd k**: avoids ties in voting.\n",
    "* **Weighted voting**: give more weight to closer neighbours.\n",
    "\n",
    "\n",
    "### Curse of Dimensionality\n",
    "\n",
    "In high-dimensional spaces, distances between points become less meaningful, making distance-based methods like KNN less effective.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f61fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "## K-means Algorithm (Distance-based Predictive Clustering)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif\" width=\"300px\"/>\n",
    "\n",
    "1. Randomly initialize K centroids.\n",
    "2. Assign each point to the nearest centroid.\n",
    "3. Update centroids to be the mean of assigned points.\n",
    "4. Repeat steps 2–3 until centroids don’t change.\n",
    "\n",
    "<img src=\"https://communities.sas.com/t5/image/serverpage/image-id/64985i87AA648B48FF3F50?v=v2\" width=\"300px\"/>\n",
    "\n",
    "### Limitations of K-means\n",
    "\n",
    "* Sensitive to initial centroids.\n",
    "* Need to know K beforehand.\n",
    "* Uses Euclidean distance.\n",
    "\n",
    "---\n",
    "\n",
    "## K-medoids Algorithm\n",
    "\n",
    "* Works with any distance metric.\n",
    "* More robust to noise/outliers but more expensive computationally (O(n²)).\n",
    "1. Pick K random points as medoids.\n",
    "2. Assign points to closest medoid.\n",
    "3. Update medoids to minimize total distance within cluster.\n",
    "4. Repeat until medoids stabilize.\n",
    "\n",
    "\n",
    "## Evaluating Clustering\n",
    "\n",
    "* **Inertia:** total within-cluster scatter distance from the centroid.\n",
    "  Formula:\n",
    "$$\n",
    "Inertia = \\sum_{i=1}^K \\sum_{x \\in C_i} Dis(x, c_i)\n",
    "$$\n",
    "  where $C_i$ is the set of points in cluster $i$ and $c_i$ is the centroid of cluster $i$.\n",
    "\n",
    "  * Lower inertia = tighter clusters.\n",
    "* **Silhouette score:** measures how similar a point is to its own cluster vs the next closest cluster.\n",
    "\n",
    "  * Range from -1 to 1.\n",
    "  * High silhouette means good clustering.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee66648",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "## Descriptive Hierarchical Clustering\n",
    "\n",
    "* Instead of flat clusters (like K-means), build a hierarchy/tree of clusters.\n",
    "* Clusters merge step by step, forming a tree called a **dendrogram**.\n",
    "\n",
    "![](../../Files/fourth-semester/ml/5.png)\n",
    "\n",
    "* Leaves are data points; internal nodes represent merged clusters.\n",
    "* Height of nodes shows the distance at which clusters merged.\n",
    "\n",
    "---\n",
    "\n",
    "### Linkage Functions (how to measure distance between clusters)\n",
    "\n",
    "* **Single linkage:** minimum distance between points in two clusters.\n",
    "* **Complete linkage:** maximum distance between points in two clusters.\n",
    "* **Average linkage:** average distance between points in two clusters.\n",
    "* **Centroid linkage:** distance between cluster centroids.\n",
    "\n",
    "![](../../Files/fourth-semester/ml/6.png)\n",
    "\n",
    "![](../../Files/fourth-semester/ml/7.png)\n",
    "\n",
    "Different linkage methods produce different dendrogram shapes.\n",
    "\n",
    "---\n",
    "\n",
    "### Hierarchical Agglomerative Clustering (HAC) Algorithm\n",
    "\n",
    "1. Start with each point as its own cluster.\n",
    "2. Find the two closest clusters (according to linkage).\n",
    "3. Merge them.\n",
    "4. Repeat until one cluster remains.\n",
    "5. Result: dendrogram representing cluster hierarchy.\n",
    "\n",
    "---\n",
    "\n",
    "### Problems with Hierarchical Clustering\n",
    "\n",
    "* Choice of linkage affects results.\n",
    "* Sometimes clusters suggested by dendrogram don’t match real data structure (spurious clusters).\n",
    "* Silhouette scores can help check cluster quality."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
