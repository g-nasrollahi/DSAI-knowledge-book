{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81d6c0c",
   "metadata": {},
   "source": [
    "\n",
    "* **Statistics of central tendency:** Describe the center of data\n",
    "\n",
    "  * **Mean**, **Median**, **Mode:** Average, middle value, most common value.\n",
    "\n",
    "* **Statistics of dispersion:** Describe spread of data\n",
    "\n",
    "  * **Variance**, **Standard deviation**\n",
    "  * **Range:** Difference between max and min.\n",
    "  * **Midrange:** Average of max and min.\n",
    "  * **Quantiles:** Points dividing data into percentages (percentiles, quartiles, deciles).\n",
    "  * **Interquartile range:** Difference between 3rd and 1st quartile.\n",
    "\n",
    "* **Shape statistics:** Describe the shape of the data distribution\n",
    "\n",
    "  * **Skewness:** Measures if data is lopsided. Positive means right-skewed (long tail on right), negative means left-skewed.\n",
    "  * **Kurtosis:** Measures how sharp or flat the peak is compared to a normal distribution.\n",
    "\n",
    "![](../../Files/fourth-semester/ml/11.png)\n",
    "---\n",
    "\n",
    "\n",
    "* Some models treat feature types differently:\n",
    "\n",
    "  * Decision trees handle categorical and continuous differently.\n",
    "  * Naïve Bayes works only with categorical features.\n",
    "  * Distance-based models (like K-means) need quantitative or ordinal features.\n",
    "\n",
    "\n",
    "- Dealing with Missing Values = **Imputation** with Mean, Regression, or Expectation Maximisation (statistical model).\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Structured Features\n",
    "\n",
    "* Structured features capture complex info, like word counts or phrase presence in texts.\n",
    "* For example, in spam detection:\n",
    "\n",
    "  * Features could be word frequencies.\n",
    "  * Or whether a phrase like “machine learning” appears.\n",
    "* Features can also represent relations, such as if one email quotes another.\n",
    "\n",
    "\n",
    "## Feature Transformations\n",
    "\n",
    "Transform features to make them easier to use or improve models.\n",
    "\n",
    "| From \\ To    | Quantitative               | Ordinal      | Categorical  | Boolean     |\n",
    "| ------------ | -------------------------- | ------------ | ------------ | ----------- |\n",
    "| Quantitative | Normalisation, Calibration | Calibration  | Calibration  | Calibration |\n",
    "| Ordinal      | Discretisation             | Ordering     | Ordering     | Ordering    |\n",
    "| Categorical  | Discretisation             | Unordering   | Grouping     | -           |\n",
    "| Boolean      | Thresholding               | Thresholding | Binarisation | -           |\n",
    "\n",
    "* **Calibration:** Assigns values to categorical data but can wrongly give more importance to some categories.\n",
    "* **Thresholding:** Converts numeric/ordinal features into boolean by splitting at a threshold value.\n",
    "* **Discretisation:** Converts quantitative features into ordinal by grouping continuous values into bins.\n",
    "\n",
    "### Normalisation and Calibration\n",
    "\n",
    "* **Normalisation:** Make sure all numeric features are on the same scale.\n",
    "\n",
    "  * **Min-Max normalisation:** Scales data between 0 and 1.\n",
    "  * **Z-score normalisation:** Centers data around mean 0 and standard deviation 1.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Construction and Selection\n",
    "\n",
    "### 4.1 Principal Component Analysis (PCA)\n",
    "\n",
    "* PCA creates new features (principal components) by combining original features.\n",
    "* It finds directions (components) where data varies the most.\n",
    "* First principal component (PC1) explains the most variation.\n",
    "* Second component (PC2) is orthogonal (at right angles) to PC1 and explains the next most variation.\n",
    "* PCA helps reduce redundancy when features are correlated.\n",
    "\n",
    "Process:\n",
    "\n",
    "1. Find the mean of data.\n",
    "2. Center data by subtracting mean.\n",
    "3. Find principal components as linear combinations of features.\n",
    "4. Eigenvalues measure importance of each component (variance explained).\n",
    "5. Eigenvectors give direction of components.\n",
    "\n",
    "Methods to compute PCA:\n",
    "\n",
    "* **Singular Value Decomposition (SVD)**\n",
    "* **Eigenvalue decomposition**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
