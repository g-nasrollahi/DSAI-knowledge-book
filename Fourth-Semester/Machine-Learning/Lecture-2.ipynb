{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b67286c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Assessing Classification Performance\n",
    "\n",
    "### Contingency Table (Confusion Matrix)\n",
    "\n",
    "This table compares actual labels and predicted labels. For two classes (positive/negative), it looks like:\n",
    "\n",
    "|                 | Predicted Positive  | Predicted Negative  | Total Actual |\n",
    "| --------------- | ------------------- | ------------------- | ------------ |\n",
    "| Actual Positive | True Positive (TP)  | False Negative (FN) | P            |\n",
    "| Actual Negative | False Positive (FP) | True Negative (TN)  | N            |\n",
    "\n",
    "* **True Positive (TP):** Correctly predicted positive\n",
    "* **False Negative (FN):** Missed positive (predicted negative)\n",
    "* **False Positive (FP):** Incorrectly predicted positive\n",
    "* **True Negative (TN):** Correctly predicted negative\n",
    "\n",
    "### Accuracy and Error Rate\n",
    "\n",
    "* **Accuracy:** Fraction of correct predictions\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{P + N}\n",
    "$$\n",
    "\n",
    "* **Error rate:** Fraction of wrong predictions\n",
    "\n",
    "$$\n",
    "\\text{Error} = 1 - \\text{Accuracy} = \\frac{FP + FN}{P + N}\n",
    "$$\n",
    "\n",
    "### True Positive Rate (Recall or Sensitivity)\n",
    "\n",
    "* Measures how many actual positives are correctly predicted:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "### True Negative Rate (Specificity)\n",
    "\n",
    "* Measures how many actual negatives are correctly predicted:\n",
    "\n",
    "$$\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "### Precision\n",
    "\n",
    "* Out of all predicted positives, how many are actually positive?\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "\n",
    "### F-measure (F1 Score)\n",
    "\n",
    "is a performance metric that is not affected by negatives, as a weighted harmonic mean of precision and recall.\n",
    "\n",
    "$$\n",
    "F_1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "* Useful when you care most about positive class performance.\n",
    "\n",
    "---\n",
    "\n",
    "# Evaluating Model Performance\n",
    "\n",
    "### Train/Test Split\n",
    "\n",
    "* Split data into training set (to learn model) and test set (to check how well it works on new data).\n",
    "* Test set must be unseen during training to get a fair performance measure.\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "* Split data into several parts (\"folds\"), train on some folds, test on others, then average the results.\n",
    "* Gives a better estimate of performance and reduces bias.\n",
    "\n",
    "![Cross-validation example](../../Files/fourth-semester/ml/1.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26080db",
   "metadata": {},
   "source": [
    "# 2.2 Visualising Classification Performance\n",
    "\n",
    "### Decision Threshold τ\n",
    "\n",
    "* Classifier outputs a score (like probability).\n",
    "* If score > τ, predict positive; else negative.\n",
    "* Changing τ changes confusion matrix and performance metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### ROC Curve (Receiver Operating Characteristic)\n",
    "\n",
    "* Plot of **True Positive Rate (Sensitivity: y-axis)** vs **False Positive Rate (1 - Specificity: x-axis)**.\n",
    "* Shows a summary of confusion matrices for different thresholds.\n",
    "* Best threshold using ROC curve is where the curve is closest to the top left corner (0,1).\n",
    "- A coverage plot and a Receiver Operation Curve (ROC) (normalized coverage plot) both help summarise all different confusion matrices.\n",
    "\n",
    "### Area Under the Curve (AUC)\n",
    "\n",
    "* AUC measures the performance of a classifier across all thresholds.\n",
    "* If AUC for a model is higher than another, it means the first model is better at distinguishing between classes. \n",
    "---\n",
    "\n",
    "# 3. Scoring and Ranking\n",
    "\n",
    "### Scoring Classifier\n",
    "\n",
    "* Outputs scores (not just labels) showing confidence for each class.\n",
    "* For binary classes, usually one score (positive class score).\n",
    "\n",
    "### Margin\n",
    "\n",
    "* For an example x with true class c(x) (+1 or −1), margin is:\n",
    "\n",
    "$$\n",
    "z(x) = c(x) \\times \\hat{s}(x)\n",
    "$$\n",
    "\n",
    "* z(x) is the margin score, c(x) is the true class label (+1 for positive, -1 for negative), and $\\hat{s}(x)$ is the score from the classifier.\n",
    "* Positive margin means correct prediction; negative means incorrect.\n",
    "\n",
    "![Margin example](../../Files/fourth-semester/ml/2.png)\n",
    "---\n",
    "\n",
    "### Loss Functions\n",
    "\n",
    "Loss functions measure how bad predictions are and guide model training by minimizing loss.\n",
    "\n",
    "| Loss Type        | Description                                         |\n",
    "| ---------------- | --------------------------------------------------- |\n",
    "| 0–1 Loss         | 1 if wrong prediction, 0 if correct                 |\n",
    "| Hinge Loss       | Penalizes predictions that are not confident enough |\n",
    "| Exponential Loss | Penalizes wrong predictions exponentially           |\n",
    "| Logistic Loss    | Smooth loss function related to probability         |\n",
    "| Squared Loss     | Penalizes squared error of margin                   |\n",
    "\n",
    "---\n",
    "\n",
    "# 3.1 Assessing Ranking Performance\n",
    "\n",
    "### Ranking Error\n",
    "\n",
    "* Measures how often a positive example is scored lower than a negative one.\n",
    "* Lower ranking error means better ranking quality.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Class Probability Estimation\n",
    "\n",
    "### What is it?\n",
    "\n",
    "* Instead of just giving labels or scores, estimate probabilities for each class.\n",
    "* Probabilities sum to 1. For two classes:\n",
    "\n",
    "$$\n",
    "p(\\text{positive}) + p(\\text{negative}) = 1\n",
    "$$\n",
    "\n",
    "### How?\n",
    "\n",
    "* Some models like decision trees estimate class probabilities by the proportion of class examples in leaves.\n",
    "\n",
    "---\n",
    "\n",
    "# 4.1 Assessing Class Probability Estimates\n",
    "\n",
    "### Mean Squared Error (MSE) for probabilities\n",
    "\n",
    "Measures how close predicted probabilities are to true class labels:\n",
    "\n",
    "$$\n",
    "SE(x) = \\frac{1}{2} \\sum_i (\\hat{p}_i(x) - I[c(x)=C_i])^2\n",
    "$$\n",
    "\n",
    "* Average squared error over all examples is the MSE.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary of Concepts Covered:\n",
    "\n",
    "* **Classification:** Predicting labels, evaluating with confusion matrix and metrics like accuracy, precision, recall, F1.\n",
    "* **Performance evaluation:** Use train/test splits, cross-validation, watch out for over/underfitting.\n",
    "* **Visualization:** ROC curve helps understand classifier trade-offs at different thresholds.\n",
    "* **Scoring & ranking:** Models can output scores, which allows ranking instances by confidence.\n",
    "* **Loss functions:** Guide model training by penalizing wrong or uncertain predictions.\n",
    "* **Probability estimation:** Provides probabilities for classes and can be assessed with mean squared error.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
