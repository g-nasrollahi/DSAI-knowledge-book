{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08cd4989",
   "metadata": {},
   "source": [
    "- **Value Function Approximation**: Instead of storing $v_\\pi(s)$ for each state in large scale of state-actions, use a function $\\hat{v}(s, w)$ with parameters $w$ to approximate the value function.\n",
    "  - **$w$ approximation**: $w$  could be weights in a linear function or parameters of a neural network.\n",
    "  - **Goal**: Find $w$ such that $\\hat{v}(s, w)$ is close to $v_\\pi(s)$ for all states $s$.\n",
    "    - Define an error function: $VE(w) = \\sum_s \\mu(s) \\left[v_\\pi(s) - \\hat{v}(s, w)\\right]^2$\n",
    "      - Use **gradient descent** to reduce this error by updating $w$:\n",
    "        - $ w \\leftarrow w - \\alpha \\nabla VE(w)$\n",
    "    - Use **sample estimates** instead from experience:\n",
    "      - **Monte Carlo (MC):** Use full return $G_t$ after an episode.\n",
    "        - **Gradient Monte Carlo:** Adjust parameters so the estimated value $\\hat{v}$ moves closer to the observed return $G_t$.\n",
    "          - $ w \\leftarrow w + \\alpha (G_t - \\hat{v}(S_t, w)) \\nabla \\hat{v}(S_t, w) $\n",
    "          - where $G_t$ id the total return from time $t$ like $G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + ...$.\n",
    "      - **Temporal Difference (TD):** Use current reward plus estimated value of next state. \n",
    "        - **Gradient TD(0) Algorithm**: Use one-step TD target instead of full return:\n",
    "          - $ w \\leftarrow w + \\alpha \\big(R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w)\\big) \\nabla \\hat{v}(S_t, w) $\n",
    "          - Update happens after every step, not after full episode. (More efficient and online)\n",
    "  - **Approximating Values** $\\hat{v}(s, w)$: We canâ€™t store values for all states, so we use a **function** with parameters $w$ to guess values. Types:\n",
    "    - 1) **State Aggregation**: Group similar states and assign one value to each group.\n",
    "    - 2) **Linear Approximation**: Use features $x(s)$ of the state to compute value as a weighted sum:\n",
    "      - $\\hat{v}(s, w) = w_1 x_1(s) + w_2 x_2(s) + \\cdots $\n",
    "    - **Feature Types**: \n",
    "      - **Polynomial**: Use powers of state variables (e.g., $s, s^2, s^3$).\n",
    "      - **Fourier**: Use sine/cosine functions to represent states.\n",
    "      - **Coarse Coding**: Divide state space into overlapping regions; feature is 1 if state in region, else 0.\n",
    "  - Control with **Action-Value Function Approximation**: Learn optimal policy by approximating action-value function $q_*(s, a)$ using parameterized function $q(s, a, w)$.\n",
    "      - **Update Rules**:\n",
    "        - **SARSA with Function Approximation**:\n",
    "          - Update $w$ using observed transitions $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$:\n",
    "            $$\n",
    "            w \\leftarrow w + \\alpha \\big(R_{t+1} + \\gamma q(S_{t+1}, A_{t+1}, w) - q(S_t, A_t, w)\\big) \\nabla q(S_t, A_t, w)\n",
    "            $$\n",
    "        - **Q-learning with Function Approximation**:\n",
    "          - Use greedy action for next state:\n",
    "            $$\n",
    "            w \\leftarrow w + \\alpha \\big(R_{t+1} + \\gamma \\max_{a'} q(S_{t+1}, a', w) - q(S_t, A_t, w)\\big) \\nabla q(S_t, A_t, w)\n",
    "            $$\n",
    "      - **Challenge**: Instability and divergence possible with off-policy methods and nonlinear function approximators.\n",
    "  - **Policy-Based Methods**\n",
    "      - **Directly parameterize the policy** $\\pi(a|s, \\theta)$.\n",
    "        - **Advantages**:\n",
    "          - Can learn stochastic policies.\n",
    "          - Better for high-dimensional or continuous action spaces.\n",
    "          - Avoids need to estimate value for every action.\n",
    "      - **Policy Gradient Theorem**:\n",
    "        - Provides formula for gradient of expected return with respect to policy parameters.\n",
    "  - **Actor-Critic Architecture**:\n",
    "    - **Actor**: Updates policy parameters $\\theta$ (chooses actions).\n",
    "    - **Critic**: Updates value function parameters $w$ (evaluates actions).\n",
    "    - **Update**:\n",
    "      - Critic computes TD error:\n",
    "        $$\n",
    "        \\delta_t = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w)\n",
    "          $$\n",
    "        - Actor updates $\\theta$ in direction suggested by $\\delta_t$:\n",
    "          $$\n",
    "          \\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla \\log \\pi(A_t|S_t, \\theta)\n",
    "          $$\n",
    "        - Critic updates $w$ to reduce value error."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
