{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0e5e0",
   "metadata": {},
   "source": [
    "- **Types of Reinforcement Learning**:\n",
    "  - **MDP (Markov Decision Process)**: You have clear states, actions, and rewards. The system’s next state depends only on the current state and action (Markov property).\n",
    "    - **Example**: A robot vacuum cleaner deciding where to go next based on its current location and the dirt it detects.\n",
    "  - **POMDP (Partially Observable MDP)**: Like MDP, but you don’t fully see the state. You get some clues (observations) about the state but not the full picture.\n",
    "    - **Example**: A self-driving car in foggy weather, using limited visibility to make driving decisions.\n",
    "  - **Bandit Problems**: No states, only actions and rewards. Balance exploration (trying new things) and exploitation (using what works).\n",
    "    - **Example**: A website testing different ads to see which ad gets more clicks. Each ad is an action, and clicks are rewards.\n",
    "    - **Common Algorithms**:\n",
    "      - **ε-Greedy** – Explores randomly with probability **ε**, otherwise exploits the best-known action.\n",
    "      - **UCB (Upper Confidence Bound)** – Picks actions based on confidence intervals, favoring uncertain options optimistically.\n",
    "      - **Softmax (Boltzmann Exploration)**\n",
    "      - **Thompson Sampling** – Uses Bayesian sampling to pick actions based on estimated reward probabilities.\n",
    "\n",
    "    - **Applications**: A/B testing, recommendation systems, and online advertising.\n",
    "\n",
    "    - **Types**:\n",
    "      - **Multi-Armed Bandit (MAB)**: A single agent chooses between multiple actions (like pulling levers on slot machines).\n",
    "        - **Example**: A recommendation system suggesting different products to users.\n",
    "      - **Contextual Bandit**: Reward depends on some extra information (context). When actions also change the state (context), it becomes a Markov Decision Process (MDP).\n",
    "        - **Example**: Netflix recommends a movie (**action**) based on your watch history (**state**). You watch it (**reward = 1**) or skip it (**reward = 0**). \n",
    "      - **Bayesian Bandit**: Uses Bayesian methods to update beliefs about the best action based on observed rewards.\n",
    "        - **Example**: A medical trial adjusting treatment recommendations based on patient responses. \n",
    "  - **Multi-Agent RL**: Multiple agents learning together.\n",
    "    - **Example**: Multiple robots in a warehouse.\n",
    "  - **Model-Based RL**: Learns a model of the environment, (how states change with actions) and uses it to plan the best actions.\n",
    "    - **Example**: A chess-playing AI that simulates possible moves and their outcomes before making a decision.\n",
    "    - **Algorithms**:\n",
    "      - **Monte Carlo Tree Search (MCTS)**\n",
    "      - **Dyna**\n",
    "  - **Model-Free RL**: Learns directly from experience without a model.\n",
    "    - **Example**: Teaching a robot to walk by trying different movements and learning from success or failure without knowing physics equations.\n",
    "    - **Types**:\n",
    "      - **Policy Gradient**(Learn the policy directly):\n",
    "        - **REINFORCE**\n",
    "      - **Learn value functions**:\n",
    "        - **Q-Learning**: Learns value of actions independent of policy.\n",
    "        - **SARSA**: Learns action values based on the action actually taken.\n",
    "        - **Deep Q-Networks (DQN)**: Use neural networks to approximate Q-values for complex states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade29de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c4c85",
   "metadata": {},
   "source": [
    "- **Initialization of Bandits $Q(a)$ in** $Q(a) = \\mathbb{E}_{r \\sim p(r|a)}[r]$\n",
    "    - **Realistic Initialization**: $ Q(a)=0$, starts with guessing zero expected reward till algorithm updates it based on observed rewards.\n",
    "    - **Optimistic Initialization**:  $ Q(a)=ψ , ψ > 0$ make initial rewards high to encourage exploration.\n",
    "- **Objective function**: Maximize expected cumulative reward over time $ T $ under a policy $ \\pi $.\n",
    "    - $J_T(\\pi) = \\mathbb{E}_{a_t \\sim \\pi, r_t \\sim p(r|a_t)}\\left[\\sum_{t=1}^{T} r_t\\right]$\n",
    "    - This formula is a general way to measure how good a policy is.\n",
    "    - **Policies in Bandit Problems**: \n",
    "        - **Deterministic Policy**: Always choose the same action for a given state.\n",
    "        - **Greedy Policy**: Highest estimated reward $\\to$ action\n",
    "        - **Epsilon-Greedy Policy**: Mostly greedy, but sometimes explores random actions. $\\epsilon$ (Random action) $\\to$ higher $\\to$ exploration.\n",
    "        - **Softmax Policy**: Chooses actions based on their estimated rewards, with a temperature parameter controlling exploration vs. exploitation.\n",
    "        - **Thompson Sampling**: Samples actions based on their probability of being optimal, balancing exploration and exploitation. \n",
    "- **Softmax (Boltzmann) Policy**:\n",
    "    - **How it works**:\n",
    "      - Assigns probabilities to actions based on their estimated rewards.\n",
    "      - Higher estimated rewards lead to higher probabilities of being chosen. (So not a single highest will be chosen, sometimes lower rewards (but still high like rank 2nd, 3rd..) will be chosen too because of high probability).\n",
    "      - The temperature parameter ($\\tau$) controls exploration vs. exploitation:\n",
    "        - **High $\\tau$**: More exploration (actions are chosen more uniformly).\n",
    "        - **Low $\\tau$**: More exploitation (actions with higher rewards are chosen more frequently).\n",
    "- **Upper Confidence Bound (UCB) Policy**:\n",
    "    - Picks the action with the highest value of: $Q(a) + c \\times \\sqrt{\\frac{\\ln t}{n(a)}}$\n",
    "    - Where:\n",
    "      - $Q(a)$ is the estimated value of action $a$.\n",
    "      - $c$ is a constant that controls exploration.\n",
    "        - **Higher $c$**: More exploration (more weight on uncertainty).\n",
    "        - **Lower $c$**: More exploitation (focus on known rewards).\n",
    "      - $t$ is the total number of actions taken.\n",
    "      - $n(a)$ is the number of times action $a$ has been selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7567af7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- **Trace $\\tau$**: A sequence of policy based state-action-reward pairs like $\\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \\ldots)$.\n",
    "    - **Infinite Horizon**: Sum rewards infinitely unless a terminal state is reached. $ R(\\tau) = \\sum_{i=0}^{\\infty} \\gamma^i \\cdot r_{t+i} $\n",
    "- **Return $ R(\\tau) $**: The cumulative sum of rewards from a trace\n",
    "    - **Discounted Return** gives less importance to future rewards. $ R(\\tau) = r_t + \\gamma \\cdot r_{t+1} + \\gamma^2 \\cdot r_{t+2} + \\dots $\n",
    "- **Value**: The expected return $ v^\\pi(s) $, $ q^\\pi(s,a) $\n",
    "- **Optimal Value/Policy**: \n",
    "    - $ v^*(s) $: Optimal (maximum expected) value function for state $s$.\n",
    "    - $ q^*(s,a) $: Optimal (maximum expected) action-value function for state $s$ and action $a$.\n",
    "    - $ \\pi^* $: Optimal (maximum expected) policy that achieves the optimal value (maximizes expected return).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f80a79",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "- **Policy Iteration**: Find optimal policy by iteratively improving the policy based on the value function. Fast converges in small states. Caulculate two step per itteration.\n",
    "  - **Steps**:\n",
    "    - 1. Select random initial policy.\n",
    "    - 2. **Policy Evaluation**: Calculate the value function for the current policy.\n",
    "    - 3. **Policy Improvement**: Update the policy based on the value function.\n",
    "    - 4. Repeat **steps 2-3** until the policy converges (no changes).\n",
    "\n",
    "- **Value Iteration**: Combines both **policy evaluation** and **policy improvement** into a single step to find optimal policy by iteratively updating the value function until convergence. A single step but takes longer due updates the value function iteratively until it converges.\n",
    "    - **Steps**:\n",
    "        - 1. Initialize the value function $V(s)$ arbitrarily (e.g., zeros).\n",
    "        - 2. For each state $s$ we calculate the value for **all actions** at state $s$, pick the maximum, and assign that as the new $V(s)$.\n",
    "        - 3. Repeat step 2 for all states until the value function converges (changes become very small).\n",
    "        - 4. After convergence, we have the best policy now.\n",
    "- **Policy Iteration:** Small–medium states, fewer but slower iterations while **Value Iteration:** Medium–large states, more but faster iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b2496",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016300d4",
   "metadata": {},
   "source": [
    "- Dynamic Programming finds the best policy (best actions) when you know **everything** about the environment. $\\to$ Policy Iteration.\n",
    "- When we **don't have full knowledge** of the environment, we use **Monte Carlo methods** to learn from **experience**. $\\to$ Model-Free RL.\n",
    "- **Monte Carlo Methods**: Learn from complete episodes (traces) to estimate value functions.\n",
    "  - MC estimates $v^\\pi(s)$ and $q^\\pi(s,a)$ by averaging returns from multiple episodes we have seen before.\n",
    "    - **First-Visit MC**: Averages returns from the first time each state-action pair is visited in an episode.\n",
    "    - **Every-Visit MC**: Averages returns from every time each state-action pair is visited in an episode.\n",
    "  - Begin episodes from random state-action pairs to ensure all pairs get visited sometimes because if some states/action pairs are never visited, MC can’t estimate their values.\n",
    "  - $\\epsilon$-greedy exploration is often used in MC to ensure all state-action pairs are explored.\n",
    "  - Monte Carlo can be used inside **Generalized Policy Iteration (GPI)**:\n",
    "    - **Two main types**:\n",
    "      - **On-Policy MC**: Evaluates and improves the policy being used to generate episodes.\n",
    "      - **Off-Policy MC**: Evaluates a different policy than the one being used to generate episodes.\n",
    "        - **Example**: Using a behavior policy to explore while evaluating a target policy.\n",
    "          - **Behavior Policy**: The policy used to generate episodes (exploration).\n",
    "          - **Target Policy**: The policy we want to improve (evaluation).\n",
    "        - **Problem**: Behavior and target policies can be different and make different decisions. We can't just average rewards because episodes came from a different policy. \n",
    "          - **Solution** = **Importance Sampling**: Give a weight to each episode based on how likely it was under the target policy compared to the behavior policy.\n",
    "\n",
    "- **Temporal Difference (TD) Learning**: another way to learn from **experience**, combines Monte Carlo and dynamic programming. It learns **step-by-step** during an episode instead of waiting until the episode ends.\n",
    "  - **Update Rule**: $ v(s_t) \\leftarrow v(s_t) + \\alpha \\left[ r_{t+1} + \\gamma v(s_{t+1}) - v(s_t) \\right]$\n",
    "    - Part in $[\\cdots]$ is called the **TD error**, which will correct the value estimate right after the action is taken unlike MC. This part is **policy evaluation** and the whole update is **policy improvement**.\n",
    "\n",
    "| Aspect              | Monte Carlo (MC)                    | Temporal Difference (TD)                     |\n",
    "| ------------------- | ----------------------------------- | -------------------------------------------- |\n",
    "| When update happens | After whole episode ends            | After every step                             |\n",
    "| Use of next state   | No (waits for total return)         | Yes (bootstraps with estimated $v(s_{t+1})$) |\n",
    "| Variance            | High (because it uses total return) | Lower (uses one-step estimate)               |\n",
    "| Bias                | No bias (unbiased)                  | Slight bias due to bootstrapping             |\n",
    "| Learning speed      | Slower                              | Faster and more online                       |\n",
    "\n",
    "- **TD Control algorithms**\n",
    "  - **SARSA (On-policy TD Control)**: Learns action values for the policy being followed. (Learning how good a move is based on the next move you actually make.)\n",
    "    - Update rule: $Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]$\n",
    "  - **Q-learning (Off-policy TD Control)**: Learns the optimal action values regardless of the policy being followed. (Uses the **maximum** estimated value of the next state (best possible action), not necessarily the action actually taken.)\n",
    "    - Update rule: $Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]$\n",
    "    - **Maximization Bias**: Q-learning can overestimate action values due to always selecting the maximum Q-value in future states.\n",
    "    - **Double Q-learning**: Addresses this bias by maintaining two separate Q-tables and using one to select actions and the other to evaluate them, reducing overestimation.\n",
    "\n",
    "  - **Expected SARSA**: A variant of SARSA that uses the expected value over possible next actions instead of the single next action.\n",
    "    - Update rule: $Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\sum_a \\pi(a|s_{t+1}) Q(s_{t+1}, a) - Q(s_t, a_t) \\right]$\n",
    "\n",
    "\n",
    "- **N-step TD and N-step SARSA**: Extensions of TD and SARSA that consider multiple steps for better value estimation.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "| Algorithm      | On-policy / Off-policy | Next state update                | Main idea                                           |\n",
    "| -------------- | ---------------------- | -------------------------------- | --------------------------------------------------- |\n",
    "| SARSA          | On-policy              | Uses next action actually taken  | Learns action values following policy               |\n",
    "| Q-learning     | Off-policy             | Uses max action value next state | Learns optimal action values regardless of behavior |\n",
    "| Expected SARSA | On or Off-policy       | Uses expected value over actions | Averages over possible next actions                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb25a35",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Note:\n",
    "\n",
    "* **Monte Carlo Tree Search (MCTS)** is a **planning** method. It uses a **model** to simulate many possible future paths from the current state (reversible access) and builds a search tree to decide the best next action. It focuses on a **local solution**.\n",
    "\n",
    "* **Monte Carlo methods in RL** are **model-free learning** methods. They learn from **real experience** by averaging returns from full episodes. They don’t use a model and update values globally for all states visited.\n",
    "\n",
    "So, MCTS = planning with a model, and Monte Carlo RL = learning from experience without a model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d11ff5",
   "metadata": {},
   "source": [
    "\n",
    "- **Back-up**: update estimation of state values $V(s)$ or state-action values $Q(s,a)$ (future reward) in reinforcement learning.\n",
    "    - **Expected Back-ups**: used in planning with a full model of the environment. (e.g., Dynamic Programming)\n",
    "    - **Sample Back-ups**: used in reinforcement learning without a full model, updating values based on sampled experiences. (e.g., TD learning, Monte Carlo methods)\n",
    "- **On-policy Back-ups**: updates values based on the policy being followed. The agent learns about the consequences of its current behavior. \n",
    "  * Example: **SARSA** — updates use the next action that the agent actually takes.\n",
    "- **Off-policy Back-ups**: updates values based on a different policy than the one being followed. The agent learns about the consequences of actions that may not be taken in the current behavior.\n",
    "  * Example: **Q-learning** — updates use the best possible next action, regardless of what action the agent actually took.\n",
    "* **1-Step Back-ups (Shallow updates):**\n",
    "  * Updates use information from only one step ahead (the immediate next state).\n",
    "  * Faster but may be less accurate.\n",
    "  * Examples:\n",
    "    * **TD(0)** updates values using the reward and the value estimate of the next state only.\n",
    "    * **Monte Carlo** can be considered 1-step if it updates only after an episode ends (looking at total return).\n",
    "* **Multi-Step Back-ups (Deep updates):**\n",
    "  * Updates consider multiple steps into the future, often the entire remaining episode or a sequence of states.\n",
    "  * More accurate but computationally heavier.\n",
    "  * Examples:\n",
    "    * **Monte Carlo** updates after entire episodes (multi-step).\n",
    "    * **TD(λ)** uses a weighted average of different multi-step returns.\n",
    "    * **Dynamic Programming** computes expected returns over all possible future steps.\n",
    "\n",
    "| Algorithm                    | Type of Back-up  | Step-size  | Explanation                                                        |\n",
    "| ---------------------------- | ---------------- | ---------- | ------------------------------------------------------------------ |\n",
    "| **TD(0)**                    | Sample Back-up   | 1-step     | Uses one sample, updates values after one step                     |\n",
    "| **Monte Carlo (MC)**         | Sample Back-up   | Multi-step | Uses full episode returns, updates after episode ends              |\n",
    "| **Dynamic Programming (DP)** | Expected Back-up | Multi-step | Uses full model to calculate expected returns over all next states |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e57cc5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "- **MDP Dynamics**\n",
    "    - **Models** (Reversible access):\n",
    "        - **Distribution model:** Give you full probability distribution from next till target state.\n",
    "        - **Sample model:** We don’t get the whole distribution, just one possible next state from the randomness.\n",
    "    - **Environments** (Irreversible access):\n",
    "        - **Sample Environment:** You almost always get only samples, because you actually take an action and observe what happens next, you don't get full distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0accdb82",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- **MDP Approaches**:\n",
    "    - **Planning**:  assumes reversible access to the environment dynamics, meaning you have a model that you can query at any state-action pair, anytime (you can simulate outcomes without actually moving).\n",
    "    - **Learning**: (specifically model-free RL) assumes irreversible access, meaning you can only interact with the real environment step-by-step, moving forward and learning from experience.\n",
    "\n",
    "| Access Type        | Solution Type   | Method Category                 | Example Algorithm(s)           |\n",
    "| ------------------ | --------------- | ------------------------------- | ------------------------------ |\n",
    "| Reversible (Model) | Local solution  | **Planning**                    | Monte Carlo Tree Search (MCTS) |\n",
    "| Reversible (Model) | Global solution | **Model-based RL** (borderline) | Dynamic Programming (DP)       |\n",
    "| Irreversible (Env) | Global solution | **Model-free RL**               | Q-learning, SARSA              |\n",
    "\n",
    "- **Tabular Model Learning**: This is a way to learn a model of the environment when you don’t have it beforehand, by collecting experience from the environment.\n",
    "* Keep arrays for $n(s,a,s')$ and $R_{sum}(s,a,s')$, size $|S| \\times |A| \\times |S|$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e9d4b4",
   "metadata": {},
   "source": [
    "- **Dyna**: Model-based RL algorithm that combines learning and planning.\n",
    "    - **Steps**:\n",
    "        1. Learn a model of the environment from experience.\n",
    "            - Store in table the reward of taking action $a$ in state $s$ and ending up in state $s'$\n",
    "        2. Use the model to simulate experiences and update the value function or policy.\n",
    "\n",
    "        3. Update the value function or policy based on real experiences.\n",
    "        - Update Q-values by simulating transitions as simulated the transitions without actually taking actions in the environment.\n",
    "    - **Benefits**: Combines the strengths of model-based and model-free approaches, allowing for faster learning and better exploration.\n",
    "    - **Parameters**:\n",
    "        - Number of planning updates K: How many simulated updates you do using the model for every real experience.\n",
    "        - Learning rate αα: How much you update the Q-values each time.\n",
    "        - Discount factor γγ: How much future rewards count compared to immediate rewards.\n",
    "        - Exploration parameter ϵϵ: Probability of choosing a random action (to explore).\n",
    "    - **Algorithm**:\n",
    "        1. Start with Q-values and model counts all zero.\n",
    "        2. At each step, pick an action using epsilon-greedy policy.\n",
    "        3. Take the action, get reward and next state from the real environment.\n",
    "        4. Update the model (counts and rewards) based on this experience.\n",
    "        5. Update Q-value using the real experience (Q-learning update).\n",
    "        6. Do extra updates by simulating experiences from the model K times, updating Q-values each time.\n",
    "\n",
    "- **Prioritized Sweeping**: Extension of Dyna (Model-Based) that instead of updating all states equally or randomly, it focuses on the most important or most promising states that need an update. This speeds up learning by spending effort where it matters most.\n",
    "\n",
    "    - **Steps**:\n",
    "        1. Keep a **priority queue** of states prioritized by their TD error magnitude (Higher priority $p \\to$ higher TD error $\\to$ more important to update).\n",
    "            - Priority queue is a data structure keeps items sorted by their priority $\\to$ when pop $p$ the highest priority item is removed first.\n",
    "        2. Update the most important states first, ensuring that the learning process focuses on the most impactful changes.\n",
    "    - **Key Parameter**: \n",
    "        - **Threshold $\\theta$:** Minimum priority for a state-action to be added to the queue. Controls how sensitive the algorithm is to changes.\n",
    "    - **Algorithm**:\n",
    "        - 1. **Initialize** Q-values, model counts, and an empty priority queue.\n",
    "        - 2. **Interact** with the environment:\n",
    "            * Take action in current state using exploration (like epsilon-greedy).\n",
    "            * Observe reward and next state.\n",
    "            * Update the model with this transition.\n",
    "        - 3. **Calculate priority** for that state-action: $ p = |r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)| $ If $p > \\theta$ (threshold), add $(s, a)$ to the priority queue.\n",
    "\n",
    "        - 4. **Planning updates (repeat K times):**\n",
    "            * Pop the highest priority $(s, a)$ from the queue.\n",
    "            * Use the model to simulate next state and reward.\n",
    "            * Update Q-value for $(s, a)$ with TD update.\n",
    "            * For all state-actions leading to $s$, calculate their priority and add to queue if above threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9206b2b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- **Types of Planning**:\n",
    "  * **Decision-time planning:** Focuses on picking the best action for the current state. (e.g., A\\*, MCTS)\n",
    "  * **Background planning:** Improves global policy or value function (like learning). (e.g., Dynamic Programming, Dyna)\n",
    "  - **Classic Planning:**\n",
    "    * **Problems:** \n",
    "        - 1-Need a good heuristic to reduce depth. \n",
    "        - 2- Pruning is risky and harder with stochasticity.\n",
    "        - 3- Often need exact transition models, which may not be available (only simulators).\n",
    "    * **Tree search:** Explores states like a tree, but can waste time on repeated states.\n",
    "    * **Graph search:** Avoids repeated states by tracking explored (closed list) and frontier (open list).\n",
    "    * **Uninformed search:** BFS, DFS, Iterative Deepening. Downsides: may miss better paths because they ignore costs/rewards/weights.\n",
    "    * **Uniform cost search (Dijkstra):** Considers cost so far but ignores future potential.\n",
    "    * **Heuristic search (A\\*):** Actual cumulative cost from start to state $s$ $(g(s))$ + estimated cumulative cost from $s$ to end $(h(s))$.\n",
    "        * Heuristic $h(s)$ must be **admissible** (never overestimate) to guarantee finding the best path.\n",
    "        * Perfect heuristic = optimal value function $V^*(s)$.\n",
    "        * $f(s) = g(s) \\to \\text{Dijkstra’s algorithm/Uniform-cost search} + h(s) \\to \\text{cumulative cost from s to end} = \\text{A* search}$.\n",
    "* **Forward pruning:** Limits actions explored (like beam search), but risks removing the best action.\n",
    "* **Stochastic planning:** Extends classic algorithms (e.g., A\\* → AO\\*) by expanding all possible outcomes of actions.\n",
    "  * Problems: Needs analytic model and heuristic, but often only simulators are available; large branching in stochastic cases.\n",
    "\n",
    "**Rollouts**: A technique to estimate the value of actions by simulating random future states. (Simulation)\n",
    "- **Sample-based Planning**:\n",
    "  * **Monte Carlo (MC) methods** to estimate action values by sampling instead of enumerating all possibilities.\n",
    "    * **Benefits:**\n",
    "\n",
    "      * No need for heuristic (use rollouts).\n",
    "      * No need to prune actions (use uncertainty to guide search).\n",
    "      * Can work with simulators, no exact model needed.\n",
    "    - **a) Monte Carlo Search (MCS)**\n",
    "      * For each action, sample N trajectories (rollouts) until depth D with a rollout policy (random or better).\n",
    "      * Estimate action value Q(s,a) by average returns, pick action with highest Q.\n",
    "      * **Downside:** Does not improve policy below the current step; no memory of past samples.\n",
    "    - **b) Sparse Sampling**\n",
    "        * Like MCS but repeats sampling at every level up to depth D (policy improvement at all depths).\n",
    "        * Sample complexity grows exponentially with D: (A \\* N)^D. Very expensive.\n",
    "    - **c) Monte Carlo Tree Search (MCTS)**\n",
    "        * Improves on Sparse Sampling by focusing sampling on promising actions using **adaptive bandit algorithms** (e.g., UCT).\n",
    "        * Builds an asymmetric tree, deeper where returns are better.\n",
    "        * Four phases:\n",
    "\n",
    "          1. **Selection:** Use UCT to select action balancing exploration/exploitation.\n",
    "          2. **Expansion:** Add new state when an unvisited action is chosen.\n",
    "          3. **Simulation:** Rollout from new state to estimate value.\n",
    "          4. **Backup:** Update statistics (visit counts and mean returns) up the tree.\n",
    "        * Sample complexity: proportional to $M$ (number of traces) $\\cdot D $(depth).\n",
    "            - **Sparse Sampling**: $ (A \\cdot N )^D $ and MCTS complexity is $ M \\cdot D $ due promising paths.\n",
    "        * Very effective especially without a good heuristic (used in AlphaGo).\n",
    "- **Iterated Planning & Learning**\n",
    "\n",
    "  * Pure planning is expensive and may lack good heuristics.\n",
    "  * Pure learning may have errors in value/policy estimates.\n",
    "  * Combining both is powerful:\n",
    "\n",
    "    * Use planning to fix errors in learned values at decision-time.\n",
    "    * Use planning to generate data for learning in background.\n",
    "  * **AlphaGo** is an example that combines planning and learning iteratively.\n",
    "  * Analogy:\n",
    "\n",
    "    * Learned value function = \"Thinking fast\" (quick decisions).\n",
    "    * Decision-time planning = \"Thinking slow\" (careful analysis).\n",
    "  * Both work together for better decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed99b839",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52bee9d",
   "metadata": {},
   "source": [
    "\n",
    "- **Monte Carlo Tree Search (MCTS)** finds the best move by repeatedly simulating games and building a search tree with four steps repeated:\n",
    "  - **Selection:** Traverse the tree using a policy to select a node to expand\n",
    "  - **Expansion:** Add new child nodes (possible moves)\n",
    "  - **Simulation (Rollout):** Play random moves till the end or a cutoff to estimate the outcome\n",
    "  - **Backup:** Update values in the tree nodes based on the simulation result\n",
    "\n",
    "---\n",
    "\n",
    "### MCTS in Detail:\n",
    "\n",
    "- **UCT formula (Upper Confidence Bound for Trees):** a policy that choose the best action $a$ from state $s$\n",
    "\n",
    "$$\n",
    "\\pi_{UCT}(s) = \\arg\\max_a Q(s,a) + c \\sqrt{\\frac{\\ln n(s)}{n(s,a)}}\n",
    "$$\n",
    "\n",
    "* $Q(s,a)$ is the value estimation for action $a$ at state $s$\n",
    "\n",
    "* $n(s)$ is how many times state $s$ was visited\n",
    "\n",
    "* $n(s,a)$ is how many times action $a$ was chosen at $s$\n",
    "\n",
    "* $c$ is a constant balancing exploration and exploitation\n",
    "\n",
    "Means: Choose the action aa that has the best mix of a high value Q(s,a)Q(s,a) and an exploration bonus that favors less tried moves.\n",
    "\n",
    "* **Q-value update:**\n",
    "\n",
    "$$\n",
    "Q_{\\text{new}} \\leftarrow Q_{\\text{old}} + \\frac{1}{n} (R_{\\text{sum}} - Q_{\\text{old}})\n",
    "$$\n",
    "\n",
    "* $R_{\\text{sum}}$ is sum of rewards from simulations\n",
    "* $n$ is the number of visits\n",
    "\n",
    "Means: Is like a average update, to refine the value estimate based on new information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b368176",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- **AlphaGo**: Leveraging prior knowledge both during learning and deployment.\n",
    "    - **Behavior cloning**: Train a neural network to mimic expert moves.\n",
    "    - **Monte Carlo Tree Search (MCTS)**: Uses smart search to look ahead in the game by simulating moves and outcomes before deciding what to do.\n",
    "    - **Reinforcement learning**: Trial and error learning from self-play to improve the model over time.\n",
    "    - **Self-play**: Curriculum learning by playing against itself, starting from random moves and gradually improving.\n",
    "    \n",
    "- **AlphaGo Neural Network**: \n",
    "    - **Rollout Policy $p_{\\pi}$:** NN trained by vlassification on human expert positions, plays quick, random moves (rollouts).\n",
    "    - **Supervised Learning Policy $p_{\\sigma}$:** Trained on many **human expert moves**. Learns to predict expert moves by **classifying** the next move given a board state, copying expert behavior (behavior cloning).\n",
    "    - **Reinforcement Learning (RL) Policy Network $p_{\\rho}$:** Starts from the SL policy network $p_{\\sigma}$ by improves by playing against itself (**self-play**) using **policy gradient** methods.\n",
    "    - **Value Network $v_{\\theta}$:** NN **predict the chance of winning** from any board position. Trained by **regression** on results of self-play games.\n",
    "- **Summary of the flow**:\n",
    "    * Start with expert data (human games) → train rollout and SL policy networks.\n",
    "    * Use SL policy as a base and improve by self-play → train RL policy network.\n",
    "    * Use self-play results to train value network that predicts winning chances.\n",
    "    * All these networks help AlphaGo play much better than just copying humans.\n",
    "\n",
    "\n",
    "* **KL divergence** measures how different two probability distributions are, minimizing KL divergence means **making your model’s policy close to the expert’s policy**.\n",
    "  - $\\theta = \\arg\\min_\\theta KL(p_\\mu(\\cdot | s) \\| p_\\theta(\\cdot | s))$ : Means you find the best parameters $\\theta$ that minimize the difference as\n",
    "    - $p_\\mu(\\cdot | s)$: The expert’s policy — how the expert chooses moves given state $s$.\n",
    "    - $p_\\theta(\\cdot | s)$: Your model’s policy — how your AI chooses moves given state $s$, controlled by parameters $\\theta$.\n",
    "  - **Maximum Likelihood Estimation (MLE)** is used to find parameters $\\theta$ that maximize the likelihood of expert moves, meaning you adjust your model to make it think the expert’s moves are very likely.\n",
    "    - $ \\arg\\max_\\theta \\sum \\mu(c) \\log p_\\theta(c|s) $\n",
    "\n",
    "- **AlphaGo Algorithm Notation:**\n",
    "  - **Policy Prior $P(s,a) \\in [0,1]$:** Probability of taking action $a$ in state $s$.\n",
    "  - **# value function evaluation $N_v(s,a) \\in N_0$:** Number of times state $s$ has been evaluated.\n",
    "  - **# rollout evaluations $N_r(s,a) \\in N_0$:** Number of times action $a$ has been rolled out from state $s$.\n",
    "  - **Cumlative evaluated values $W_v(s,a) \\in R$:** Sum of values from evaluating state $s$ with action $a$.\n",
    "  - **Cumlative evaluated rollouts $W_r(s,a) \\in R$:** Sum of values from rolling out action $a$ from state $s$.\n",
    "  - **Transition value $Q(s,a) \\in [-1,1]$:** Estimated value of taking action $a$ in state $s$. The average quality (value) of taking action $a$ in state $s$. It represents the expected outcome (win or lose) of making that move.\n",
    "\n",
    "  - **Reward function $r(s_t, a_t)$**:\n",
    "\n",
    "$$\n",
    "r(s_t, a_t) = \n",
    "\\begin{cases} \n",
    "+1 & \\text{if win (terminal state)} \\\\\n",
    "-1 & \\text{if lose (terminal state)} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae73b8c8",
   "metadata": {},
   "source": [
    "- **Value Function Approximation**: Instead of storing $v_\\pi(s)$ for each state in large scale of state-actions, use a function $\\hat{v}(s, w)$ with parameters $w$ to approximate the value function.\n",
    "  - **$w$ approximation**: $w$  could be weights in a linear function or parameters of a neural network.\n",
    "  - **Goal**: Find $w$ such that $\\hat{v}(s, w)$ is close to $v_\\pi(s)$ for all states $s$.\n",
    "    - Define an error function: $VE(w) = \\sum_s \\mu(s) \\left[v_\\pi(s) - \\hat{v}(s, w)\\right]^2$\n",
    "      - Use **gradient descent** to reduce this error by updating $w$:\n",
    "        - $ w \\leftarrow w - \\alpha \\nabla VE(w)$\n",
    "    - Use **sample estimates** instead from experience:\n",
    "      - **Monte Carlo (MC):** Use full return $G_t$ after an episode.\n",
    "        - **Gradient Monte Carlo:** Adjust parameters so the estimated value $\\hat{v}$ moves closer to the observed return $G_t$.\n",
    "          - $ w \\leftarrow w + \\alpha (G_t - \\hat{v}(S_t, w)) \\nabla \\hat{v}(S_t, w) $\n",
    "          - where $G_t$ id the total return from time $t$ like $G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + ...$.\n",
    "      - **Temporal Difference (TD):** Use current reward plus estimated value of next state. \n",
    "        - **Gradient TD(0) Algorithm**: Use one-step TD target instead of full return:\n",
    "          - $ w \\leftarrow w + \\alpha \\big(R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w)\\big) \\nabla \\hat{v}(S_t, w) $\n",
    "          - Update happens after every step, not after full episode. (More efficient and online)\n",
    "  - **Approximating Values** $\\hat{v}(s, w)$: We can’t store values for all states, so we use a **function** with parameters $w$ to guess values. Types:\n",
    "    - 1) **State Aggregation**: Group similar states and assign one value to each group.\n",
    "    - 2) **Linear Approximation**: Use features $x(s)$ of the state to compute value as a weighted sum:\n",
    "      - $\\hat{v}(s, w) = w_1 x_1(s) + w_2 x_2(s) + \\cdots $\n",
    "    - **Feature Types**: \n",
    "      - **Polynomial**: Use powers of state variables (e.g., $s, s^2, s^3$).\n",
    "      - **Fourier**: Use sine/cosine functions to represent states.\n",
    "      - **Coarse Coding**: Divide state space into overlapping regions; feature is 1 if state in region, else 0.\n",
    "  - Control with **Action-Value Function Approximation**: Learn optimal policy by approximating action-value function $q_*(s, a)$ using parameterized function $q(s, a, w)$.\n",
    "      - **Update Rules**:\n",
    "        - **SARSA with Function Approximation**:\n",
    "          - Update $w$ using observed transitions $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$:\n",
    "            $$\n",
    "            w \\leftarrow w + \\alpha \\big(R_{t+1} + \\gamma q(S_{t+1}, A_{t+1}, w) - q(S_t, A_t, w)\\big) \\nabla q(S_t, A_t, w)\n",
    "            $$\n",
    "        - **Q-learning with Function Approximation**:\n",
    "          - Use greedy action for next state:\n",
    "            $$\n",
    "            w \\leftarrow w + \\alpha \\big(R_{t+1} + \\gamma \\max_{a'} q(S_{t+1}, a', w) - q(S_t, A_t, w)\\big) \\nabla q(S_t, A_t, w)\n",
    "            $$\n",
    "      - **Challenge**: Instability and divergence possible with off-policy methods and nonlinear function approximators.\n",
    "  - **Policy-Based Methods**\n",
    "      - **Directly parameterize the policy** $\\pi(a|s, \\theta)$.\n",
    "        - **Advantages**:\n",
    "          - Can learn stochastic policies.\n",
    "          - Better for high-dimensional or continuous action spaces.\n",
    "          - Avoids need to estimate value for every action.\n",
    "      - **Policy Gradient Theorem**:\n",
    "        - Provides formula for gradient of expected return with respect to policy parameters.\n",
    "  - **Actor-Critic Architecture**:\n",
    "    - **Actor**: Updates policy parameters $\\theta$ (chooses actions).\n",
    "    - **Critic**: Updates value function parameters $w$ (evaluates actions).\n",
    "    - **Update**:\n",
    "      - Critic computes TD error:\n",
    "        $$\n",
    "        \\delta_t = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w)\n",
    "          $$\n",
    "        - Actor updates $\\theta$ in direction suggested by $\\delta_t$:\n",
    "          $$\n",
    "          \\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla \\log \\pi(A_t|S_t, \\theta)\n",
    "          $$\n",
    "        - Critic updates $w$ to reduce value error."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
