{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0e5e0",
   "metadata": {},
   "source": [
    "- **RL main components**:\n",
    "  - **Policy**: The strategy for selecting actions.\n",
    "  - **Reward Function $R$**: Determines immediate desirability of states/actions.\n",
    "  - **Value Function $V$**: Measures long-term expected rewards.\n",
    "  - **Model (optional)**: Predicts state transitions for planning.\n",
    "- **Types of Reinforcement Learning**:\n",
    "  - **MDP (Markov Decision Process)**: You have clear states, actions, and rewards. The system’s next state depends only on the current state and action (Markov property).\n",
    "    - **Example**: A robot vacuum cleaner deciding where to go next based on its current location and the dirt it detects.\n",
    "  - **POMDP (Partially Observable MDP)**: Like MDP, but you don’t fully see the state. You get some clues (observations) about the state but not the full picture.\n",
    "    - **Example**: A self-driving car in foggy weather, using limited visibility to make driving decisions.\n",
    "  - **Bandit Problems**: No states, only actions and rewards. Balance exploration (trying new things) and exploitation (using what works).\n",
    "    - **Example**: A website testing different ads to see which ad gets more clicks. Each ad is an action, and clicks are rewards.\n",
    "    - **Common Algorithms**:\n",
    "      - **ε-Greedy** – Explores randomly with probability **ε**, otherwise exploits the best-known action.\n",
    "      - **UCB (Upper Confidence Bound)** – Picks actions based on confidence intervals, favoring uncertain options optimistically.\n",
    "      - **Thompson Sampling** – Uses Bayesian sampling to pick actions based on estimated reward probabilities.\n",
    "\n",
    "    - **Applications**: A/B testing, recommendation systems, and online advertising.\n",
    "\n",
    "    - **Types**:\n",
    "      - **Multi-Armed Bandit (MAB)**: A single agent chooses between multiple actions (like pulling levers on slot machines).\n",
    "        - **Example**: A recommendation system suggesting different products to users.\n",
    "      - **Contextual Bandit**: One action, one reward, then stops based on context; repeated over many rounds.\n",
    "        - **Example**: Netflix recommends a movie (**action**) based on your watch history (**state**). You watch it (**reward = 1**) or skip it (**reward = 0**). \n",
    "      - **Bayesian Bandit**: Uses Bayesian methods to update beliefs about the best action based on observed rewards.\n",
    "        - **Example**: A medical trial adjusting treatment recommendations based on patient responses. \n",
    "  - **Multi-Agent RL**: Multiple agents learning together.\n",
    "    - **Example**: Multiple robots in a warehouse.\n",
    "  - **Model-Based RL**: Learns a model of the environment, (how states change with actions) and uses it to plan the best actions.\n",
    "    - **Example**: A chess-playing AI that simulates possible moves and their outcomes before making a decision.\n",
    "    - **Algorithms**:\n",
    "      - **Monte Carlo Tree Search (MCTS)**\n",
    "      - **Dyna**\n",
    "  - **Model-Free RL**: Learns directly from experience without a model.\n",
    "    - **Example**: Teaching a robot to walk by trying different movements and learning from success or failure without knowing physics equations.\n",
    "    - **Types**:\n",
    "      - **Policy Gradient**(Learn the policy directly):\n",
    "        - **REINFORCE**\n",
    "      - **Learn value functions**:\n",
    "        - **Q-Learning**: Learns value of actions independent of policy.\n",
    "        - **SARSA**: Learns action values based on the action actually taken.\n",
    "        - **Deep Q-Networks (DQN)**: Use neural networks to approximate Q-values for complex states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade29de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
