{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0e5e0",
   "metadata": {},
   "source": [
    "- **RL main components**:\n",
    "  - **Policy**: The strategy for selecting actions.\n",
    "  - **Reward Function $R$**: Determines immediate desirability of states/actions.\n",
    "  - **Value Function $V$**: Measures long-term expected rewards.\n",
    "  - **Model (optional)**: Predicts state transitions for planning.\n",
    "- **Types of Reinforcement Learning**:\n",
    "  - **MDP (Markov Decision Process)**: You have clear states, actions, and rewards. The system’s next state depends only on the current state and action (Markov property).\n",
    "    - **Example**: A robot vacuum cleaner deciding where to go next based on its current location and the dirt it detects.\n",
    "  - **POMDP (Partially Observable MDP)**: Like MDP, but you don’t fully see the state. You get some clues (observations) about the state but not the full picture.\n",
    "    - **Example**: A self-driving car in foggy weather, using limited visibility to make driving decisions.\n",
    "  - **Bandit Problems**: No states, only actions and rewards. Balance exploration (trying new things) and exploitation (using what works).\n",
    "    - **Example**: A website testing different ads to see which ad gets more clicks. Each ad is an action, and clicks are rewards.\n",
    "    - **Common Algorithms**:\n",
    "      - **ε-Greedy** – Explores randomly with probability **ε**, otherwise exploits the best-known action.\n",
    "      - **UCB (Upper Confidence Bound)** – Picks actions based on confidence intervals, favoring uncertain options optimistically.\n",
    "      - **Softmax (Boltzmann Exploration)**\n",
    "      - **Thompson Sampling** – Uses Bayesian sampling to pick actions based on estimated reward probabilities.\n",
    "\n",
    "    - **Applications**: A/B testing, recommendation systems, and online advertising.\n",
    "\n",
    "    - **Types**:\n",
    "      - **Multi-Armed Bandit (MAB)**: A single agent chooses between multiple actions (like pulling levers on slot machines).\n",
    "        - **Example**: A recommendation system suggesting different products to users.\n",
    "      - **Contextual Bandit**: Reward depends on some extra information (context). When actions also change the state (context), it becomes a Markov Decision Process (MDP).\n",
    "        - **Example**: Netflix recommends a movie (**action**) based on your watch history (**state**). You watch it (**reward = 1**) or skip it (**reward = 0**). \n",
    "      - **Bayesian Bandit**: Uses Bayesian methods to update beliefs about the best action based on observed rewards.\n",
    "        - **Example**: A medical trial adjusting treatment recommendations based on patient responses. \n",
    "  - **Multi-Agent RL**: Multiple agents learning together.\n",
    "    - **Example**: Multiple robots in a warehouse.\n",
    "  - **Model-Based RL**: Learns a model of the environment, (how states change with actions) and uses it to plan the best actions.\n",
    "    - **Example**: A chess-playing AI that simulates possible moves and their outcomes before making a decision.\n",
    "    - **Algorithms**:\n",
    "      - **Monte Carlo Tree Search (MCTS)**\n",
    "      - **Dyna**\n",
    "  - **Model-Free RL**: Learns directly from experience without a model.\n",
    "    - **Example**: Teaching a robot to walk by trying different movements and learning from success or failure without knowing physics equations.\n",
    "    - **Types**:\n",
    "      - **Policy Gradient**(Learn the policy directly):\n",
    "        - **REINFORCE**\n",
    "      - **Learn value functions**:\n",
    "        - **Q-Learning**: Learns value of actions independent of policy.\n",
    "        - **SARSA**: Learns action values based on the action actually taken.\n",
    "        - **Deep Q-Networks (DQN)**: Use neural networks to approximate Q-values for complex states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade29de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c4c85",
   "metadata": {},
   "source": [
    "- **Initialization of Bandits $Q(a)$ in** $Q(a) = \\mathbb{E}_{r \\sim p(r|a)}[r]$\n",
    "    - **Realistic Initialization**: $ Q(a)=0, starts with guessing zero expected reward till algorithm updates it based on observed rewards.\n",
    "    - **Optimistic Initialization**:  $ Q(a)=ψ , ψ > 0$ make initial rewards high to encourage exploration.\n",
    "- **Objective function**: Maximize expected cumulative reward over time $ T $ under a policy $ \\pi $.\n",
    "    - $J_T(\\pi) = \\mathbb{E}_{a_t \\sim \\pi, r_t \\sim p(r|a_t)}\\left[\\sum_{t=1}^{T} r_t\\right]$\n",
    "    - This formula is a general way to measure how good a policy is.\n",
    "    - **Policies in Bandit Problems**: \n",
    "        - **Greedy Policy**: Highest estimated reward $\\to$ action\n",
    "        - **Epsilon-Greedy Policy**: Mostly greedy, but sometimes explores random actions.\n",
    "        - **Softmax Policy**: Chooses actions based on their estimated rewards, with a temperature parameter controlling exploration vs. exploitation.\n",
    "        - **Thompson Sampling**: Samples actions based on their probability of being optimal, balancing exploration and exploitation. \n",
    "- **Update the mean reward**:\n",
    "    - **Incremental Mean Update**: \n",
    "      - $Q_{n} = Q_{n-1} + \\frac{1}{n} [r_{n} - Q_{n-1}]$\n",
    "      - Where $N(a)$ is the number of times action $a$ has been selected.\n",
    "    - **Learning update** (Exponential Moving Average): \n",
    "      - $Q_n \\leftarrow Q_{n-1} + \\alpha \\left[ r_n - Q_{n-1} \\right]$\n",
    "      - Simply move the new mean a bit in the direction of the last observed reward, where $\\alpha$ is the learning rate (0 < $\\alpha$ < 1).\n",
    "- **Epsilon-Greedy Policy**:\n",
    "    - **How it works**:\n",
    "      - With probability $\\epsilon$, choose a random action.\n",
    "      - With probability $1 - \\epsilon$, choose the action with the highest estimated reward.\n",
    "      - Epsilon-greedy exploration is simple but can waste time trying bad actions equally.\n",
    "        - **Small** $\\epsilon$, the policy mostly exploits the best action.\n",
    "        - **Large** $\\epsilon$, the policy explores more.\n",
    "- **Softmax (Boltzmann) Policy**:\n",
    "    - **How it works**:\n",
    "      - Assigns probabilities to actions based on their estimated rewards.\n",
    "      - Higher estimated rewards lead to higher probabilities of being chosen. (So not a single highest will be chosen, sometimes lower rewards (but still high like rank 2nd, 3rd..) will be chosen too because of high probability).\n",
    "      - The temperature parameter ($\\tau$) controls exploration vs. exploitation:\n",
    "        - **High $\\tau$**: More exploration (actions are chosen more uniformly).\n",
    "        - **Low $\\tau$**: More exploitation (actions with higher rewards are chosen more frequently).\n",
    "- **Upper Confidence Bound (UCB) Policy**:\n",
    "    - Picks the action with the highest value of: $Q(a) + c \\times \\sqrt{\\frac{\\ln t}{n(a)}}$\n",
    "    - Where:\n",
    "      - $Q(a)$ is the estimated value of action $a$.\n",
    "      - $c$ is a constant that controls exploration.\n",
    "        - **Higher $c$**: More exploration (more weight on uncertainty).\n",
    "        - **Lower $c$**: More exploitation (focus on known rewards).\n",
    "      - $t$ is the total number of actions taken.\n",
    "      - $n(a)$ is the number of times action $a$ has been selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7567af7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Bandit algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045df8e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### Upper Confidence Bound (UCB) Policy\n",
    "\n",
    "* Picks the action with the highest value of: $Q(a) + c \\times \\sqrt{\\frac{\\ln t}{n(a)}}$\n",
    "* Here:\n",
    "\n",
    "  * $Q(a)$: estimated reward of action $a$.\n",
    "  * $n(a)$: how many times action $a$ was tried.\n",
    "  * $t$: current time step.\n",
    "  * $c$: controls how much to explore.\n",
    "* It prefers actions with high rewards but also gives a bonus to actions tried less often, encouraging exploration of new or less-tried actions.\n",
    "\n",
    "---\n",
    "\n",
    "### Hyperparameters and their impact\n",
    "\n",
    "* $\\epsilon$ in epsilon-greedy: controls how much you explore randomly.\n",
    "* Optimistic Initialization: starts with high $Q(a)$ values to encourage trying all actions early.\n",
    "* $c$ in UCB: controls how strongly you explore actions you haven’t tried much.\n",
    "\n",
    "---\n",
    "\n",
    "### Contextual Bandit and MDP\n",
    "\n",
    "* **Contextual bandit:** reward depends on some extra information (context).\n",
    "* When actions also change the state (context), it becomes a **Markov Decision Process (MDP)**.\n",
    "* MDPs are the base of reinforcement learning, which deals with balancing exploration and exploitation in changing situations.\n",
    "\n",
    "---\n",
    "\n",
    "Want me to explain any of these policies in more detail?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a3a1a",
   "metadata": {},
   "source": [
    " \n",
    "### **Softmax (Boltzmann) Policy**:\n",
    "  - Uses a probabilistic approach where actions with higher $ Q(a) $ values are more likely to be chosen.\n",
    "  - **Mathematically**:\n",
    "    $$\n",
    "    \\pi_{\\text{softmax}}(a) = \\frac{\\exp(Q(a)/\\tau)}{\\sum_{b \\in A} \\exp(Q(b)/\\tau)}\n",
    "    $$\n",
    "    Where $ \\tau $ (temperature parameter) controls the level of exploration. A high $ \\tau $ encourages exploration (more randomness), while a low $ \\tau $ encourages exploitation of the best-known actions.\n",
    "\n",
    "### **Upper Confidence Bound (UCB) Policy** (Exploration approach):\n",
    "  - **Mathematically**:\n",
    "    $$\n",
    "    \\pi_{\\text{UCB}}(a) = \\begin{cases} \n",
    "    1, & \\text{if } a = \\arg\\max_{b} \\left[ Q(b) + c \\cdot \\sqrt{\\frac{\\ln t}{n(b)}} \\right] \\\\\n",
    "    0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    Here, $ n(a) $ is the number of times action $ a $ has been taken, $ t $ is the current timestep, and $ c $ is a constant controlling exploration.\n",
    "    \n",
    "     The UCB formula **ensures untried actions are explored more**.\n",
    "\n",
    "![](../../Files/fourth-semester/rl/6.png)\n",
    "\n",
    "---\n",
    "### Hyperparameters\n",
    "\n",
    "- **$ \\epsilon $-Greedy**: $ \\epsilon $ controls exploration; higher $ \\epsilon $ increases exploration.\n",
    "- **Optimistic Initialization**: The **initial value** encourages exploration by setting high initial rewards.\n",
    "- **UCB**: The **$ c $** parameter scales exploration; higher $ c $ increases exploration.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Contextual Bandit**\n",
    "\n",
    "- Often the reward distribution of the bandit you face depends on context\n",
    "\n",
    "- When the state also changes based on our action we call it a\n",
    "Markov Decision Process (MDP)\n",
    "\n",
    "- MDPs are the foundation of reinforcement learning, where the goal is to balance exploration and exploitation in dynamic environments."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
