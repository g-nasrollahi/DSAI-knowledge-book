{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0e5e0",
   "metadata": {},
   "source": [
    "- **RL main components**:\n",
    "  - **Policy**: The strategy for selecting actions.\n",
    "  - **Reward Function $R$**: Determines immediate desirability of states/actions.\n",
    "  - **Value Function $V$**: Measures long-term expected rewards.\n",
    "  - **Model (optional)**: Predicts state transitions for planning.\n",
    "- **Types of Reinforcement Learning**:\n",
    "  - **MDP (Markov Decision Process)**: You have clear states, actions, and rewards. The system’s next state depends only on the current state and action (Markov property).\n",
    "    - **Example**: A robot vacuum cleaner deciding where to go next based on its current location and the dirt it detects.\n",
    "  - **POMDP (Partially Observable MDP)**: Like MDP, but you don’t fully see the state. You get some clues (observations) about the state but not the full picture.\n",
    "    - **Example**: A self-driving car in foggy weather, using limited visibility to make driving decisions.\n",
    "  - **Bandit Problems**: No states, only actions and rewards. Balance exploration (trying new things) and exploitation (using what works).\n",
    "    - **Example**: A website testing different ads to see which ad gets more clicks. Each ad is an action, and clicks are rewards.\n",
    "    - **Common Algorithms**:\n",
    "      - **ε-Greedy** – Explores randomly with probability **ε**, otherwise exploits the best-known action.\n",
    "      - **UCB (Upper Confidence Bound)** – Picks actions based on confidence intervals, favoring uncertain options optimistically.\n",
    "      - **Softmax (Boltzmann Exploration)**\n",
    "      - **Thompson Sampling** – Uses Bayesian sampling to pick actions based on estimated reward probabilities.\n",
    "\n",
    "    - **Applications**: A/B testing, recommendation systems, and online advertising.\n",
    "\n",
    "    - **Types**:\n",
    "      - **Multi-Armed Bandit (MAB)**: A single agent chooses between multiple actions (like pulling levers on slot machines).\n",
    "        - **Example**: A recommendation system suggesting different products to users.\n",
    "      - **Contextual Bandit**: Reward depends on some extra information (context). When actions also change the state (context), it becomes a Markov Decision Process (MDP).\n",
    "        - **Example**: Netflix recommends a movie (**action**) based on your watch history (**state**). You watch it (**reward = 1**) or skip it (**reward = 0**). \n",
    "      - **Bayesian Bandit**: Uses Bayesian methods to update beliefs about the best action based on observed rewards.\n",
    "        - **Example**: A medical trial adjusting treatment recommendations based on patient responses. \n",
    "  - **Multi-Agent RL**: Multiple agents learning together.\n",
    "    - **Example**: Multiple robots in a warehouse.\n",
    "  - **Model-Based RL**: Learns a model of the environment, (how states change with actions) and uses it to plan the best actions.\n",
    "    - **Example**: A chess-playing AI that simulates possible moves and their outcomes before making a decision.\n",
    "    - **Algorithms**:\n",
    "      - **Monte Carlo Tree Search (MCTS)**\n",
    "      - **Dyna**\n",
    "  - **Model-Free RL**: Learns directly from experience without a model.\n",
    "    - **Example**: Teaching a robot to walk by trying different movements and learning from success or failure without knowing physics equations.\n",
    "    - **Types**:\n",
    "      - **Policy Gradient**(Learn the policy directly):\n",
    "        - **REINFORCE**\n",
    "      - **Learn value functions**:\n",
    "        - **Q-Learning**: Learns value of actions independent of policy.\n",
    "        - **SARSA**: Learns action values based on the action actually taken.\n",
    "        - **Deep Q-Networks (DQN)**: Use neural networks to approximate Q-values for complex states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade29de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c4c85",
   "metadata": {},
   "source": [
    "- **Initialization of Bandits $Q(a)$ in** $Q(a) = \\mathbb{E}_{r \\sim p(r|a)}[r]$\n",
    "    - **Realistic Initialization**: $ Q(a)=0, starts with guessing zero expected reward till algorithm updates it based on observed rewards.\n",
    "    - **Optimistic Initialization**:  $ Q(a)=ψ , ψ > 0$ make initial rewards high to encourage exploration.\n",
    "- **Objective function**: Maximize expected cumulative reward over time $ T $ under a policy $ \\pi $.\n",
    "    - $J_T(\\pi) = \\mathbb{E}_{a_t \\sim \\pi, r_t \\sim p(r|a_t)}\\left[\\sum_{t=1}^{T} r_t\\right]$\n",
    "    - This formula is a general way to measure how good a policy is.\n",
    "    - **Policies in Bandit Problems**: \n",
    "        - **Deterministic Policy**: Always choose the same action for a given state.\n",
    "        - **Greedy Policy**: Highest estimated reward $\\to$ action\n",
    "        - **Epsilon-Greedy Policy**: Mostly greedy, but sometimes explores random actions.\n",
    "        - **Softmax Policy**: Chooses actions based on their estimated rewards, with a temperature parameter controlling exploration vs. exploitation.\n",
    "        - **Thompson Sampling**: Samples actions based on their probability of being optimal, balancing exploration and exploitation. \n",
    "- **Update the mean reward**:\n",
    "    - **Incremental Mean Update**: \n",
    "      - $Q_{n} = Q_{n-1} + \\frac{1}{n} [r_{n} - Q_{n-1}]$\n",
    "      - Where $N(a)$ is the number of times action $a$ has been selected.\n",
    "    - **Learning update** (Exponential Moving Average): \n",
    "      - $Q_n \\leftarrow Q_{n-1} + \\alpha \\left[ r_n - Q_{n-1} \\right]$\n",
    "      - Simply move the new mean a bit in the direction of the last observed reward, where $\\alpha$ is the learning rate (0 < $\\alpha$ < 1).\n",
    "- **Epsilon-Greedy Policy**:\n",
    "    - **How it works**:\n",
    "      - With probability $\\epsilon$, choose a random action.\n",
    "      - With probability $1 - \\epsilon$, choose the action with the highest estimated reward.\n",
    "      - Epsilon-greedy exploration is simple but can waste time trying bad actions equally.\n",
    "        - **Small** $\\epsilon$, the policy mostly exploits the best action.\n",
    "        - **Large** $\\epsilon$, the policy explores more.\n",
    "- **Softmax (Boltzmann) Policy**:\n",
    "    - **How it works**:\n",
    "      - Assigns probabilities to actions based on their estimated rewards.\n",
    "      - Higher estimated rewards lead to higher probabilities of being chosen. (So not a single highest will be chosen, sometimes lower rewards (but still high like rank 2nd, 3rd..) will be chosen too because of high probability).\n",
    "      - The temperature parameter ($\\tau$) controls exploration vs. exploitation:\n",
    "        - **High $\\tau$**: More exploration (actions are chosen more uniformly).\n",
    "        - **Low $\\tau$**: More exploitation (actions with higher rewards are chosen more frequently).\n",
    "- **Upper Confidence Bound (UCB) Policy**:\n",
    "    - Picks the action with the highest value of: $Q(a) + c \\times \\sqrt{\\frac{\\ln t}{n(a)}}$\n",
    "    - Where:\n",
    "      - $Q(a)$ is the estimated value of action $a$.\n",
    "      - $c$ is a constant that controls exploration.\n",
    "        - **Higher $c$**: More exploration (more weight on uncertainty).\n",
    "        - **Lower $c$**: More exploitation (focus on known rewards).\n",
    "      - $t$ is the total number of actions taken.\n",
    "      - $n(a)$ is the number of times action $a$ has been selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7567af7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- **Trace $\\tau$**: A sequence of policy based state-action-reward pairs like $\\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \\ldots)$.\n",
    "    - **Infinite Horizon**: Sum rewards infinitely unless a terminal state is reached. $ R(\\tau) = \\sum_{i=0}^{\\infty} \\gamma^i \\cdot r_{t+i} $\n",
    "- **Return $ R(\\tau) $**: The cumulative sum of rewards from a trace\n",
    "    - **Discounted Return** gives less importance to future rewards. $ R(\\tau) = r_t + \\gamma \\cdot r_{t+1} + \\gamma^2 \\cdot r_{t+2} + \\dots $\n",
    "- **Value**: The expected return $ v^\\pi(s) $, $ q^\\pi(s,a) $\n",
    "- **Optimal Value/Policy**: \n",
    "    - $ v^*(s) $: Optimal (maximum expected) value function for state $s$.\n",
    "    - $ q^*(s,a) $: Optimal (maximum expected) action-value function for state $s$ and action $a$.\n",
    "    - $ \\pi^* $: Optimal (maximum expected) policy that achieves the optimal value (maximizes expected return).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f80a79",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "- **Policy Iteration**: Find optimal policy by iteratively improving the policy based on the value function. Fast converges in small states. Caulculate two step per itteration.\n",
    "  - **Steps**:\n",
    "    - 1. Select random initial policy.\n",
    "    - 2. **Policy Evaluation**: Calculate the value function for the current policy.\n",
    "    - 3. **Policy Improvement**: Update the policy based on the value function.\n",
    "    - 4. Repeat **steps 2-3** until the policy converges (no changes).\n",
    "\n",
    "- **Value Iteration**: Combines both **policy evaluation** and **policy improvement** into a single step to find optimal policy by iteratively updating the value function until convergence. A single step but takes longer due updates the value function iteratively until it converges.\n",
    "    - **Steps**:\n",
    "        - 1. Initialize the value function $V(s)$ arbitrarily (e.g., zeros).\n",
    "        - 2. For each state $s$ we calculate the value for **all actions** at state $s$, pick the maximum, and assign that as the new $V(s)$.\n",
    "        - 3. Repeat step 2 for all states until the value function converges (changes become very small).\n",
    "        - 4. After convergence, we have the best policy now.\n",
    "- **Policy Iteration:** Small–medium states, fewer but slower iterations while **Value Iteration:** Medium–large states, more but faster iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b2496",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
