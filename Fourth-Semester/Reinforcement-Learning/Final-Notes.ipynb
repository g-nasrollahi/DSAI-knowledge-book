{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0e5e0",
   "metadata": {},
   "source": [
    "- **RL main components**:\n",
    "  - **Policy**: The strategy for selecting actions.\n",
    "  - **Reward Function $R$**: Determines immediate desirability of states/actions.\n",
    "  - **Value Function $V$**: Measures long-term expected rewards.\n",
    "  - **Model (optional)**: Predicts state transitions for planning.\n",
    "- **Types of Reinforcement Learning**:\n",
    "  - **MDP (Markov Decision Process)**: You have clear states, actions, and rewards. The system’s next state depends only on the current state and action (Markov property).\n",
    "    - **Example**: A robot vacuum cleaner deciding where to go next based on its current location and the dirt it detects.\n",
    "  - **POMDP (Partially Observable MDP)**: Like MDP, but you don’t fully see the state. You get some clues (observations) about the state but not the full picture.\n",
    "    - **Example**: A self-driving car in foggy weather, using limited visibility to make driving decisions.\n",
    "  - **Bandit Problems**: No states, only actions and rewards. Balance exploration (trying new things) and exploitation (using what works).\n",
    "    - **Example**: A website testing different ads to see which ad gets more clicks. Each ad is an action, and clicks are rewards.\n",
    "    - **Common Algorithms**:\n",
    "      - **ε-Greedy** – Explores randomly with probability **ε**, otherwise exploits the best-known action.\n",
    "      - **UCB (Upper Confidence Bound)** – Picks actions based on confidence intervals, favoring uncertain options optimistically.\n",
    "      - **Softmax (Boltzmann Exploration)**\n",
    "      - **Thompson Sampling** – Uses Bayesian sampling to pick actions based on estimated reward probabilities.\n",
    "\n",
    "    - **Applications**: A/B testing, recommendation systems, and online advertising.\n",
    "\n",
    "    - **Types**:\n",
    "      - **Multi-Armed Bandit (MAB)**: A single agent chooses between multiple actions (like pulling levers on slot machines).\n",
    "        - **Example**: A recommendation system suggesting different products to users.\n",
    "      - **Contextual Bandit**: Reward depends on some extra information (context). When actions also change the state (context), it becomes a Markov Decision Process (MDP).\n",
    "        - **Example**: Netflix recommends a movie (**action**) based on your watch history (**state**). You watch it (**reward = 1**) or skip it (**reward = 0**). \n",
    "      - **Bayesian Bandit**: Uses Bayesian methods to update beliefs about the best action based on observed rewards.\n",
    "        - **Example**: A medical trial adjusting treatment recommendations based on patient responses. \n",
    "  - **Multi-Agent RL**: Multiple agents learning together.\n",
    "    - **Example**: Multiple robots in a warehouse.\n",
    "  - **Model-Based RL**: Learns a model of the environment, (how states change with actions) and uses it to plan the best actions.\n",
    "    - **Example**: A chess-playing AI that simulates possible moves and their outcomes before making a decision.\n",
    "    - **Algorithms**:\n",
    "      - **Monte Carlo Tree Search (MCTS)**\n",
    "      - **Dyna**\n",
    "  - **Model-Free RL**: Learns directly from experience without a model.\n",
    "    - **Example**: Teaching a robot to walk by trying different movements and learning from success or failure without knowing physics equations.\n",
    "    - **Types**:\n",
    "      - **Policy Gradient**(Learn the policy directly):\n",
    "        - **REINFORCE**\n",
    "      - **Learn value functions**:\n",
    "        - **Q-Learning**: Learns value of actions independent of policy.\n",
    "        - **SARSA**: Learns action values based on the action actually taken.\n",
    "        - **Deep Q-Networks (DQN)**: Use neural networks to approximate Q-values for complex states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade29de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c4c85",
   "metadata": {},
   "source": [
    "- **Initialization of Bandits $Q(a)$ in** $Q(a) = \\mathbb{E}_{r \\sim p(r|a)}[r]$\n",
    "    - **Realistic Initialization**: $ Q(a)=0, starts with guessing zero expected reward till algorithm updates it based on observed rewards.\n",
    "    - **Optimistic Initialization**:  $ Q(a)=ψ , ψ > 0$ make initial rewards high to encourage exploration.\n",
    "- **Objective function**: Maximize expected cumulative reward over time $ T $ under a policy $ \\pi $.\n",
    "    - $J_T(\\pi) = \\mathbb{E}_{a_t \\sim \\pi, r_t \\sim p(r|a_t)}\\left[\\sum_{t=1}^{T} r_t\\right]$\n",
    "    - This formula is a general way to measure how good a policy is.\n",
    "    - **Policies in Bandit Problems**: \n",
    "        - **Deterministic Policy**: Always choose the same action for a given state.\n",
    "        - **Greedy Policy**: Highest estimated reward $\\to$ action\n",
    "        - **Epsilon-Greedy Policy**: Mostly greedy, but sometimes explores random actions.\n",
    "        - **Softmax Policy**: Chooses actions based on their estimated rewards, with a temperature parameter controlling exploration vs. exploitation.\n",
    "        - **Thompson Sampling**: Samples actions based on their probability of being optimal, balancing exploration and exploitation. \n",
    "- **Update the mean reward**:\n",
    "    - **Incremental Mean Update**: \n",
    "      - $Q_{n} = Q_{n-1} + \\frac{1}{n} [r_{n} - Q_{n-1}]$\n",
    "      - Where $N(a)$ is the number of times action $a$ has been selected.\n",
    "    - **Learning update** (Exponential Moving Average): \n",
    "      - $Q_n \\leftarrow Q_{n-1} + \\alpha \\left[ r_n - Q_{n-1} \\right]$\n",
    "      - Simply move the new mean a bit in the direction of the last observed reward, where $\\alpha$ is the learning rate (0 < $\\alpha$ < 1).\n",
    "- **Epsilon-Greedy Policy**:\n",
    "    - **How it works**:\n",
    "      - With probability $\\epsilon$, choose a random action.\n",
    "      - With probability $1 - \\epsilon$, choose the action with the highest estimated reward.\n",
    "      - Epsilon-greedy exploration is simple but can waste time trying bad actions equally.\n",
    "        - **Small** $\\epsilon$, the policy mostly exploits the best action.\n",
    "        - **Large** $\\epsilon$, the policy explores more.\n",
    "- **Softmax (Boltzmann) Policy**:\n",
    "    - **How it works**:\n",
    "      - Assigns probabilities to actions based on their estimated rewards.\n",
    "      - Higher estimated rewards lead to higher probabilities of being chosen. (So not a single highest will be chosen, sometimes lower rewards (but still high like rank 2nd, 3rd..) will be chosen too because of high probability).\n",
    "      - The temperature parameter ($\\tau$) controls exploration vs. exploitation:\n",
    "        - **High $\\tau$**: More exploration (actions are chosen more uniformly).\n",
    "        - **Low $\\tau$**: More exploitation (actions with higher rewards are chosen more frequently).\n",
    "- **Upper Confidence Bound (UCB) Policy**:\n",
    "    - Picks the action with the highest value of: $Q(a) + c \\times \\sqrt{\\frac{\\ln t}{n(a)}}$\n",
    "    - Where:\n",
    "      - $Q(a)$ is the estimated value of action $a$.\n",
    "      - $c$ is a constant that controls exploration.\n",
    "        - **Higher $c$**: More exploration (more weight on uncertainty).\n",
    "        - **Lower $c$**: More exploitation (focus on known rewards).\n",
    "      - $t$ is the total number of actions taken.\n",
    "      - $n(a)$ is the number of times action $a$ has been selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7567af7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- **Trace $\\tau$**: A sequence of policy based state-action-reward pairs like $\\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \\ldots)$.\n",
    "    - **Infinite Horizon**: Sum rewards infinitely unless a terminal state is reached. $ R(\\tau) = \\sum_{i=0}^{\\infty} \\gamma^i \\cdot r_{t+i} $\n",
    "- **Return $ R(\\tau) $**: The cumulative sum of rewards from a trace\n",
    "    - **Discounted Return** gives less importance to future rewards. $ R(\\tau) = r_t + \\gamma \\cdot r_{t+1} + \\gamma^2 \\cdot r_{t+2} + \\dots $\n",
    "- **Value**: The expected return $ v^\\pi(s) $, $ q^\\pi(s,a) $\n",
    "- **Optimal Value/Policy**: \n",
    "    - $ v^*(s) $: Optimal (maximum expected) value function for state $s$.\n",
    "    - $ q^*(s,a) $: Optimal (maximum expected) action-value function for state $s$ and action $a$.\n",
    "    - $ \\pi^* $: Optimal (maximum expected) policy that achieves the optimal value (maximizes expected return).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f80a79",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "- **Policy Iteration**: Find optimal policy by iteratively improving the policy based on the value function. Fast converges in small states. Caulculate two step per itteration.\n",
    "  - **Steps**:\n",
    "    - 1. Select random initial policy.\n",
    "    - 2. **Policy Evaluation**: Calculate the value function for the current policy.\n",
    "    - 3. **Policy Improvement**: Update the policy based on the value function.\n",
    "    - 4. Repeat **steps 2-3** until the policy converges (no changes).\n",
    "\n",
    "- **Value Iteration**: Combines both **policy evaluation** and **policy improvement** into a single step to find optimal policy by iteratively updating the value function until convergence. A single step but takes longer due updates the value function iteratively until it converges.\n",
    "    - **Steps**:\n",
    "        - 1. Initialize the value function $V(s)$ arbitrarily (e.g., zeros).\n",
    "        - 2. For each state $s$ we calculate the value for **all actions** at state $s$, pick the maximum, and assign that as the new $V(s)$.\n",
    "        - 3. Repeat step 2 for all states until the value function converges (changes become very small).\n",
    "        - 4. After convergence, we have the best policy now.\n",
    "- **Policy Iteration:** Small–medium states, fewer but slower iterations while **Value Iteration:** Medium–large states, more but faster iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b2496",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016300d4",
   "metadata": {},
   "source": [
    "- Dynamic Programming finds the best policy (best actions) when you know **everything** about the environment. $\\to$ Policy Iteration.\n",
    "- When we **don't have full knowledge** of the environment, we use **Monte Carlo methods** to learn from **experience**. $\\to$ Model-Free RL.\n",
    "- **Monte Carlo Methods**: Learn from complete episodes (traces) to estimate value functions.\n",
    "  - MC estimates $v^\\pi(s)$ and $q^\\pi(s,a)$ by averaging returns from multiple episodes we have seen before.\n",
    "    - **First-Visit MC**: Averages returns from the first time each state-action pair is visited in an episode.\n",
    "    - **Every-Visit MC**: Averages returns from every time each state-action pair is visited in an episode.\n",
    "  - Begin episodes from random state-action pairs to ensure all pairs get visited sometimes because if some states/action pairs are never visited, MC can’t estimate their values.\n",
    "  - $\\epsilon$-greedy exploration is often used in MC to ensure all state-action pairs are explored.\n",
    "  - Monte Carlo can be used inside **Generalized Policy Iteration (GPI)**:\n",
    "    - **Two main types**:\n",
    "      - **On-Policy MC**: Evaluates and improves the policy being used to generate episodes.\n",
    "      - **Off-Policy MC**: Evaluates a different policy than the one being used to generate episodes.\n",
    "        - **Example**: Using a behavior policy to explore while evaluating a target policy.\n",
    "          - **Behavior Policy**: The policy used to generate episodes (exploration).\n",
    "          - **Target Policy**: The policy we want to improve (evaluation).\n",
    "        - **Problem**: Behavior and target policies can be different and make different decisions. We can't just average rewards because episodes came from a different policy. \n",
    "          - **Solution** = **Importance Sampling**: Give a weight to each episode based on how likely it was under the target policy compared to the behavior policy.\n",
    "\n",
    "- **Temporal Difference (TD) Learning**: another way to learn from **experience**, combines Monte Carlo and dynamic programming. It learns **step-by-step** during an episode instead of waiting until the episode ends.\n",
    "  - **MC**: Waits until the end of an episode to update values.\n",
    "  - **TD**: Updates values after each step using the immediate reward and estimated value of the next state.\n",
    "  - **Update Rule**: $ v(s_t) \\leftarrow v(s_t) + \\alpha \\left[ r_{t+1} + \\gamma v(s_{t+1}) - v(s_t) \\right]$\n",
    "    - Part in $[\\cdots]$ is called the **TD error**, which will correct the value esti,ate right after the action is taken unlike MC. This part is **policy evaluation** and the whole update is **policy improvement**.\n",
    "\n",
    "| Aspect              | Monte Carlo (MC)                    | Temporal Difference (TD)                     |\n",
    "| ------------------- | ----------------------------------- | -------------------------------------------- |\n",
    "| When update happens | After whole episode ends            | After every step                             |\n",
    "| Use of next state   | No (waits for total return)         | Yes (bootstraps with estimated $v(s_{t+1})$) |\n",
    "| Variance            | High (because it uses total return) | Lower (uses one-step estimate)               |\n",
    "| Bias                | No bias (unbiased)                  | Slight bias due to bootstrapping             |\n",
    "| Learning speed      | Slower                              | Faster and more online                       |\n",
    "\n",
    "- **TD Control algorithms**\n",
    "  - **SARSA (On-policy TD Control)**: Learns action values for the policy being followed. (Learning how good a move is based on the next move you actually make.)\n",
    "    - Update rule: $Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]$\n",
    "  - **Q-learning (Off-policy TD Control)**: Learns the optimal action values regardless of the policy being followed. (Uses the **maximum** estimated value of the next state (best possible action), not necessarily the action actually taken.)\n",
    "    - Update rule: $Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]$\n",
    "    - **Maximization Bias**: Q-learning can overestimate action values due to always selecting the maximum Q-value in future states.\n",
    "    - **Double Q-learning**: Addresses this bias by maintaining two separate Q-tables and using one to select actions and the other to evaluate them, reducing overestimation.\n",
    "\n",
    "  - **Expected SARSA**: A variant of SARSA that uses the expected value over possible next actions instead of the single next action.\n",
    "    - Update rule: $Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\sum_a \\pi(a|s_{t+1}) Q(s_{t+1}, a) - Q(s_t, a_t) \\right]$\n",
    "\n",
    "\n",
    "- **N-step TD and N-step SARSA**: Extensions of TD and SARSA that consider multiple steps for better value estimation.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "| Algorithm      | On-policy / Off-policy | Next state update                | Main idea                                           |\n",
    "| -------------- | ---------------------- | -------------------------------- | --------------------------------------------------- |\n",
    "| SARSA          | On-policy              | Uses next action actually taken  | Learns action values following policy               |\n",
    "| Q-learning     | Off-policy             | Uses max action value next state | Learns optimal action values regardless of behavior |\n",
    "| Expected SARSA | On or Off-policy       | Uses expected value over actions | Averages over possible next actions                 |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
