{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0172ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### **Access to MDP Dynamics**\n",
    "#### **Reversible vs. Irreversible Access:**\n",
    "\n",
    "  * **Model (Reversible Access):** In a model, we can query the MDP dynamics at any time and get the probability distribution $p(s' | s, a)$.\n",
    "  * **Environment (Irreversible Access):** In an environment, after taking an action $a$, we cannot undo it. We must move forward and observe the resulting state $s'$.\n",
    "* **Distribution vs. Sample Models:**\n",
    "\n",
    "  * **Distribution Models:** You get the full probability distribution over next states \n",
    "  * **Sample Models:**  You get only one sample of the next state\n",
    "  \n",
    "#### **Planning vs. Learning**\n",
    "\n",
    "The distinction between **planning** and **learning** lies in two factors:\n",
    "\n",
    "<img src=\"../../Files/fourth-semester/rl/10.png\" alt=\"Planning vs. Learning\" width=\"400\">\n",
    "\n",
    "* **Access to MDP Dynamics:**\n",
    "\n",
    "  * **Planning:** Uses a model with reversible access, allowing for queries on any state-action pair. Stores a **local solution**\n",
    "  * **Reinforcement Learning (RL):** Uses an environment with irreversible access, where you must proceed through the environment step-by-step. Stores a **global solution**\n",
    "\n",
    "|                            | Local solution               | Global solution              |\n",
    "|----------------------------|------------------------------|------------------------------|\n",
    "| **Reversible MDP access**   | Planning (e.g., MCTS)        | Borderline/Model-based RL (e.g., Dynamic Programming) |\n",
    "| **Irreversible MDP access** | (impossible)                 | Model-free RL (e.g., Q-learning) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d761553",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "### **Back-ups:**\n",
    "\n",
    "In reinforcement learning, a **back-up** refers to how we update the value function or action-value function when estimating the value of a state or action.\n",
    "\n",
    "* **Expected Back-ups:**\n",
    "These are primarily used in **planning**. In planning, we have a model (reversible access to MDP dynamics), so we can fully calculate the expected value of the next state.\n",
    "\n",
    "* **Sample Back-ups:**\n",
    "These are mostly used in **reinforcement learning** (RL), where you often do not have a model of the environment. Instead, you experience one transition at a time and use that experience to update your estimates.\n",
    "\n",
    "### **Back-up Diagrams:**\n",
    "\n",
    "The back-up diagrams visually show how updates are made in the **state values** $V(s)$ and **state-action values** $Q(s,a)$. The diagrams correspond to different types of updates.\n",
    "\n",
    "* **On-policy vs Off-policy:**\n",
    "\n",
    "  * **On-policy**: The value or Q-values are updated using the same policy that is being followed (e.g., **Sarsa** where the policy is followed directly).\n",
    "  * **Off-policy**: The value or Q-values are updated using a different policy than the one being followed (e.g., **Q-learning** where the greedy policy is used to update, but the agent may be exploring randomly).\n",
    "\n",
    "\n",
    "### **1-Step vs. Multi-Step Back-ups:**\n",
    "\n",
    "* **1-Step Back-ups (Shallow Updates):**\n",
    "\n",
    "  * These updates involve only a single step (transition from one state to the next) and are **shallow** in terms of the depth of the update.\n",
    "  * **TD(0)** and **Monte Carlo** methods are examples of shallow updates because they only consider the immediate next state or the final return (in the case of Monte Carlo).\n",
    "\n",
    "* **Multi-Step Back-ups (Deep Updates):**\n",
    "\n",
    "  * **Multi-step updates** involve looking further into the future, considering multiple steps in the environment (depth).\n",
    "  * For example, **Monte Carlo** learning looks at complete episodes, while **TD(λ)** or **dynamic programming** methods may use longer sequences of states and actions to perform updates.\n",
    "  * These updates are **deeper** and often lead to **more accurate estimates** of value functions but are computationally more expensive.\n",
    "\n",
    "<img src=\"../../Files/fourth-semester/rl/11.png\" alt=\"Planning vs. Learning\" width=\"600\">\n",
    "<img src=\"../../Files/fourth-semester/rl/12.png\" alt=\"Planning vs. Learning\" width=\"600\">\n",
    "  \n",
    "\n",
    "### **Relation to Algorithms:**\n",
    "\n",
    "* **TD(0)** is a shallow update that uses **sample back-ups** (one-step sample updates).\n",
    "* **Monte Carlo** can be a **multi-step** back-up method (it performs updates over entire episodes).\n",
    "* **Dynamic Programming (DP)** involves **expected back-ups** over the entire state space and is typically used in planning where the model is fully known.\n",
    "\n",
    "### **Notes:**\n",
    "* **Expected back-ups** are used in **planning** where you have a model and can compute the full expected value over the next state.\n",
    "* **Sample back-ups** are used in **RL** when you only have access to sampled experiences.\n",
    "* **Shallow updates** look at immediate outcomes (1-step), while **deep updates** consider longer-term effects (multi-step).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2ee30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ed5e2",
   "metadata": {},
   "source": [
    "#### **Tabular Model Learning**\n",
    "\n",
    "In a tabular model, we learn both the **transition dynamics** and the **reward function** by collecting samples of transitions:\n",
    "\n",
    "* **Transition counts:** $n(s, a, s')$ is the number of times we transition from state $s$ to $s'$ using action $a$.\n",
    "* **Reward sums:** $R_{\\text{sum}}(s, a, s')$ is the sum of rewards obtained when transitioning from $s$ to $s'$ using action $a$.\n",
    "\n",
    "Using these counts, we can estimate:\n",
    "\n",
    "* **Transition model:**\n",
    "\n",
    "  $$\n",
    "  \\hat{p}(s'|s, a) = \\frac{n(s, a, s')}{n(s, a)}\n",
    "  $$\n",
    "* **Reward model:**\n",
    "\n",
    "  $$\n",
    "  \\hat{r}(s, a, s') = \\frac{R_{\\text{sum}}(s, a, s')}{n(s, a, s')}\n",
    "  $$\n",
    "\n",
    "\n",
    "### Algorithm 1: Tabular Model Update Pseudo-code\n",
    "\n",
    "```text\n",
    "Algorithm 1: Tabular model update pseudo-code. \n",
    "Input: Maximum number of timesteps T. \n",
    "Initialization: Initialize n(s, a, s0) = 0 and Rsum(s, a, s0) = 0 ∀s ∈ S, a ∈ A\n",
    "\n",
    "repeat T times\n",
    "    Observe hs, a, r, s0i  /* Observe new transition */\n",
    "    n(s, a, s0) ← n(s, a, s0) + 1  /* Update transition counts */\n",
    "    Rsum(s, a, s0) ← Rsum(s, a, s0) + r  /* Update reward sums */\n",
    "    pˆ(s0|s, a) ← n(s,a,s0 / ∑s0 n(s,a,s0)) /* Estimate transition function */\n",
    "    rˆ(s, a, s0) ← Rsum(s,a,s0) / n(s,a,s0) /* Estimate reward function */\n",
    "    pˆ(s, a|s0) ← n(s,a,s0 / ∑s,a n(s,a,s0))  /* Reverse model (only for PS) */\n",
    "end\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb26eca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "## **Model-based RL Algorithms**\n",
    "\n",
    "Model-based RL methods combine planning and learning by using a learned model to simulate the environment.\n",
    "\n",
    "## Learning a model from data\n",
    "\n",
    "Given a dataset of observed transitions, we can estimate:\n",
    "\n",
    "* **Dynamics model:** The probability distribution $p(s'|s,a)$.\n",
    "* **Reward function:** The expected reward $r(s,a,s')$.\n",
    "\n",
    "To do this:\n",
    "\n",
    "1. **Track counts** $n(s,a,s')$ and reward sums $R_{\\text{sum}}(s,a,s')$.\n",
    "2. Estimate the transition model as:\n",
    "\n",
    "   $$\n",
    "   \\hat{p}(s'|s,a) = \\frac{n(s,a,s')}{n(s,a)}\n",
    "   $$\n",
    "3. Estimate the reward model as:\n",
    "\n",
    "   $$\n",
    "   \\hat{r}(s,a,s') = \\frac{R_{\\text{sum}}(s,a,s')}{n(s,a,s')}\n",
    "   $$\n",
    "\n",
    "### 1- **Real-time Dynamic Programming (RTDP):**\n",
    "\n",
    "Real-time Dynamic Programming (RTDP) is a variant of Dynamic Programming that efficiently combines planning and learning. Here's a quick breakdown:\n",
    "\n",
    "* **Classic Bridging Algorithm**: RTDP updates values using the Bellman optimality equation, similar to Dynamic Programming, but focuses on reachable states instead of all states in the space.\n",
    "\n",
    "* **Curse of Dimensionality**: Traditional DP is inefficient in large state spaces because it updates all states, even unreachable ones. RTDP solves this by focusing only on states that are actually visited.\n",
    "\n",
    "* **Efficient Updates**: RTDP applies updates only to states that are part of the trajectory from the start, making it more efficient and scalable.\n",
    "\n",
    "* **Uniform Updates**: While traditional DP updates all states uniformly, RTDP targets only reachable states, improving performance in large problems.\n",
    "\n",
    "In short, **RTDP** reduces unnecessary calculations by focusing on reachable states, making it more efficient for large state spaces.\n",
    "\n",
    "### 2- **Dyna:**\n",
    "\n",
    "Dyna is a model-based RL algorithm that combines learning and planning. First, we **learn a model** of the environment, then we use the model to simulate **one-step planning updates** to our value function.\n",
    "\n",
    "The algorithm updates the Q-values using both actual environment samples and simulated samples from the learned model. This helps improve **data efficiency** by leveraging the model.\n",
    "\n",
    "Algorithm details for Dyna Q-learning:\n",
    "\n",
    "1. Initialize Q-values, transition counts, and reward sums.\n",
    "2. For each timestep:\n",
    "\n",
    "   * Take an action using an $\\epsilon$-greedy policy.\n",
    "   * Observe the transition and update the model.\n",
    "   * Perform Q-learning updates using real experiences.\n",
    "   * Perform planning updates using simulated transitions from the model.\n",
    "\n",
    "### Algorithm 2: Dyna Q-learning with Epsilon-Greedy Exploration\n",
    "```text\n",
    "Input: Number of planning updates K, exploration parameter epsilon ∈ (0, 1], learning rate alpha ∈ (0, 1], discount parameter gamma ∈ [0, 1], maximum number of timesteps T.\n",
    "Initialization: Initialize Q(s, a) = 0, n(s, a, s0) = 0, Rsum(s, a, s0) = 0 ∀s ∈ S, a ∈ A.\n",
    "\n",
    "for t = 1...T do\n",
    "    s ← current state  /* Reset when environment terminates */\n",
    "    a ∼ πε-greedy(a|s)  /* Sample action */\n",
    "    r, s0 ∼ p(r, s0|s, a)  /* Simulate environment */\n",
    "    pˆ(s0, r|s, a) ← Update(s, a, r, s0)  /* Update model (Alg.1) */\n",
    "    Q(s, a) ← Q(s, a) + alpha · [r + gamma · maxa0 Q(s0, a0) − Q(s, a)]  /* Update Q-table */\n",
    "    \n",
    "    repeat K times\n",
    "        s ← random previously observed state  /* State to plan on */\n",
    "        a ← previously taken action in state s  /* Planning action */\n",
    "        s0, r ∼ pˆ(s0, r|s, a)  /* Simulate model */\n",
    "        Q(s, a) ← Q(s, a) + alpha · [r + gamma · maxa0 Q(s0, a0) − Q(s, a)]  /* Update Q-table */\n",
    "    end\n",
    "end\n",
    "```\n",
    "\n",
    "### 3- **Prioritized Sweeping:**\n",
    "\n",
    "Uses both forward and backward models to prioritize states for updating, spreading information faster.\n",
    "\n",
    "Prioritized sweeping focuses on efficiently updating the value function by identifying **states with high-priority updates**:\n",
    "\n",
    "* When the Q-value estimate for a state-action pair changes significantly, the predecessor states that lead to this state should also be updated.\n",
    "* This prioritization helps focus updates on the most promising state-action pairs.\n",
    "\n",
    "The algorithm maintains a **priority queue** where states with larger TD errors (difference between predicted and actual rewards) are prioritized for updates. It also performs **backward search** to identify which states to update based on the value changes.\n",
    "\n",
    "### Prioritized Sweeping (Q-learning with epsilon-greedy exploration)\n",
    "\n",
    "```text\n",
    "Input: Number of planning updates K, exploration parameter epsilon ∈ (0, 1], learning rate alpha ∈ (0, 1], discount parameter gamma ∈ [0, 1], maximum number of timesteps T, priority threshold theta.\n",
    "Initialization: Initialize Q(s, a) = 0, n(s, a, s0) = 0, Rsum(s, a, s0) = 0 ∀s ∈ S, a ∈ A, and prioritized queue PQ.\n",
    "\n",
    "for t = 1...T do\n",
    "    s ← current state  /* Reset when environment terminates */\n",
    "    a ∼ πε-greedy(a|s)  /* Sample action */\n",
    "    r, s0 ∼ p(r, s0|s, a)  /* Simulate environment */\n",
    "    pˆ(s0, r|s, a) ← Update(s, a, r, s0)  /* Update model (Alg.1) */\n",
    "    p ← |r + gamma · maxa0 Q(s0, a0) − Q(s, a)|  /* Compute priority p */\n",
    "    if p > theta then\n",
    "        Insert (s, a) into PQ with priority p  /* State-action needs update */\n",
    "    end\n",
    "    \n",
    "    repeat K times\n",
    "        s, a ← pop highest priority from PQ  /* Sample PQ, break when empty */\n",
    "        s0, r ∼ pˆ(s0, r|s, a)  /* Simulate model */\n",
    "        Q(s, a) ← Q(s, a) + alpha · [r + gamma · maxa0 Q(s0, a0) − Q(s, a)]  /* Update Q-table */\n",
    "        \n",
    "        for all (s, a) with pˆ(s0, a0|s) > 0 do\n",
    "            r¯ = ˆr(s0, a, s)  /* Get reward from model */\n",
    "            p ← |r¯ + gamma · maxa Q(s, a) − Q(s0, a0)|  /* Compute priority p */\n",
    "            if p > theta then\n",
    "                Insert (s, a) into PQ with priority p  /* State-action needs update */\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
