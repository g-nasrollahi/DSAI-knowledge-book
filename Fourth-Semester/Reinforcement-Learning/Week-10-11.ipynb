{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0172ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### **Access to MDP Dynamics**\n",
    "#### **Reversible vs. Irreversible Access:**\n",
    "\n",
    "  * **Model (Reversible Access):** In a model, we can query the MDP dynamics at any time and get the probability distribution $p(s' | s, a)$, i.e., the probability of transitioning to state $s'$ given action $a$ in state $s$.\n",
    "  * **Environment (Irreversible Access):** In an environment, after taking an action $a$, we cannot undo it. We must move forward and observe the resulting state $s'$, making it irreversible.\n",
    "* **Distribution vs. Sample Models:**\n",
    "\n",
    "  * **Distribution Models:** You get the full probability distribution over next states (more complete, but requires more knowledge or computation)\n",
    "  * **Sample Models:**  You get only one sample of the next state, meaning one possible outcome based on the distribution\n",
    "\n",
    "\n",
    "#### **Planning vs. Learning**\n",
    "\n",
    "The distinction between **planning** and **learning** lies in two factors:\n",
    "\n",
    "<img src=\"../../Files/fourth-semester/rl/10.png\" alt=\"Planning vs. Learning\" width=\"400\">\n",
    "\n",
    "* **Access to MDP Dynamics:**\n",
    "\n",
    "  * **Planning:** Uses a model with reversible access, allowing for queries on any state-action pair.\n",
    "  * **Reinforcement Learning (RL):** Uses an environment with irreversible access, where you must proceed through the environment step-by-step.\n",
    "* **Solution Representation:**\n",
    "\n",
    "  * **Planning:** Stores a **local solution**, focusing only on the current state.\n",
    "  * **RL:** Stores a **global solution**, which accounts for all possible states.\n",
    "\n",
    "The boundary between planning and learning can be summarized as:\n",
    "\n",
    "* **Planning**: Uses a local solution with reversible access to the MDP.\n",
    "* **Model-based RL**: Uses a global solution with reversible access (such as Dynamic Programming).\n",
    "* **Model-free RL**: Uses a global solution with irreversible access (e.g., Q-learning).\n",
    "\n",
    "|                            | Local solution               | Global solution              |\n",
    "|----------------------------|------------------------------|------------------------------|\n",
    "| **Reversible MDP access**   | Planning (e.g., MCTS)        | Borderline/Model-based RL (e.g., Dynamic Programming) |\n",
    "| **Irreversible MDP access** | (impossible)                 | Model-free RL (e.g., Q-learning) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d761553",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "### **Back-ups:**\n",
    "\n",
    "In reinforcement learning, a **back-up** refers to how we update the value function or action-value function when estimating the value of a state or action.\n",
    "\n",
    "* **Expected Back-ups:**\n",
    "\n",
    "  * These are primarily used in **planning**. In planning, we have a model (reversible access to MDP dynamics), so we can fully calculate the expected value of the next state.\n",
    "  * In the diagram, the **expected back-up** computes the **Bellman equation** using probabilities (e.g., the transition probabilities and rewards).\n",
    "  * For instance, when calculating the value $V(s)$, you sum over all possible actions $a$ and all possible next states $s'$, weighted by the transition probabilities $p(s'|s,a)$ and rewards $r(s,a,s')$:\n",
    "\n",
    "    $$\n",
    "    V(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} p(s'|s,a) \\left[r(s,a,s') + \\gamma V(s')\\right]\n",
    "    $$\n",
    "  * This is done with **expectation**, meaning you are considering all possible transitions and rewards.\n",
    "\n",
    "* **Sample Back-ups:**\n",
    "\n",
    "  * These are mostly used in **reinforcement learning** (RL), where you often do not have a model of the environment. Instead, you experience one transition at a time and use that experience to update your estimates.\n",
    "  * In the diagram, the **sample back-up** for the **TD(0)** or **Sarsa** algorithm, for example, involves using a single sample (one observed next state $s'$ and reward $r$) to update the estimate for $V(s)$ or $Q(s,a)$.\n",
    "  * In **TD(0)** or **Sarsa**, the update is done based on actual experiences rather than expectations. The update rule in Sarsa (for state-action values) is:\n",
    "\n",
    "    $$\n",
    "    Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma Q(s',a') - Q(s,a) \\right)\n",
    "    $$\n",
    "  * The difference here is that you only get a sample of what actually happened, rather than considering the entire distribution of possible outcomes.\n",
    "\n",
    "### **Back-up Diagrams:**\n",
    "\n",
    "The back-up diagrams visually show how updates are made in the **state values** $V(s)$ and **state-action values** $Q(s,a)$. The diagrams correspond to different types of updates.\n",
    "\n",
    "* **On-policy vs Off-policy:**\n",
    "\n",
    "  * **On-policy**: The value or Q-values are updated using the same policy that is being followed (e.g., **Sarsa** where the policy is followed directly).\n",
    "  * **Off-policy**: The value or Q-values are updated using a different policy than the one being followed (e.g., **Q-learning** where the greedy policy is used to update, but the agent may be exploring randomly).\n",
    "\n",
    "\n",
    "### **1-Step vs. Multi-Step Back-ups:**\n",
    "\n",
    "* **1-Step Back-ups (Shallow Updates):**\n",
    "\n",
    "  * These updates involve only a single step (transition from one state to the next) and are **shallow** in terms of the depth of the update.\n",
    "  * **TD(0)** and **Monte Carlo** methods are examples of shallow updates because they only consider the immediate next state or the final return (in the case of Monte Carlo).\n",
    "  * These updates are **fast** but may not capture the long-term effects of actions well.\n",
    "\n",
    "* **Multi-Step Back-ups (Deep Updates):**\n",
    "\n",
    "  * **Multi-step updates** involve looking further into the future, considering multiple steps in the environment (depth).\n",
    "  * For example, **Monte Carlo** learning looks at complete episodes, while **TD(Î»)** or **dynamic programming** methods may use longer sequences of states and actions to perform updates.\n",
    "  * These updates are **deeper** and often lead to **more accurate estimates** of value functions but are computationally more expensive.\n",
    "\n",
    "<img src=\"../../Files/fourth-semester/rl/11.png\" alt=\"Planning vs. Learning\" width=\"600\">\n",
    "<img src=\"../../Files/fourth-semester/rl/12.png\" alt=\"Planning vs. Learning\" width=\"600\">\n",
    "\n",
    "### **Relation to Algorithms:**\n",
    "\n",
    "* **TD(0)** is a shallow update that uses **sample back-ups** (one-step sample updates).\n",
    "* **Monte Carlo** can be a **multi-step** back-up method (it performs updates over entire episodes).\n",
    "* **Dynamic Programming (DP)** involves **expected back-ups** over the entire state space and is typically used in planning where the model is fully known.\n",
    "\n",
    "### **Notes:**\n",
    "* **Expected back-ups** are used in **planning** where you have a model and can compute the full expected value over the next state.\n",
    "* **Sample back-ups** are used in **RL** when you only have access to sampled experiences.\n",
    "* **Shallow updates** look at immediate outcomes (1-step), while **deep updates** consider longer-term effects (multi-step).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2ee30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf31652",
   "metadata": {},
   "source": [
    "# **To be reviewd again from Slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34717ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Poitns from Lecture Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ed5e2",
   "metadata": {},
   "source": [
    "#### **Tabular Model Learning**\n",
    "\n",
    "In a tabular model, we learn both the **transition dynamics** and the **reward function** by collecting samples of transitions:\n",
    "\n",
    "* **Transition counts:** $n(s, a, s')$ is the number of times we transition from state $s$ to $s'$ using action $a$.\n",
    "* **Reward sums:** $R_{\\text{sum}}(s, a, s')$ is the sum of rewards obtained when transitioning from $s$ to $s'$ using action $a$.\n",
    "\n",
    "Using these counts, we can estimate:\n",
    "\n",
    "* **Transition model:**\n",
    "\n",
    "  $$\n",
    "  \\hat{p}(s'|s, a) = \\frac{n(s, a, s')}{n(s, a)}\n",
    "  $$\n",
    "* **Reward model:**\n",
    "\n",
    "  $$\n",
    "  \\hat{r}(s, a, s') = \\frac{R_{\\text{sum}}(s, a, s')}{n(s, a, s')}\n",
    "  $$\n",
    "\n",
    "#### **Dyna Algorithm**\n",
    "\n",
    "Dyna is a model-based RL algorithm that combines learning and planning:\n",
    "\n",
    "* First, we **learn a model** of the environment.\n",
    "* Then, we use the model to simulate **one-step planning updates** to our value function.\n",
    "\n",
    "The algorithm updates the Q-values using both actual environment samples and simulated samples from the learned model. This helps improve **data efficiency** by leveraging the model.\n",
    "\n",
    "Algorithm details for Dyna Q-learning:\n",
    "\n",
    "1. Initialize Q-values, transition counts, and reward sums.\n",
    "2. For each timestep:\n",
    "\n",
    "   * Take an action using an $\\epsilon$-greedy policy.\n",
    "   * Observe the transition and update the model.\n",
    "   * Perform Q-learning updates using real experiences.\n",
    "   * Perform planning updates using simulated transitions from the model.\n",
    "\n",
    "#### **Prioritized Sweeping**\n",
    "\n",
    "Prioritized sweeping focuses on efficiently updating the value function by identifying **states with high-priority updates**:\n",
    "\n",
    "* When the Q-value estimate for a state-action pair changes significantly, the predecessor states that lead to this state should also be updated.\n",
    "* This prioritization helps focus updates on the most promising state-action pairs.\n",
    "\n",
    "The algorithm maintains a **priority queue** where states with larger TD errors (difference between predicted and actual rewards) are prioritized for updates. It also performs **backward search** to identify which states to update based on the value changes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb26eca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "#### **Model-based RL Algorithms**\n",
    "\n",
    "Model-based RL methods combine planning and learning by using a learned model to simulate the environment:\n",
    "\n",
    "* **Real-time Dynamic Programming (RTDP):** A modification of dynamic programming that focuses on **reachable states** instead of all states, making it more efficient for large state spaces.\n",
    "* **Dyna:** Learns a model from real transitions and uses that model to simulate additional transitions for planning.\n",
    "* **Prioritized Sweeping:** Uses both forward and backward models to prioritize states for updating, spreading information faster.\n",
    "\n",
    "#### **Learning a Model from Data**\n",
    "\n",
    "Given a dataset of observed transitions, we can estimate:\n",
    "\n",
    "* **Dynamics model:** The probability distribution $p(s'|s,a)$.\n",
    "* **Reward function:** The expected reward $r(s,a,s')$.\n",
    "\n",
    "To do this:\n",
    "\n",
    "1. **Track counts** $n(s,a,s')$ and reward sums $R_{\\text{sum}}(s,a,s')$.\n",
    "2. Estimate the transition model as:\n",
    "\n",
    "   $$\n",
    "   \\hat{p}(s'|s,a) = \\frac{n(s,a,s')}{n(s,a)}\n",
    "   $$\n",
    "3. Estimate the reward model as:\n",
    "\n",
    "   $$\n",
    "   \\hat{r}(s,a,s') = \\frac{R_{\\text{sum}}(s,a,s')}{n(s,a,s')}\n",
    "   $$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
