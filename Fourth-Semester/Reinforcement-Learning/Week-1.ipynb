{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reinforcement Learning (RL) - Lecture 1**\n",
    "\n",
    "- **Reinforcement Learning is inspired by Instrumental Conditioning (Skinner Box)**\n",
    "\n",
    "| Feature | Supervised Learning | Reinforcement Learning |\n",
    "|---------|---------------------|------------------------|\n",
    "| Dataset | Given | Actively collected |\n",
    "| Feedback | Full (x with correct y) | Partial (state-action feedback) |\n",
    "| Goal | Learn a function from labeled data | Learn an **optimal policy** through interaction |\n",
    "\n",
    "---\n",
    "\n",
    "### **Chapter 1 Summary & Exam Notes**\n",
    "\n",
    "#### **Summary**\n",
    "This chapter introduces *Reinforcement Learning (RL)* as a computational approach to learning from interaction. Unlike supervised learning, RL involves an agent that learns through *trial-and-error* and *delayed rewards*, making decisions to maximize long-term rewards. The key difference from other learning methods is the trade-off between *exploration* (trying new actions) and *exploitation* (using known successful actions).\n",
    "\n",
    "RL is framed as a *Markov Decision Process (MDP)*, emphasizing three core elements:\n",
    "1. **Sensation (State)** – Perceiving the environment.\n",
    "2. **Action** – Choosing actions to interact with the environment.\n",
    "3. **Goal (Reward)** – A numerical signal guiding behavior.\n",
    "\n",
    "Key elements in RL include:\n",
    "- **Policy**: The strategy for selecting actions.\n",
    "- **Reward Function**: Determines immediate desirability of states/actions.\n",
    "- **Value Function**: Measures long-term expected rewards.\n",
    "- **Model (optional)**: Predicts state transitions for planning.\n",
    "\n",
    "The chapter presents examples like playing chess, controlling a refinery, and making breakfast to illustrate how RL applies to real-world decision-making. The *tic-tac-toe example* demonstrates RL principles using value function estimation and temporal-difference learning.\n",
    "\n",
    "Historically, RL stems from both psychology (trial-and-error learning) and optimal control theory (dynamic programming). Modern RL integrates these fields, evolving into a key area bridging AI and engineering.\n",
    "\n",
    "---\n",
    "\n",
    "- **Reinforcement Learning (RL)**: Learning through interaction to maximize cumulative reward.\n",
    "- **Trial-and-Error Learning**: Discovering the best actions by experimenting.\n",
    "- **Delayed Reward**: Actions influence future rewards, requiring foresight.\n",
    "- **Exploration vs. Exploitation**: Balancing trying new actions and leveraging past knowledge.\n",
    "- **Markov Decision Process (MDP)**: The mathematical framework underlying RL.\n",
    "\n",
    "#### **Main Elements of RL**\n",
    "1. **Policy (π)** – Mapping states to actions (deterministic or stochastic).\n",
    "2. **Reward Function (R)** – Immediate feedback for an action.\n",
    "3. **Value Function (V)** – Expected future rewards from a state.\n",
    "4. **Model (optional)** – Predicts environment transitions.\n",
    "\n",
    "#### **Comparison with Other Learning Methods**\n",
    "| Feature              | Reinforcement Learning | Supervised Learning | Evolutionary Methods |\n",
    "|----------------------|----------------------|----------------------|----------------------|\n",
    "| **Learning Style**   | Trial-and-error       | Given labeled data   | Direct policy search |\n",
    "| **Feedback Type**    | Reward signals        | Correct labels       | Population fitness   |\n",
    "| **Exploration Need** | Yes                   | No                   | Yes                  |\n",
    "\n",
    "#### **Important Challenges**\n",
    "- **Exploration-Exploitation Dilemma**: Agent must balance trying new actions vs. using known rewards.\n",
    "- **Credit Assignment Problem**: Determining which past actions led to success.\n",
    "- **Curse of Dimensionality**: Large state spaces make learning difficult.\n",
    "\n",
    "#### **Tic-Tac-Toe Example**\n",
    "- Uses **value function learning** to estimate probabilities of winning.\n",
    "- Updates values based on **temporal-difference (TD) learning**.\n",
    "- Learns optimal strategy through self-play.\n",
    "\n",
    "#### **Historical Context**\n",
    "- **Thorndike’s Law of Effect (1911)**: Reinforcement strengthens associations.\n",
    "- **Bellman’s Dynamic Programming (1957)**: Key foundation for RL.\n",
    "- **Minsky, Samuel (1950s-60s)**: Early AI applications of RL.\n",
    "- **Watkins (1989)**: Introduced *Q-learning*, a major RL algorithm.\n",
    "- **Tesauro’s TD-Gammon (1992)**: Demonstrated RL’s success in complex games.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **Reinforcement Learning (RL) Basics**  \n",
    "- **Agent** interacts with an **environment**, observes a **state**, takes an **action**, and gets a **reward**.  \n",
    "- Loop continues until the environment **stops** (e.g., game ends).  \n",
    "  - **Example**: A self-driving car (**agent**) sees a red light (**state**) and decides to stop (**action**) to avoid a fine (**reward**).  \n",
    "\n",
    "### **Contextual Bandit (One-Step RL)**  \n",
    "- **No time dimension** → One action, one reward, then stops.  \n",
    "  - **Example**: Netflix recommends a movie (**action**) based on your watch history (**state**). You watch it (**reward = 1**) or skip it (**reward = 0**).  \n",
    "\n",
    "### **Bandit (No Context)**  \n",
    "- **No state information** → Just picks an action blindly.  \n",
    "  - **Example**: A slot machine with **multiple arms**. The player (**agent**) pulls one (**action**) and gets a reward (money).  \n",
    "\n",
    "### **Multi-Armed Bandit & Exploration vs. Exploitation**  \n",
    "- **Multi-Armed Bandit**: Multiple actions, unknown rewards.  \n",
    "- **Exploration** → Try all actions to learn.  \n",
    "- **Exploitation** → Stick to the best-known action.  \n",
    "  - **Example**: A business tests ads on **Google, Facebook, and YouTube**. After learning Google works best, they focus more on it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<img src='../../Files/fourth-semester/rl/1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "In Reinforcement Learning (RL), a **bandit** refers to the **multi-armed bandit (MAB) problem**, a simplified RL setting where an agent repeatedly chooses among multiple actions (or \"arms\") to maximize rewards. Each action has an unknown reward distribution, and the goal is to balance **exploration** (trying new actions) and **exploitation** (choosing the best-known action).\n",
    "\n",
    "### Key Types of Bandit Problems:\n",
    "1. **Multi-Armed Bandit (MAB)** – Standard setting with independent rewards per arm.\n",
    "2. **Contextual Bandit** – The agent gets additional context (features) before choosing an action.\n",
    "3. **Bayesian Bandit** – Uses Bayesian methods to update beliefs about arm rewards.\n",
    "\n",
    "### Common Algorithms:\n",
    "- **ε-Greedy** – Explores randomly with probability **ε**, otherwise exploits the best-known arm.\n",
    "- **UCB (Upper Confidence Bound)** – Selects arms optimistically based on confidence intervals.\n",
    "- **Thompson Sampling** – Uses Bayesian probability to sample from reward distributions.\n",
    "\n",
    "Bandits are widely used in **A/B testing, recommendation systems, and online advertising**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
