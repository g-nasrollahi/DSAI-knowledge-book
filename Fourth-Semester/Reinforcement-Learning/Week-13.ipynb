{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3e2ac9",
   "metadata": {},
   "source": [
    "**Chess** solved with **minimax search** and **alpha-beta pruning**.\n",
    "- **Minimax** is a *backtracking* search method in a zero-sum game tree that assumes an optimal (utility-minimizing) opponent while you maximize your own utility.\n",
    "\n",
    "* **Game tree complexity:** $b^d$\n",
    "\n",
    "  * $b$ = branching factor (number of choices per move)\n",
    "  * $d$ = depth (number of moves in the game)\n",
    "* Go has a **big branching factor** and **huge depth**, making the search space enormous.\n",
    "* **Brute force search is impossible** because:\n",
    "\n",
    "  1. The search space is huge\n",
    "  2. No good heuristics to cut off the search early (unlike chess)\n",
    "\n",
    "**Image:** Go board with black and white stones illustrating complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Prior Approaches to Game Playing\n",
    "\n",
    "* **Minimax Search:** Used in chess with pruning (alpha-beta pruning) to reduce search space.\n",
    "* **Reinforcement Learning (RL):** Temporal difference learning works for some games, but weak for Go.\n",
    "* **Monte Carlo Tree Search (MCTS):** State-of-the-art before AlphaGo. It uses random simulations to estimate move quality.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Monte Carlo Tree Search (MCTS)\n",
    "\n",
    "MCTS finds the best move by repeatedly simulating games and building a search tree with four steps repeated:\n",
    "\n",
    "* **Selection:** Traverse the tree using a policy to select a node to expand\n",
    "* **Expansion:** Add new child nodes (possible moves)\n",
    "* **Simulation (Rollout):** Play random moves till the end or a cutoff to estimate the outcome\n",
    "* **Backup:** Update values in the tree nodes based on the simulation result\n",
    "\n",
    "---\n",
    "\n",
    "### MCTS in Detail:\n",
    "\n",
    "* **UCT formula (Upper Confidence Bound for Trees):**\n",
    "\n",
    "$$\n",
    "\\pi_{UCT}(s) = \\arg\\max_a Q(s,a) + c \\sqrt{\\frac{\\ln n(s)}{n(s,a)}}\n",
    "$$\n",
    "\n",
    "* $Q(s,a)$ is the value estimate for state $s$ and action $a$\n",
    "\n",
    "* $n(s)$ is how many times state $s$ was visited\n",
    "\n",
    "* $n(s,a)$ is how many times action $a$ was chosen at $s$\n",
    "\n",
    "* $c$ is a constant balancing exploration and exploitation\n",
    "\n",
    "* **Q-value update:**\n",
    "\n",
    "$$\n",
    "Q_{\\text{new}} \\leftarrow Q_{\\text{old}} + \\frac{1}{n} (R_{\\text{sum}} - Q_{\\text{old}})\n",
    "$$\n",
    "\n",
    "* $R_{\\text{sum}}$ is sum of rewards from simulations\n",
    "* $n$ is the number of visits\n",
    "\n",
    "---\n",
    "\n",
    "### Example Tree (Backup step):\n",
    "\n",
    "* Values $Q$ and visit counts $n$ are updated moving backward after rollout.\n",
    "* This helps refine move quality estimates over time.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Behavior Cloning\n",
    "\n",
    "* Goal: Mimic an expertâ€™s behavior using supervised learning.\n",
    "* Minimize KL divergence between expert policy $p_\\mu$ and learned policy $p_\\theta$:\n",
    "\n",
    "$$\n",
    "\\theta = \\arg\\min_\\theta KL(p_\\mu(\\cdot | s) \\| p_\\theta(\\cdot | s)) = \\arg\\max_\\theta \\sum \\mu(c) \\log p_\\theta(c|s)\n",
    "$$\n",
    "\n",
    "* Equivalent to Maximum Likelihood Estimation (MLE).\n",
    "* Learn from a dataset of expert moves.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Policy Gradient Methods\n",
    "\n",
    "* Directly optimize the policy parameters to maximize expected rewards.\n",
    "* **REINFORCE algorithm:** Uses sampled trajectories to estimate gradients.\n",
    "* **Actor-Critic:** Combines policy (actor) and value function (critic) for better learning.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. AlphaGo: Key Ideas\n",
    "\n",
    "* Combines **MCTS**, **RL**, and **Behavior Cloning**.\n",
    "* Uses **prior knowledge** from expert data and self-play to improve.\n",
    "* Self-play acts like curriculum learning, improving through repeated games.\n",
    "* MCTS uses learned policies and value functions to guide search, not just random rollouts.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. AlphaGo: Solving Breadth and Depth\n",
    "\n",
    "* Breadth solved by integrating expert policy priors to focus MCTS on promising moves.\n",
    "* Depth solved by learning value functions to evaluate board states without needing deep rollouts.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. AlphaGo: Simulation / Rollouts\n",
    "\n",
    "* Rollout policy is simple and computationally cheap, but not necessarily accurate.\n",
    "* Quick random play helps estimate outcomes but is improved by learned policies.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. AlphaGo: Backup and Expansion\n",
    "\n",
    "* Backup updates $Q$ values with learned values and rollout results.\n",
    "* Expansion adds new nodes based on promising moves suggested by policy network.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Results and Extra Steps\n",
    "\n",
    "* AlphaGo achieved superhuman performance.\n",
    "* Additional improvements included:\n",
    "\n",
    "  * Using pools of players in self-play\n",
    "  * Exploiting board symmetries\n",
    "  * Feature engineering for input representation\n",
    "  * Hyperparameter tuning\n",
    "  * Large computational resources (many CPUs and GPUs)\n",
    "\n",
    "---\n",
    "\n",
    "## 11. AlphaGo Zero\n",
    "\n",
    "* Learned from scratch without human data or expert guidance.\n",
    "* Only rules and raw board states used.\n",
    "* No rollouts; search guided by learned policy and value functions.\n",
    "* Simpler pipeline and even stronger performance.\n",
    "\n",
    "---\n",
    "\n",
    "# Important Images (include in notes):\n",
    "\n",
    "1. **Go board example** showing complexity.\n",
    "2. **MCTS tree with the four phases:** Selection, Expansion, Simulation, Backup (highlighted with arrows).\n",
    "3. **UCT formula** boxed and explained.\n",
    "4. **Q-value update formula** boxed and explained.\n",
    "5. **Example MCTS tree with Q and n values labeled** showing backup step.\n",
    "6. **AlphaGo architecture:** showing interaction between behavior cloning, MCTS, RL, and self-play.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "* Go is hard because of a huge branching factor and deep game tree.\n",
    "* Traditional methods (minimax, RL) struggle with Go.\n",
    "* MCTS improved performance by simulating games and updating a search tree.\n",
    "* AlphaGo combined MCTS with learning from expert data and RL self-play.\n",
    "* This led to a breakthrough in AI game playing.\n",
    "* AlphaGo Zero further simplified and improved the approach by learning purely from self-play.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
