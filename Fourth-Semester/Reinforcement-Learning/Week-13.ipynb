{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3e2ac9",
   "metadata": {},
   "source": [
    "- **Chess** solved with **minimax search** and **alpha-beta pruning**.\n",
    "  - **Minimax** is a *backtracking* search method in a zero-sum game tree trying utility-minmaxing. (Also used in Tic-Tac-Toe.)\n",
    "    - **Utility** a number that measures how good or bad a game position is for a player.\n",
    "  - **Alpha-beta pruning** reduces the search space by eliminating branches that won't affect the final decision.\n",
    "\n",
    "- **Go** $\\to$ **huge branching factor** and **depth**.\n",
    "  - **Game tree complexity:** $b^d$\n",
    "    - $b$ = branching factor (number of choices per move)\n",
    "    - $d$ = depth (number of moves in the game)\n",
    "  - **Brute force search** is (intractable) impossible:\n",
    "    - There is no **heruistic function** to **truncate** (cut off/purn) the search early, unlike chess.\n",
    "    \n",
    "---\n",
    "\n",
    "- **Monte Carlo Tree Search (MCTS)** finds the best move by repeatedly simulating games and building a search tree with four steps repeated:\n",
    "  - **Selection:** Traverse the tree using a policy to select a node to expand\n",
    "  - **Expansion:** Add new child nodes (possible moves)\n",
    "  - **Simulation (Rollout):** Play random moves till the end or a cutoff to estimate the outcome\n",
    "  - **Backup:** Update values in the tree nodes based on the simulation result\n",
    "\n",
    "---\n",
    "\n",
    "### MCTS in Detail:\n",
    "\n",
    "- **UCT formula (Upper Confidence Bound for Trees):** a policy that choose the best action $a$ from state $s$\n",
    "\n",
    "$$\n",
    "\\pi_{UCT}(s) = \\arg\\max_a Q(s,a) + c \\sqrt{\\frac{\\ln n(s)}{n(s,a)}}\n",
    "$$\n",
    "\n",
    "* $Q(s,a)$ is the value estimation for action $a$ at state $s$\n",
    "\n",
    "* $n(s)$ is how many times state $s$ was visited\n",
    "\n",
    "* $n(s,a)$ is how many times action $a$ was chosen at $s$\n",
    "\n",
    "* $c$ is a constant balancing exploration and exploitation\n",
    "\n",
    "Means: Choose the action aa that has the best mix of a high value Q(s,a)Q(s,a) and an exploration bonus that favors less tried moves.\n",
    "\n",
    "* **Q-value update:**\n",
    "\n",
    "$$\n",
    "Q_{\\text{new}} \\leftarrow Q_{\\text{old}} + \\frac{1}{n} (R_{\\text{sum}} - Q_{\\text{old}})\n",
    "$$\n",
    "\n",
    "* $R_{\\text{sum}}$ is sum of rewards from simulations\n",
    "* $n$ is the number of visits\n",
    "\n",
    "Means: Is like a average update, to refine the value estimate based on new information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b2fe29",
   "metadata": {},
   "source": [
    "**AlphaGo**: Leveraging prior knowledge both during learning and deployment.\n",
    "    - **Behavior cloning**: Train a neural network to mimic expert moves.\n",
    "    - **Monte Carlo Tree Search (MCTS)**: Uses smart search to look ahead in the game by simulating moves and outcomes before deciding what to do.\n",
    "    - **Reinforcement learning**: Trial and error learning from self-play to improve the model over time.\n",
    "    - **Self-play**: Curriculum learning by playing against itself, starting from random moves and gradually improving.\n",
    "    \n",
    "- **AlphaGo Neural Network**: \n",
    "    - **Rollout Policy $p_{\\pi}$:** NN trained by vlassification on human expert positions, plays quick, random moves (rollouts).\n",
    "    - **Supervised Learning Policy $p_{\\sigma}$:** Trained on many **human expert moves**. Learns to predict expert moves by **classifying** the next move given a board state, copying expert behavior (behavior cloning).\n",
    "    - **Reinforcement Learning (RL) Policy Network $p_{\\rho}$:** Starts from the SL policy network $p_{\\sigma}$ by improves by playing against itself (**self-play**) using **policy gradient** methods.\n",
    "    - **Value Network $v_{\\theta}$:** NN **predict the chance of winning** from any board position. Trained by **regression** on results of self-play games.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of the flow:\n",
    "\n",
    "* Start with expert data (human games) → train rollout and SL policy networks.\n",
    "* Use SL policy as a base and improve by self-play → train RL policy network.\n",
    "* Use self-play results to train value network that predicts winning chances.\n",
    "* All these networks help AlphaGo play much better than just copying humans.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae2083",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "* **KL divergence** measures how different two probability distributions are, minimizing KL divergence means **making your model’s policy close to the expert’s policy**.\n",
    "  - $\\theta = \\arg\\min_\\theta KL(p_\\mu(\\cdot | s) \\| p_\\theta(\\cdot | s))$ : Means you find the best parameters $\\theta$ that minimize the difference as\n",
    "    - $p_\\mu(\\cdot | s)$: The expert’s policy — how the expert chooses moves given state $s$.\n",
    "    - $p_\\theta(\\cdot | s)$: Your model’s policy — how your AI chooses moves given state $s$, controlled by parameters $\\theta$.\n",
    "  - **Maximum Likelihood Estimation (MLE)** is used to find parameters $\\theta$ that maximize the likelihood of expert moves, meaning you adjust your model to make it think the expert’s moves are very likely.\n",
    "    - $ \\arg\\max_\\theta \\sum \\mu(c) \\log p_\\theta(c|s) $\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ece416",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- **AlphaGo Algorithm Notation:**\n",
    "  - **Policy Prior $P(s,a) \\in [0,1]$:** Probability of taking action $a$ in state $s$.\n",
    "  - **# value function evaluation $N_v(s,a) \\in N_0$:** Number of times state $s$ has been evaluated.\n",
    "  - **# rollout evaluations $N_r(s,a) \\in N_0$:** Number of times action $a$ has been rolled out from state $s$.\n",
    "  - **Cumlative evaluated values $W_v(s,a) \\in R$:** Sum of values from evaluating state $s$ with action $a$.\n",
    "  - **Cumlative evaluated rollouts $W_r(s,a) \\in R$:** Sum of values from rolling out action $a$ from state $s$.\n",
    "  - **Transition value $Q(s,a) \\in [-1,1]$:** Estimated value of taking action $a$ in state $s$. The average quality (value) of taking action $a$ in state $s$. It represents the expected outcome (win or lose) of making that move.\n",
    "\n",
    "  - **Reward function $r(s_t, a_t)$**:\n",
    "\n",
    "$$\n",
    "r(s_t, a_t) = \n",
    "\\begin{cases} \n",
    "+1 & \\text{if win (terminal state)} \\\\\n",
    "-1 & \\text{if lose (terminal state)} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
