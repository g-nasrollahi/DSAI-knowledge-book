{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7845d29",
   "metadata": {},
   "source": [
    "* **Planning/search:** Using a model to look ahead and find good actions.\n",
    "* **Exhaustive search:** Finds the best action but needs huge computation (samples \\~ b^d, where b=branching factor, d=depth).\n",
    "* Search algorithms try to visit important states first to save computation.\n",
    "- **Types of Planning**:\n",
    "  * **Decision-time planning:** Focuses on picking the best action for the current state. (e.g., A\\*, MCTS)\n",
    "  * **Background planning:** Improves global policy or value function (like learning). (e.g., Dynamic Programming, Dyna)\n",
    "- **Classic Planning**\n",
    "* **Tree search:** Explores states like a tree, but can waste time on repeated states.\n",
    "* **Graph search:** Avoids repeated states by tracking explored (closed list) and frontier (open list).\n",
    "* **Uninformed search:** BFS, DFS, Iterative Deepening. Downsides: may miss better paths because they ignore costs/rewards/weights.\n",
    "* **Uniform cost search (Dijkstra):** Considers cost so far but ignores future potential.\n",
    "* **Heuristic search (A\\*):** Actual cumulative cost from start to state $s$ $(g(s))$ + estimated cumulative cost from $s$ to end $(h(s))$.\n",
    "\n",
    "  * Heuristic $h(s)$ must be **admissible** (never overestimate) to guarantee finding the best path.\n",
    "  * Perfect heuristic = optimal value function $V^*(s)$.\n",
    "  * $f(s) = g(s) \\to \\text{Dijkstra’s algorithm/Uniform-cost search} + h(s) \\to \\text{cumulative cost from s to end} = \\text{A* search}$.\n",
    "* **Forward pruning:** Limits actions explored (like beam search), but risks removing the best action.\n",
    "* **Stochastic planning:** Extends classic algorithms (e.g., A\\* → AO\\*) by expanding all possible outcomes of actions.\n",
    "  * Problems: Needs analytic model and heuristic, but often only simulators are available; large branching in stochastic cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a040a97",
   "metadata": {},
   "source": [
    "- **Sample-based Planning**:\n",
    "  * **Monte Carlo (MC) methods** to estimate action values by sampling instead of enumerating all possibilities.\n",
    "    * **Benefits:**\n",
    "\n",
    "      * No need for heuristic (use rollouts).\n",
    "      * No need to prune actions (use uncertainty to guide search).\n",
    "      * Can work with simulators, no exact model needed.\n",
    "    - **a) Monte Carlo Search (MCS)**\n",
    "      * For each action, sample N trajectories (rollouts) until depth D with a rollout policy (random or better).\n",
    "      * Estimate action value Q(s,a) by average returns, pick action with highest Q.\n",
    "      * **Downside:** Does not improve policy below the current step; no memory of past samples.\n",
    "    - **b) Sparse Sampling**\n",
    "        * Like MCS but repeats sampling at every level up to depth D (policy improvement at all depths).\n",
    "        * Sample complexity grows exponentially with D: (A \\* N)^D. Very expensive.\n",
    "    - **c) Monte Carlo Tree Search (MCTS)**\n",
    "        * Improves on Sparse Sampling by focusing sampling on promising actions using **adaptive bandit algorithms** (e.g., UCT).\n",
    "        * Builds an asymmetric tree, deeper where returns are better.\n",
    "        * Four phases:\n",
    "\n",
    "          1. **Selection:** Use UCT to select action balancing exploration/exploitation.\n",
    "          2. **Expansion:** Add new state when an unvisited action is chosen.\n",
    "          3. **Simulation:** Rollout from new state to estimate value.\n",
    "          4. **Backup:** Update statistics (visit counts and mean returns) up the tree.\n",
    "        * Sample complexity: proportional to M (number of traces) × D (depth).\n",
    "        * Very effective especially without a good heuristic (used in AlphaGo).\n",
    "- **Iterated Planning & Learning**\n",
    "\n",
    "  * Pure planning is expensive and may lack good heuristics.\n",
    "  * Pure learning may have errors in value/policy estimates.\n",
    "  * Combining both is powerful:\n",
    "\n",
    "    * Use planning to fix errors in learned values at decision-time.\n",
    "    * Use planning to generate data for learning in background.\n",
    "  * **AlphaGo** is an example that combines planning and learning iteratively.\n",
    "  * Analogy:\n",
    "\n",
    "    * Learned value function = \"Thinking fast\" (quick decisions).\n",
    "    * Decision-time planning = \"Thinking slow\" (careful analysis).\n",
    "  * Both work together for better decisions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
