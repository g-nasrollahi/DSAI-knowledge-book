{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- **Bandit**: One-time action, no state.\n",
    "    - **Example**: Imagine you're at a casino with slot machines (bandits). Each time you pull a lever, you get a random reward (e.g., coins). There's no state or context to consider—just pick a machine and try your luck.\n",
    "- **Contextual Bandit**: Action influenced by the current state (context).\n",
    "    - **Example**: Imagine you're a restaurant owner, and you need to offer different meals to customers based on their preferences (context). If you know the customer's preference (e.g., vegetarian or non-vegetarian), your choice of meal (action) will have a different reward.\n",
    "- **MDP**: Sequence of decisions, where actions influence future states and rewards.\n",
    "    - **Example**: Think of a video game where you control a character (e.g., a maze). Every action you take (e.g., moving left or right) changes the environment (state). You may choose a path that gives you a lower reward now but leads to a higher reward later (sequential thinking)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "| **Item**               | **Symbol**    | **Description**                                                  |\n",
    "|------------------------|---------------|------------------------------------------------------------------|\n",
    "| **State Space**         | $S $            | All possible states the system can be in                        |\n",
    "| **Action Space**        | $A  $           | All possible actions the agent can take                         |\n",
    "| **Transition Function** |$ p(s'\\|s,a)$     | Probability of transitioning to a state given an action         |\n",
    "| **Reward Function**     | $r(s,a,s')  $   | Immediate reward for transitioning between states                |\n",
    "| **Discount Factor**     | $γ   $          | Factor that determines how much future rewards are valued       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- **Policy**: Describes how the agent chooses actions in different states ($ \\pi(a|s) $).\n",
    "- **Trace**: A sequence of state-action-reward pairs induced by the policy ($ \\tau $).\n",
    "- **Return**: The cumulative sum of rewards from a trace ($ R(\\tau) $).\n",
    "- **Value**: The expected return from each state or action under a policy ($ v^\\pi(s) $, $ q^\\pi(s,a) $).\n",
    "- **Optimal Value/Policy**: There exists one optimal value function with a greedy policy that maximizes the return ($ v^*(s) $, $ q^*(s,a) $, $ \\pi^*(s) $)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## State\n",
    "##### Atomic State:\n",
    "- **Unique states, no relation**  \n",
    "- **Example**: A vending machine with each product as a separate state (e.g., Coke = state 1, Chips = state 2).\n",
    "\n",
    "##### Factorized State:\n",
    "- **State is a set of related features**  \n",
    "- **Example**: A self-driving car, where the state includes variables like speed, direction, and distance to obstacles (e.g., speed = 50 km/h, distance = 10m, direction = right). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Policy\n",
    "\n",
    "A **policy** specifies the probability of taking each action in a given state.\n",
    "\n",
    "- **Random Policy**: Each action has an equal chance of being selected in any state.\n",
    "- **Deterministic Policy**: Always chooses the same action in a given state.\n",
    "- **Greedy Policy**: Chooses the action that maximizes immediate reward, often focusing on short-term gains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Trace:\n",
    "- **Trace** refers to the sequence of state-action-reward pairs observed when an agent acts in an MDP.\n",
    "- **Notation**: $ \\tau = \\{s_t, a_t, r_t, s_{t+1}, a_{t+1}, r_{t+1}, \\dots\\} $\n",
    "  - **Subscript $t$**: Denotes the timestep.\n",
    "  - **Greek letter $ \\tau $**: Represents the entire sequence (trace).\n",
    "\n",
    "### Trace Probability:\n",
    "- To calculate the probability of a trace, multiply the individual probabilities of actions, transiti`ons, and rewards along the trace.\n",
    "- **Formula**: \n",
    "  $$\n",
    "  p(\\tau) = \\pi(a_t | s_t) \\cdot p(r_t, s_{t+1} | s_t, a_t) \\cdot \\pi(a_{t+1} | s_{t+1}) \\cdot p(r_{t+1}, s_{t+2} | s_{t+1}, a_{t+1}) \\dots\n",
    "  $$\n",
    "  - This product rule combines the probabilities of each action, reward, and state transition along the trace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Return:\n",
    "- **Return** is the total sum of rewards obtained over a trace.\n",
    "- **Formula**: $ R(\\tau) = r_t + r_{t+1} + r_{t+2} + \\dots $\n",
    "\n",
    "### Discounted Return:\n",
    "- **Discounted Return** gives less importance to future rewards.\n",
    "- **Formula**: $ R(\\tau) = r_t + \\gamma \\cdot r_{t+1} + \\gamma^2 \\cdot r_{t+2} + \\dots $\n",
    "  - **$ \\gamma $** is the discount factor ($ 0 \\leq \\gamma \\leq 1 $).\n",
    "\n",
    "### Trace Horizon:\n",
    "- **Infinite Horizon**: Sum rewards infinitely unless a terminal state is reached.\n",
    "- **Formula**: $ R(\\tau) = \\sum_{i=0}^{\\infty} \\gamma^i \\cdot r_{t+i} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Value:\n",
    "- **Value of a State**: It represents the expected cumulative reward when starting from state $ s $ under a given policy $ \\pi $.\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  v^\\pi(s) = E_{\\tau \\sim p^\\pi(\\tau)} [r_t + \\gamma \\cdot r_{t+1} + \\gamma^2 \\cdot r_{t+2} + \\dots | s_t = s]\n",
    "  $$\n",
    "  - This gives the expected total reward from a state, considering future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Each policy $π$ has its own value function shown with $v^\\pi(s)$ , which can be optimal ($v^*$) and if it's based on action-state pairs, it's shown with $q^\\pi(s,a)$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
