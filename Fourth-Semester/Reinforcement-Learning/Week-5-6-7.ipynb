{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Traditional MDPs assume we know everything about the environment. But we want to learn good policies without knowing everything in advance.\n",
    "- In **Model-based RL**, we aim to estimate transition and reward functions from experience.\n",
    "\n",
    "---\n",
    "\n",
    "### **Monte Carlo (MC) Methods**  \n",
    "- **Goal**: Estimate the value of states under a policy $ \\pi $.\n",
    "- **First-Visit MC Method**:\n",
    "  - **Initialize state values** $ v^\\pi(s) $ by our desire.\n",
    "  - **Generate episodes**: You start at an initial state and take actions according to the **policy** which forms an **episode** (a sequence of states, actions, and rewards).\n",
    "  - **For each state visited in the episode**, update its value using the **cumulative discounted rewards** from that state onwards.\n",
    "\n",
    "\n",
    "- The **update rule**: For each state visited, calculate the **average reward** after its first occurrence.\n",
    "  - **Formula**:  \n",
    "    $$\n",
    "    v^\\pi(s) = \\frac{(v^\\pi(s) \\text{ from previous episodes} + \\text{new reward})}{\\text{number of episodes}}\n",
    "    $$\n",
    "  \n",
    "- **Goal**: Refine state value estimates after multiple episodes.\n",
    "\n",
    "- **Monte Carlo for Action Values**:\n",
    "  - Use similar MC methods but calculate averages for **state-action pairs**.\n",
    "- **Exploration Issue**: Some state-action pairs may not be visited. **Exploring starts** (starting from a random state-action pair) can address this.\n",
    "\n",
    "#### **First-Visit vs Every-Visit MC**\n",
    "- **First-Visit MC**: Updates the value of a state only after its first occurrence in an episode.\n",
    "- **Every-Visit MC**: Updates the value of a state every time it is visited in an episode.\n",
    "- **Example**: \n",
    "  - **First-Visit**: If a state is visited multiple times in an episode, only the first visit's reward is used for the update.\n",
    "  - **Every-Visit**: All visits to the state in the episode contribute to the update.\n",
    "\n",
    "\n",
    "<img src=\"../../Files/fourth-semester/rl/7.png\" width=\"500px\">\n",
    "<img src=\"../../Files/fourth-semester/rl/8.png\" width=\"500px\">\n",
    "<img src=\"../../Files/fourth-semester/rl/9.png\" width=\"500px\">\n",
    "\n",
    "#### **Generalized Policy Iteration (GPI)**\n",
    "- **Goal**: Balance exploration and exploitation.\n",
    "- **Policy Evaluation**: Estimate the value of a policy.\n",
    "- **Policy Improvement**: Update the policy based on the estimated values.\n",
    "- **GPI Process**:\n",
    "  1. **Policy Evaluation**: Use MC methods to estimate the value of the current policy.\n",
    "  2. **Policy Improvement**: Update the policy to be greedy with respect to the estimated values.\n",
    "  3. Repeat until convergence.\n",
    "\n",
    "\n",
    "### **On-Policy vs. Off-Policy Methods**\n",
    "- **On-policy**: The policy used to generate episodes is the same as the one being optimized.\n",
    "  - **Advantages**: Simple and easy to implement.\n",
    "  - **Disadvantages**: Can be suboptimal due to constant exploration.\n",
    "  \n",
    "- **Off-policy**: The policy used to generate episodes differs from the one being optimized.\n",
    "  - **Advantages**: More powerful and flexible.\n",
    "  - **Disadvantages**: More complicated and slower to converge.\n",
    "    - In off-policy learning, the behavior policy $ b $ (the policy used to generate episodes) differs from the target policy $ \\pi $ (the policy being optimized).\n",
    "    - **Problem**: If an action is not taken in the behavior policy, the value for that state-action pair is unknown.\n",
    "    - **Solution**: **Importance sampling** adjusts the returns to account for differences in probabilities between $ b $ and $ \\pi $.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Temporal Difference (TD) Learning**\n",
    "- **Goal**: Avoid waiting until the end of an episode to update values. TD learning allows for updating values incrementally during the episode.\n",
    "- **TD(0)**: A one-step update method where the value of a state is updated based on the next step:\n",
    "  $$\n",
    "  v^\\pi(s_t) \\leftarrow v^\\pi(s_t) + \\alpha \\left[ R_{t+1} + \\gamma v^\\pi(s_{t+1}) - v^\\pi(s_t) \\right]\n",
    "  $$\n",
    "- **Advantages**: Updates are made during the episode, enabling faster learning.\n",
    "\n",
    "\n",
    "#### **Q-Learning**\n",
    "- **Off-policy TD Control**: Q-learning is an off-policy method where the agent learns the optimal policy using a greedy target:\n",
    "  $$\n",
    "  Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n",
    "  $$\n",
    "- **Key Point**: The **maximization bias** occurs when the agent overestimates action values.\n",
    "\n",
    "- **Solution**: **Double Q-learning** uses two independent Q-tables to reduce maximization bias.\n",
    "\n",
    "### **Sarsa**\n",
    "- **On-policy TD Control**: Sarsa is an on-policy method where the agent learns the value of the current policy:\n",
    "  $$\n",
    "  Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n",
    "  $$\n",
    "- **Key Point**: Sarsa updates the Q-value based on the action taken in the next state, making it more conservative than Q-learning.\n",
    "\n",
    "\n",
    "**Sarsa vs Q-learning**: **Sarsa** uses the current policy to update the Q-values, while **Q-learning** uses the maximum future Q-value, independent of the current policy.\n",
    "\n",
    "### **Maximization Bias and Double Q-learning**\n",
    "- **Maximization Bias**: Q-learning can overestimate action values due to always selecting the maximum Q-value in future states.\n",
    "- **Double Q-learning**: Addresses this bias by maintaining two separate Q-tables and using one to select actions and the other to evaluate them, reducing overestimation.\n",
    "\n",
    "### **N-step TD and N-step SARSA**\n",
    "- **N-step TD**: An extension of TD learning that uses multiple steps of temporal difference learning to update values instead of just one step, thus incorporating more information for better value estimation.\n",
    "- **N-step SARSA**: Similar to N-step TD but applies to the SARSA algorithm, allowing for more accurate value updates by considering multiple steps of rewards.\n",
    "### **Expected SARSA**\n",
    "- **Expected SARSA**: An extension of SARSA where the agent uses the expected value of the next state-action pair, weighted by the policy's probabilities. This can be used both on-policy and off-policy.\n",
    "- **Key Point**: Expected SARSA can provide a more stable learning process by averaging over possible actions rather than selecting the maximum.\n",
    "### **Summary**\n",
    "- **Monte Carlo (MC) Methods**: Learn state values by averaging returns over episodes.\n",
    "- **Temporal-Difference (TD) Methods**: Learn values incrementally during episodes without waiting for the end.\n",
    "- **On-Policy vs. Off-Policy**: On-policy uses the same policy for generating episodes and improving it, while off-policy uses different behavior and target policies.\n",
    "- **Q-learning**: An off-policy TD method that uses the **greedy approach** to maximize rewards, but has **maximization bias**, which can be solved with Double Q-learning.\n",
    "- **Sarsa**: An on-policy TD method that updates values based on the current policy, making it more conservative.\n",
    "- **N-step TD and N-step SARSA**: Extensions of TD and SARSA that consider multiple steps for better value estimation.\n",
    "- **Expected SARSA**: An extension of SARSA that uses expected values for more stable learning.\n",
    "- **Generalized Policy Iteration (GPI)**: A framework that combines policy evaluation and improvement to iteratively refine policies."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
