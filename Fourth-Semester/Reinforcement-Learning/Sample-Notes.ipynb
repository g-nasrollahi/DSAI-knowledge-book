{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6d7392",
   "metadata": {},
   "source": [
    "* **ε-greedy**:\n",
    "    - Mostly **exploits**\n",
    "    - ε choice is random and doesn’t depend on how uncertain we are; \n",
    "    - Switches between exploration and exploitation **suddenly** and not smoothly.\n",
    "\n",
    "* **UCB**:\n",
    "    - Uses **uncertainty** (like confidence bounds) to decide what to try.\n",
    "    - If an action hasn’t been tried much, its uncertainty is high, so UCB will **explore** it more. \n",
    "    - As the algorithm learns more, **uncertainty decreases**, so it slowly shifts from exploring uncertain options to exploiting the best one. \n",
    "    - Transition from exploration to exploitation is **gradual** and based on actual confidence.\n",
    "\n",
    "* **Tabular Dynamic Programming**: finds a **global** solution, and is **guaranteed** to **converge** to the optimal solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cacf98",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- **First-visit Monte Carlo** $\\to$ the value of a state is updated by averaging the total reward received **from the first time the state is visited until the end of the episode** (which is the cumulative future discounted reward).\n",
    "- **MCTS** focuses on **exploring the most promising move**s (root nodes) using **heuristics** (smart guesses).\n",
    "- Policy gradient methods learn a policy that **gives the chance (probability) of picking each action**. So, they can choose actions with **any possible probabilities**, not just the best one every time.\n",
    "- Policy gradient methods directly learn a policy (a way to pick actions), using parameters.\n",
    "- Action-value methods (like Q-learning) learn values for actions, and then a policy is made from those values (usually picking the best action).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f36ae8",
   "metadata": {},
   "source": [
    "- There is only one optimal value function, but there may be multiple optimal policies.\n",
    "- **Bootstrapping** means updating estimates using other estimates. For example, in Dynamic Programming and Temporal Difference (TD) learning, you update the value of a state based partly on the value of the next state (an estimate). Unlike Monte Carlo methods, which wait until the end of an episode to update values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a46157",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- In MCTS:\n",
    "    - Playout = Simulation\n",
    "    - Backpropagation = Backup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75648fd7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Model-based reinforcement learning uses a model of the environment, mainly the transition function, which tells how the environment moves from one state to another after an action.\n",
    "\n",
    "* **Variable-depth:** MCTS grows the tree deeper only where needed, not a fixed number of steps everywhere. This helps focus on important moves.\n",
    "\n",
    "* **Random playout:** After adding a node, MCTS plays random moves to quickly estimate how good that position is.\n",
    "\n",
    "* **Value iteration** uses the **max** over actions, which is different from the current policy → so it's **off-policy**.\n",
    "* It calculates the backup by taking the **expected value over all possible next states** (the full model), not just samples → so it's **expected over the dynamics**.\n",
    "\n",
    "* **RL object** $\\to$ Find optimal policy"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
