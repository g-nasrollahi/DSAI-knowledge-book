{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### **Policy Iteration**:\n",
    "1. Initialize a random policy.\n",
    "2. **Policy Evaluation**: Calculate the value of each state for the current policy.\n",
    "3. **Policy Improvement**: Update the policy by choosing the best action (greedy) for each state based on its value.\n",
    "4. Repeat steps 2 and 3 until the policy converges (i.e., it doesn't change anymore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **2. Value Iteration**:\n",
    "- **Value Iteration** combines both **policy evaluation** and **policy improvement** into a single step. Instead of separately evaluating and then improving the policy, value iteration iteratively updates the **value function** until it converges to the optimal values for all states. From these values, you can derive the optimal policy.\n",
    "\n",
    "### **Steps of Value Iteration**:\n",
    "1. **Initialize the value function** $ v(s) $ for all states, typically set to 0.\n",
    "2. **Repeat** the following step for each state:\n",
    "   - Update the value of each state using the **Bellman Equation**:\n",
    "     $$\n",
    "     v(s) = \\max_a \\left[ \\sum_{s'} p(s'|s,a) \\left( r(s, a, s') + \\gamma \\cdot v(s') \\right) \\right]\n",
    "     $$\n",
    "   - This equation estimates the value of a state by looking at all possible actions and choosing the one that maximizes the expected future reward.\n",
    "3. Repeat the value update until the values converge (the difference between old and new values becomes very small).\n",
    "4. **Derive the optimal policy** by selecting the action that maximizes the value for each state:\n",
    "   $$\n",
    "   \\pi^*(s) = \\arg \\max_a [ v(s) ]\n",
    "   $$\n",
    "   - Once the values have converged, the optimal policy can be obtained by simply choosing the action that maximizes the expected value for each state.\n",
    "\n",
    "---\n",
    "### **Key Differences**:\n",
    "1. **Policy Iteration**:\n",
    "   - Alternates between **policy evaluation** and **policy improvement**.\n",
    "   - Converges faster in practice, but requires **two steps per iteration** (policy evaluation and improvement).\n",
    "   - **Policy Iteration** tends to converge **faster** than value iteration when the state space is small to medium.\n",
    "\n",
    "2. **Value Iteration**:\n",
    "   - Performs a **single step** that combines both evaluation and improvement.\n",
    "   - Can take longer because it updates the value function iteratively until it converges, but it doesn't require explicit policy evaluation.\n",
    "   - **Value Iteration** can be more efficient when the problem requires fewer iterations, but it may take longer to converge on larger state spaces.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
