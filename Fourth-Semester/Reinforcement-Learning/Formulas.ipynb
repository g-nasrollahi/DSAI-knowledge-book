{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b575acf6",
   "metadata": {},
   "source": [
    "- General Bellman Equation for State Value Function $v^\\pi(s)$: $ v^\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} p(s'|s,a) \\left[ r(s,a,s') + \\gamma v^\\pi(s') \\right] $\n",
    "- **Bellman Expectation Equation for $q^\\pi(s,a)$** : $ q^\\pi(s,a) = \\sum_{s'} p(s'|s,a) \\left[ r(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') q^\\pi(s',a') \\right] $\n",
    "- **Bellman Optimality Equation for $q^*(s,a)$:** $ q^*(s,a) = \\sum_{s'} p(s'|s,a) \\left[ r(s,a,s') + \\gamma \\max_{a'} q^*(s', a') \\right] $\n",
    "- Bellman Optimality Equation for State Value Function $v^*(s)$: $ v^*(s) = \\max_a \\sum_{s'} p(s'|s,a) \\left[ r(s,a,s') + \\gamma v^*(s') \\right] $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f58c49",
   "metadata": {},
   "source": [
    "* **Softmax (Boltzmann) Policy:** $ P(a) = \\frac{e^{Q(a)/\\tau}}{\\sum_b e^{Q(b)/\\tau}} $\n",
    "* **Upper Confidence Bound (UCB):** $ \\text{Choose } a = \\arg\\max_a \\left[ Q(a) + c \\sqrt{\\frac{\\ln t}{n(a)}} \\right] $\n",
    "* **Policy Evaluation:** $v^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma v^\\pi(s')]$\n",
    "* **Policy Improvement:** $ \\pi'(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a) [r + \\gamma v^\\pi(s')] $\n",
    "* **Value Iteration**: $ V_{k+1}(s) = \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma V_k(s')]$\n",
    "* **Temporal Difference Value update (TD(0))**: $ v(s_t) \\leftarrow v(s_t) + \\alpha \\left[ r_{t+1} + \\gamma v(s_{t+1}) - v(s_t) \\right] $\n",
    "* **TD error:** $\\delta_t = r_{t+1} + \\gamma v(s_{t+1}) - v(s_t)$\n",
    "* **SARSA (On-policy TD Control):** $ Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right] $\n",
    "* **Q-learning (Off-policy TD Control):** $Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]$\n",
    "* **Expected SARSA:** $ Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\sum_a \\pi(a|s_{t+1}) Q(s_{t+1}, a) - Q(s_t, a_t) \\right] $\n",
    "* **UCT:** $\\pi_{UCT}(s) = \\arg\\max_a \\left[ Q(s,a) + c \\sqrt{\\frac{\\ln n(s)}{n(s,a)}} \\right]$\n",
    "* **KL divergence minimization:** $\\theta = \\arg\\min_\\theta KL(p_\\mu(\\cdot|s) \\| p_\\theta(\\cdot|s))$\n",
    "* **Maximum Likelihood Estimation:** $\\arg\\max_\\theta \\sum \\mu(c) \\log p_\\theta(c|s)$\n",
    "* **SARSA with function approximation:** $w \\leftarrow w + \\alpha \\left( R_{t+1} + \\gamma q(S_{t+1}, A_{t+1}, w) - q(S_t, A_t, w) \\right) \\nabla q(S_t, A_t, w) $\n",
    "* **Q-learning with function approximation:** $w \\leftarrow w + \\alpha \\left( R_{t+1} + \\gamma \\max_{a'} q(S_{t+1}, a', w) - q(S_t, A_t, w) \\right) \\nabla q(S_t, A_t, w) $\n",
    "* **Value error:** $VE(w) = \\sum_s \\mu(s) \\left[ v_\\pi(s) - \\hat{v}(s, w) \\right]^2 $\n",
    "* **Gradient descent update:** $w \\leftarrow w - \\alpha \\nabla VE(w)$\n",
    "* **Gradient Monte Carlo:** $w \\leftarrow w + \\alpha (G_t - \\hat{v}(S_t, w)) \\nabla \\hat{v}(S_t, w)$\n",
    "* **Gradient TD(0):** $w \\leftarrow w + \\alpha \\left( R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w) \\right) \\nabla \\hat{v}(S_t, w)$\n",
    "* **TD error (critic):** $\\delta_t = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w) $\n",
    "* **Actor update:** $\\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla \\log \\pi(A_t | S_t, \\theta) $"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
