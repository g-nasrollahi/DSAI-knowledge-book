{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f33aa7a",
   "metadata": {},
   "source": [
    "- **AlphaGO**: Rollout Policy $p_{\\pi}$ and SL NN (expert classification) $\\to$ self play and with SL train RL Ploicy NN $\\to$ Value NN (regression) winning chance.\n",
    "- **Sparse Sampling** (repeats for $\\pi$ improvement at every level up to depth): complexity grows exponentially. $ (A \\cdot N )^D $\n",
    "* **UCB**: Action not tried $\\to$ high uncertainty $\\to$ **explore**  (Smooth and gradual)\n",
    "\n",
    "- **Model-Based RL**:\n",
    "  - **Dyna** (combine learning and planning): Learn model of env $\\to$ table $\\to$ simulate to update $\\pi$ and $V(s)$ and transitions.\n",
    "    - Number of planning (simulation) $K$\n",
    "  - **Prioritized Sweeping**: Focus on states that are likely to change significantly, using a priority queue to update the most important states first. \n",
    "- **Model-Free RL (Value based RL)**: Learn the Value function approximation directly\n",
    "  - **Monte Carlo Tree Search (MCTS)** (Planning): builds a search tree using simulated rollouts but doesn’t require a full model uses sampling to explore possible moves (env)\n",
    "    - Planning (sample based)\n",
    "      - MCTS complexity is $ M \\cdot D $\n",
    "    - Local solution\n",
    "    - Use UCT\n",
    "    - MCTS explores unknown parts of the tree efficiently without expanding everything.\n",
    "    - MCTS focuses search on the most promising actions near the root using heuristics like UCT.\n",
    "  - **Monte Carlo Methods** (High variance): learn from complete episodes of experience without needing a model of the environment.\n",
    "  - **Temporal Difference Learning** ((Valu function approximation directly), bootstraps -> bias, fast): From experience, Mix MC and DP, step by step in episodes.\n",
    "    - **TD(0)**: immediate reward and the value of the next state.\n",
    "    - **TD(λ)**: generalized, using eligibility traces, get parameter for multiple ahead steps.\n",
    "    - **SARSA**(On-policy): improves the policy by learning from the actual actions it takes (following the current policy). \n",
    "    - **Q-Learning**(Off-policy): improves the policy by learning from the best possible actions, even if it didn’t take those \n",
    "      - Pick Max Q $\\to$ **Maximization Bias** $\\to$ **Double Q-Learning** $\\to$ 2 Q-tables.\n",
    "    - **Expected SARSA**: uses the expected value of the next action instead of the max action.\n",
    "      - more stable and less noisy\n",
    "  - **Policy Gradient**:\n",
    "    - **REINFORCE** (MC): update policy parameter $\\theta$ using full episodes.\n",
    "\n",
    "- **Value Function Approximation** (gradient descent): Sample estimate instead experience.\n",
    "  - **Gradient Monte Carlo:** Update $V(S)$ using gradient on expected return (V get close to G)\n",
    "  - **Gradient TD(0) Algorithm**:  Update $V(S)$ using TD error, next state: reward + estimated value\n",
    "  - **Approximating Values** $\\hat{v}(s, w)$: We can’t store values for all states, so we use a **function** with parameters $w$ to guess values. Types:\n",
    "    - **State Aggregation**: Group , simple, loose datils\n",
    "    - **Linear Approximation**: Weighted sum vector, simple, efficient\n",
    "    - **Feature Types**: Polynomial, Fourier (sin/cos), Coarse Coding (overlapping regions)\n",
    "  - **Challenge**: Instability and divergence possible with off-policy methods and nonlinear function approximators.\n",
    "  - **Policy-Based Methods** (Directly learn policy):\n",
    "      - Policy gradient methods can pick actions using any probabilities they want. (have selection of actions with arbitrary probabilities.)\n",
    "      - **Directly parameterize the policy** $\\pi(a|s, \\theta)$.\n",
    "        - **Advantages**:\n",
    "          - Can learn stochastic policies.\n",
    "          - Better for high-dimensional or continuous action spaces.\n",
    "          - Avoids need to estimate value for every action.\n",
    "      - **Policy Gradient Theorem**:\n",
    "        - Provides formula for gradient of expected return with respect to policy parameters.\n",
    "\n",
    "- **Types of Planning**:\n",
    "  * **Decision-time planning:** A\\*, MCTS\n",
    "  * **Background planning:** Improves global policy or value function (like learning). (e.g., Dynamic Programming, Dyna)\n",
    "  - **Classic Planning: (Need Heuristic)**\n",
    "    * **Uniform cost search (Dijkstra):** Considers cost so far but ignores future potential.\n",
    "    * **Heuristic search (A\\*):** $f(s) = g(s) \\to \\text{Dijkstra’s algorithm/Uniform-cost search} + h(s) \\to \\text{cumulative cost from s to end} = \\text{A* search}$.\n",
    "  * **Stochastic planning:** Extends classic algorithms (e.g., A\\* → AO\\*) by expanding all possible outcomes of actions.\n",
    "    * Problems: Needs analytic model and heuristic, but often only simulators are available; large branching in stochastic cases.\n",
    "\n",
    "- **Actor-Critic**: Online and incrementally (like MC)\n",
    "  - **Actor**: Updates policy and parameters $\\theta$ \n",
    "  - **Critic**: Updates $V(S)$ parameters $w$ (evaluates actions) with TD."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
